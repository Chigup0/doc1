{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TheDrive - AI-Powered Cross-Platform File Storage System","text":""},{"location":"#project-overview","title":"Project Overview","text":"<p>TheDrive is an AI-powered cloud storage platform designed to transform the way users store, interact with, and manage their digital assets. Unlike traditional cloud storage systems like Google Drive or OneDrive, TheDrive integrates advanced AI technologies to provide smarter, more intuitive ways to interact with files, derive meaningful insights, and enhance overall user experience. </p> <p>With the growing concerns around data privacy and the increasing need for security, TheDrive has been built to not only offer intelligent features but also ensure that user data remains safe, secure, and private. This platform is not just about storing files\u2014it's about empowering users with the tools to analyze, manage, and gain insights from their digital assets, all while maintaining the highest standards of security.</p>"},{"location":"#problem-statement","title":"Problem Statement","text":"<p>As the amount of digital data grows, users are faced with challenges not only in managing this information but also in ensuring its security and privacy. Traditional cloud storage systems allow for basic file storage, sharing, and collaboration, but they fall short when it comes to the following:</p> <ul> <li>Intelligence: Current systems do not provide advanced tools to analyze and interact with file contents effectively.</li> <li>Security: While they offer security features, many systems are vulnerable to data breaches or unauthorized access, often exposing user data to administrators and third parties.</li> <li>User Experience: Current solutions don't integrate AI-powered features such as content-based search, query resolution, or semantic understanding of data in a seamless and intuitive way.</li> </ul> <p>TheDrive aims to solve these issues by creating an intelligent, secure cloud storage system that enables users to interact with their files in ways never before possible.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#1-multi-file-format-support","title":"1. Multi-File Format Support","text":"<p>TheDrive supports a wide range of file formats, making it a versatile platform for various use cases:</p> <ul> <li>Document Formats: <code>.pdf</code>, <code>.docx</code>, <code>.txt</code>, <code>.pptx</code> (PowerPoint presentations)</li> <li>Spreadsheet Formats: <code>.csv</code>, <code>.xlsx</code> (Excel spreadsheets)</li> <li>Image Formats: <code>.jpg</code>, <code>.png</code></li> </ul> <p>Each of these file types is processed and displayed with content integrity, preserving the structure, layout, and formatting. This ensures that no matter the file type, users can interact with their files in a seamless, efficient way.</p>"},{"location":"#2-ai-driven-smarter-storage-system","title":"2. AI-Driven Smarter Storage System","text":""},{"location":"#a-contextual-chat","title":"a. Contextual Chat","text":"<p>One of the most innovative features of TheDrive is its AI-powered contextual chat system. This allows users to interact with their storage by asking questions about their files, folders, or documents, and the system will provide relevant answers. </p> <ul> <li>Chat Over Whole Drive: </li> <li>Users can ask questions about the entire contents of their drive, such as \"What documents contain the word 'budget'?\" or \"Show me the documents that mention 'project proposal'.\" The system can also calculate aggregation metrics, such as the total number of words across all documents.</li> <li> <p>This feature goes beyond just keyword search\u2014it's about understanding the context of the files, providing more intelligent responses.</p> </li> <li> <p>Chat Over Folders:</p> </li> <li> <p>Users can query specific folders without revealing information from other folders, ensuring data privacy at the folder level. For example, a user could ask, \"How many documents are there in this folder related to 'marketing'?\" and receive a relevant answer without exposing files from other folders.</p> </li> <li> <p>Chat Over Documents:</p> </li> <li>Users can ask about specific documents, and the AI will provide context and insights based on the content of the file.</li> </ul>"},{"location":"#b-highlight-based-query-resolution-and-explanation","title":"b. Highlight-Based Query Resolution and Explanation","text":"<p>TheDrive goes beyond just storing files\u2014users can highlight specific text or portions of a document, image, or table, and the system will provide AI-powered insights. </p> <ul> <li>Text Highlighting: </li> <li> <p>When a user highlights a word or phrase, the system will provide its definition, usage, and related explanations. This makes it easy for users to quickly understand complex terms without leaving the platform.</p> </li> <li> <p>Image/Picture Interaction:</p> </li> <li> <p>For images or presentations, users can highlight portions of a visual (e.g., a chart, diagram, or handwritten note) to get an AI-generated description or analysis of that section.</p> </li> <li> <p>Table Analysis:</p> </li> <li>In spreadsheet files, users can select portions of tables to get aggregation metrics, summaries, or insights based on the data, helping users analyze their files more efficiently.</li> </ul>"},{"location":"#c-cross-referencing-files-and-sections","title":"c. Cross-Referencing Files and Sections","text":"<p>Cross-referencing is a key feature that allows users to trace the origins of the data used to answer a query. Whenever a user asks a question about a document, the system provides:</p> <ul> <li>Source Citation: Links to the specific file and section that answered the query.</li> <li>Chunk Highlighting: When opening the linked file, the exact section of text or data used for the answer will be highlighted for quick reference.</li> </ul>"},{"location":"#d-other-ai-powered-features","title":"d. Other AI-Powered Features","text":"<ul> <li>Image Understanding: </li> <li> <p>TheDrive can analyze and understand content within images, such as handwritten notes (via OCR), charts, diagrams, and more. Users can ask the system to describe, explain, or analyze parts of images, making the system more versatile for users dealing with visual data.</p> </li> <li> <p>Semantic Search: </p> </li> <li>Unlike traditional search engines that rely solely on keywords, semantic search allows users to search based on the meaning and context of the query, returning more relevant and accurate results.</li> </ul>"},{"location":"#system-architecture","title":"System Architecture","text":"<p>TheDrive is built using a microservices architecture with a serverless backend and scalable cloud infrastructure to ensure both reliability and performance.</p>"},{"location":"#key-components","title":"Key Components:","text":"<ul> <li>Frontend (Next.js):</li> <li> <p>Provides a dynamic, responsive web application that enables users to interact with their files, perform searches, and access AI-powered insights.</p> </li> <li> <p>Backend (FastAPI):</p> </li> <li> <p>Powers the core services of the application, handling user authentication, file management, and AI query processing. It communicates with databases and AI models to provide intelligent responses.</p> </li> <li> <p>AI Layer:</p> </li> <li> <p>A set of machine learning models and algorithms that process text, images, and tables to provide users with insights, document summaries, and analytics.</p> </li> <li> <p>Database Layer:</p> </li> <li> <p>The system uses a combination of SQL databases (for general metadata and user data) and graph databases (Neo4j) for representing complex relationships between files and metadata.</p> </li> <li> <p>Cloud Integration:</p> </li> <li>The system is hosted on cloud platforms like AWS and Azure, ensuring scalability, high availability, and reliability.</li> </ul>"},{"location":"#security","title":"Security","text":"<p>TheDrive has been built with security as a top priority. It uses end-to-end encryption (E2EE) to ensure that data is encrypted both at rest and in transit.</p> <ul> <li>Admin Access: No administrative user has access to the data. All data remains fully encrypted unless explicitly accessed by the user.</li> <li>Data Isolation: Each user\u2019s data is physically and logically isolated to prevent unauthorized access or leakage.</li> </ul>"},{"location":"#conclusion","title":"Conclusion","text":"<p>TheDrive is not just a cloud storage solution\u2014it's an intelligent, AI-powered platform that changes the way users interact with their files. By offering powerful tools for searching, analyzing, and interacting with data, all while ensuring the highest levels of security and privacy, TheDrive aims to redefine the future of cloud storage.</p> <p>TheDrive offers: - Smarter interaction with digital assets using AI. - Advanced file analysis capabilities. - Seamless integration with various data formats. - A strong focus on security and data privacy.</p> <p>This makes TheDrive an ideal solution for users who need more than just storage\u2014they need a smarter, more secure way to handle their data.</p>"},{"location":"ai_pipeline/ai-features/","title":"Ai features","text":""},{"location":"ai_pipeline/ai-features/#docsusageai-featuresmd","title":"docs/usage/ai-features.md","text":"<p>```markdown</p>"},{"location":"ai_pipeline/ai-features/#ai-features-guide","title":"AI Features Guide","text":"<p>Zenith Drive incorporates advanced AI capabilities to enhance your file management experience.</p>"},{"location":"ai_pipeline/ai-features/#contextual-chat","title":"Contextual Chat","text":""},{"location":"ai_pipeline/ai-features/#drive-wide-chat","title":"Drive-Wide Chat","text":"<p>Ask questions about all files in your drive:</p> <ul> <li>\"What's the total size of all my PDF documents?\"</li> <li>\"Find all files related to project Zenith\"</li> <li>\"Show me a summary of my financial documents\"</li> </ul>"},{"location":"ai_pipeline/ai-features/#folder-level-chat","title":"Folder-Level Chat","text":"<p>Restrict queries to specific folders:</p> <ul> <li>\"What images are in the Vacation folder?\"</li> <li>\"Summarize the documents in the Work/Reports folder\"</li> </ul>"},{"location":"ai_pipeline/ai-features/#document-specific-chat","title":"Document-Specific Chat","text":"<p>Ask questions about individual files:</p> <ul> <li>\"What are the key points in this PDF?\"</li> <li>\"Extract the main conclusions from this report\"</li> </ul>"},{"location":"ai_pipeline/ai-features/#highlight-based-features","title":"Highlight-Based Features","text":""},{"location":"ai_pipeline/ai-features/#text-selection","title":"Text Selection","text":"<p>Highlight any text in documents to:</p> <ul> <li>Get definitions of unfamiliar terms</li> <li>Receive explanations of complex concepts</li> <li>Request summaries of selected content</li> </ul>"},{"location":"ai_pipeline/ai-features/#image-analysis","title":"Image Analysis","text":"<p>Select portions of images to:</p> <ul> <li>Identify objects and people</li> <li>Extract text via OCR</li> <li>Analyze charts and diagrams</li> </ul>"},{"location":"ai_pipeline/ai-features/#table-operations","title":"Table Operations","text":"<p>Select table data to:</p> <ul> <li>Calculate aggregates (sum, average, etc.)</li> <li>Filter and sort data</li> <li>Create visualizations</li> </ul>"},{"location":"ai_pipeline/ai-features/#semantic-search","title":"Semantic Search","text":"<p>Search based on meaning rather than just keywords:</p> <ul> <li>\"Find documents about machine learning security\"</li> <li>\"Locate images from beach vacations\"</li> <li>\"Find spreadsheets with sales data\"</li> </ul>"},{"location":"ai_pipeline/ai-features/#cross-reference-features","title":"Cross-Reference Features","text":"<p>When the AI references information from files:</p> <ol> <li>Source citations are provided with links</li> <li>Clicking a citation opens the source file</li> <li>The relevant section is highlighted for context</li> </ol>"},{"location":"ai_pipeline/ai-features/#knowledge-graph-visualization","title":"Knowledge Graph Visualization","text":"<p>View connections between your files:</p> <ul> <li>Visual representation of related documents</li> <li>Topic-based clustering of content</li> <li>Timeline views of document creation and modification</li> </ul>"},{"location":"ai_pipeline/ai-features/#api-integration","title":"API Integration","text":"<p>Developers can access AI features through our API:</p> <p><code>``javascript // Example: Querying drive context const response = await fetch('/api/ai/query', {   method: 'POST',   headers: {     'Authorization':</code>Bearer ${token}`,     'Content-Type': 'application/json'   },   body: JSON.stringify({     query: \"Summarize my project documents\",     context: \"drive\", // or \"folder/:id\", \"file/:id\"     options: {       include_citations: true,       generate_visualization: false     }   }) });</p>"},{"location":"api/authentication/","title":"Authentication API for TheDrive","text":"<p>This document provides a detailed overview of the Authentication API implemented in TheDrive using FastAPI. The API includes endpoints for signup, login, logout, and retrieving user details. It uses JWT tokens for session management and bcrypt for password hashing.</p>"},{"location":"api/authentication/#1-endpoints-overview","title":"1. Endpoints Overview","text":"<p>The Authentication API consists of the following endpoints:</p> <ol> <li>POST /signup: Registers a new user.</li> <li>POST /login: Authenticates a user and issues a JWT token.</li> <li>POST /logout: Logs out the user by deleting the authentication cookie.</li> <li>GET /me: Fetches the current logged-in user's details.</li> </ol>"},{"location":"api/authentication/#2-endpoint-details","title":"2. Endpoint Details","text":""},{"location":"api/authentication/#21-post-signup","title":"2.1 POST /signup","text":"<p>This endpoint allows a new user to register by providing an email and a password. It checks if the email is already registered in the system and, if not, hashes the password and stores the new user's details in the PostgreSQL database.</p>"},{"location":"api/authentication/#request-body","title":"Request Body:","text":"<pre><code>json\n{\n    \"email\": \"user@example.com\",\n    \"password\": \"strongpassword\"\n}\n</code></pre> <p>Response:</p> <p>201 Created: User is successfully created.</p> <p>409 Conflict: Email is already registered.</p> <p>Example Response:</p> <p>{     \"email\": \"user@example.com\",     \"hashed_password\": \"$2b$12$...\",     \"id\": 1 }</p> <p>Code Example: @router.post(\"/signup\", response_model=schemas.User, status_code=status.HTTP_201_CREATED) def signup(user: schemas.UserCreate, db: Session = Depends(database.get_db)):     db_user = get_user(db, email=user.email)     if db_user:         raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=\"Email already registered\")     hashed_password = get_password_hash(user.password)     db_user = models.User(email=user.email, hashed_password=hashed_password)     db.add(db_user)     db.commit()     db.refresh(db_user)     return db_user</p> <p>2.2 POST /login</p> <p>This endpoint authenticates a user. It accepts the email and password via the OAuth2PasswordRequestForm and returns an access token (JWT) in a cookie for session management.</p> <p>Request Body: {     \"username\": \"user@example.com\",     \"password\": \"strongpassword\" }</p> <p>Response:</p> <p>200 OK: Login successful, JWT token is issued and set in the access_token cookie.</p> <p>401 Unauthorized: Incorrect email or password.</p> <p>Example Response:</p> <p>{     \"message\": \"Login successful\" }</p> <p>Code Example:</p> <pre><code>@router.post(\"/login\")\ndef login(response: Response, db: Session = Depends(database.get_db), form_data: OAuth2PasswordRequestForm = Depends()):\n    user = get_user(db, email=form_data.username)\n    if not user or not verify_password(form_data.password, user.hashed_password):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect email or password\",\n        )\n    access_token = create_access_token(data={\"sub\": user.email})\n\n    response.set_cookie(\n        key=\"access_token\", \n        value=access_token,  # UPDATED: No \"Bearer \" prefix\n        httponly=True,\n        samesite='lax',\n    )\n    return {\"message\": \"Login successful\"}\n</code></pre> <p>2.3 POST /logout</p> <p>This endpoint logs out the current user by deleting the access_token cookie. It invalidates the user's session.</p> <p>Request Body:</p> <p>None</p> <p>Response:</p> <p>200 OK: Logout successful, access_token cookie is deleted.</p> <p>Example Response:</p> <p>{     \"message\": \"Logout successful\" }</p> <p>Code Example:</p> <pre><code>@router.post(\"/logout\")\ndef logout(response: Response):\n    response.delete_cookie(\"access_token\")\n    return {\"message\": \"Logout successful\"}\n</code></pre> <p>2.4 GET /me</p> <p>This endpoint fetches the details of the currently logged-in user. The access_token is retrieved from the cookie, and the user's identity is validated using the token. The user's details are returned if the token is valid.</p> <p>Response :</p> <p>200 OK : User details are returned.</p> <p>401 Unauthorized : If the access_token is not provided or is invalid.</p> <p>Example Response:</p> <pre><code>{\n    \"email\": \"user@example.com\",\n    \"id\": 1\n}\n</code></pre> <p>Code Example:</p> <pre><code>@router.get(\"/me\", response_model=schemas.User)\ndef read_users_me(current_user: models.User = Depends(get_current_user)):\n    \"\"\"\n    Fetch the current logged in user by verifying the cookie.\n    \"\"\"\n    return current_user\n\n\n</code></pre> <ol> <li>Security Features 3.1 Password Hashing</li> </ol> <p>bcrypt is used for password hashing. When a user registers, their password is hashed and stored in the database. During login, the entered password is compared to the hashed version in the database to verify the credentials.</p> <p>3.2 Token-Based Authentication</p> <p>The system uses JWT tokens to manage user sessions. The JWT token is generated after successful login and stored in a secure, HTTP-only cookie to prevent XSS attacks. The token includes the user's email (as a claim) and an expiration time.</p> <p>Token Expiration: The token expires after a pre-configured amount of time (e.g., 30 minutes), requiring the user to log in again.</p> <p>3.3 Cookie Security</p> <p>The JWT token is stored in an HTTP-only cookie, which makes it inaccessible to JavaScript running in the user's browser. This prevents XSS attacks by ensuring that malicious scripts cannot access the authentication token.</p> <p>SameSite Cookie Attribute: The SameSite attribute is set to lax to provide protection against cross-site request forgery (CSRF) attacks.</p> <p>3.4 Session Management</p> <p>Login: When a user logs in, the backend generates a JWT token and stores it in the user's cookies.</p> <p>Logout: The /logout endpoint removes the access_token cookie, effectively ending the user's session.</p> <p>User Data: The /me endpoint uses the token to retrieve the current user's data from the database.</p> <ol> <li>Data Flow for Authentication 4.1 Sign-up Flow</li> </ol> <p>User enters their email and password.</p> <p>The backend checks if the email already exists in the database.</p> <p>If the email is not registered, the backend hashes the password and stores the user details in the database.</p> <p>The backend returns the user data (without the password) as a response.</p> <p>4.2 Login Flow</p> <p>User enters their email and password.</p> <p>The backend checks if the provided credentials are correct by verifying the password hash.</p> <p>If valid, the backend generates a JWT token and sends it to the user in an HTTP-only cookie.</p> <p>The user can now access protected endpoints by sending the token with subsequent requests.</p> <p>4.3 Logout Flow</p> <p>User clicks on the Logout button, which triggers the /logout endpoint.</p> <p>The backend deletes the access_token cookie.</p> <p>The user is logged out and must log in again to obtain a new token.</p> <p>4.4 Fetch User Data Flow</p> <p>User makes a GET /me request.</p> <p>The backend retrieves the access_token from the user's cookies.</p> <p>The backend decodes the token to verify the user's identity.</p> <p>The user's data is returned in the response.</p> <ol> <li>Conclusion</li> </ol> <p>TheDrive\u2019s Authentication API ensures that users can securely sign up, log in, and manage their sessions with JWT tokens stored in cookies. Passwords are hashed using bcrypt, ensuring that sensitive data is not exposed. The JWT-based authentication system provides secure session management, while cookie security features like HTTP-only and SameSite protect against common web security threats such as XSS and CSRF.</p> <p>This authentication system enables secure and efficient user management for TheDrive, with easy-to-understand data flows for logging in, logging out, and fetching user details.</p>"},{"location":"api/authentication/#explanation","title":"Explanation:","text":"<ul> <li>The Authentication API consists of endpoints for signup, login, logout, and user details retrieval.</li> <li>JWT tokens are used for session management, and bcrypt ensures secure password storage.</li> <li>Cookie security with HTTP-only and SameSite attributes protects against common web vulnerabilities.</li> </ul>"},{"location":"api/chat_endpoints/","title":"Chat API for TheDrive","text":"<p>The Chat API in TheDrive enables users to interact with their files through intelligent queries, powered by GraphRAG. The API handles chat-based queries, performs context validation, and provides users with answers based on AI processing. It also checks the status of AI processing for each user.</p>"},{"location":"api/chat_endpoints/#1-endpoints-overview","title":"1. Endpoints Overview","text":"<p>The following endpoints are available in the Chat API:</p> <ol> <li>POST /query: Handles chat queries related to files and folders, using GraphRAG to provide intelligent answers.</li> <li>GET /status: Provides the status of the user's chat readiness, including the number of files processed by AI.</li> </ol>"},{"location":"api/chat_endpoints/#2-endpoint-details","title":"2. Endpoint Details","text":""},{"location":"api/chat_endpoints/#21-post-query","title":"2.1 POST /query","text":"<p>This endpoint handles user chat queries. The query is processed by the GraphRAG module, which generates a response based on the file context or AI processing.</p>"},{"location":"api/chat_endpoints/#request-body","title":"Request Body:","text":"<p>A sample request body is like : json</p> <pre><code>{\n    \"query\": \"What is the total revenue from last year's financial report?\",\n    \"context\": \"file-12345\"  // Optional: Can specify a file/folder context\n}\n</code></pre> <p>query: The user's question or query.</p> <p>context: The file or folder context in which the query is being asked (optional).</p> <p>Response:</p> <p>200 OK: Returns the answer to the query and the relevant sources.</p> <p>500 Internal Server Error: If an error occurs during chat processing.</p> <p>Example Response:</p> <pre><code>{\n    \"answer\": \"The total revenue from last year's financial report is $5 million.\",\n    \"sources\": [\n        {\n            \"id\": \"community-1\",\n            \"name\": \"Knowledge Cluster 1\",\n            \"relevance\": 0.95\n        },\n        {\n            \"id\": \"community-2\",\n            \"name\": \"Knowledge Cluster 2\",\n            \"relevance\": 0.90\n        }\n    ]\n}\n</code></pre> <p>Code Example :</p> <pre><code>@router.post(\"/query\", response_model=schemas.ChatResponse)\nasync def handle_chat_query(\n    query: schemas.ChatQuery,\n    db: Session = Depends(database.get_db),\n    current_user: models.User = Depends(get_current_user)\n):\n    \"\"\"Handle chat queries with GraphRAG\"\"\"\n    try:\n        if not GRAPHRAG_AVAILABLE:\n            # Fallback to mock response\n            return {\n                \"answer\": f\"GraphRAG not available. Mock response for: '{query.query}'\",\n                \"sources\": []\n            }\n\n        # Validate context if specified\n        if query.context and query.context != \"drive\":\n            # Check if user owns the folder/file\n            item = db.query(models.FileSystemItem).filter_by(\n                id=query.context,\n                owner_id=current_user.id\n            ).first()\n            if not item:\n                raise HTTPException(status_code=404, detail=\"Context not found or access denied\")\n\n        # Get user's GraphRAG chatter\n        chatter = await get_user_chatter(current_user.id)\n\n        # Process the query\n        result = await chatter.ask_question(query.query, \"hybrid\")\n\n        if 'error' in result:\n            raise HTTPException(status_code=500, detail=result['error'])\n\n        # Format sources from GraphRAG result\n        sources = []\n        if result.get('graphrag_details'):\n            # Extract source information from GraphRAG\n            communities = result['graphrag_details'].get('selected_communities', [])\n            for i, community in enumerate(communities[:3]):  # Top 3 sources\n                sources.append({\n                    \"id\": f\"community-{i}\",\n                    \"name\": f\"Knowledge Cluster {i+1}\",\n                    \"relevance\": result.get('confidence', 0.5)\n                })\n\n        return schemas.ChatResponse(\n            answer=result['answer'],\n            sources=sources\n        )\n\n    except HTTPException:\n        raise\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Chat processing error: {str(e)}\")\n</code></pre> <p>2.2 GET /status</p> <p>This endpoint retrieves the chat readiness status for a user, including the number of files that have been processed by AI. It helps users understand if their files are ready for interaction with GraphRAG.</p> <p>Response:</p> <ul> <li> <p>200 OK: Returns the current status of chat readiness.</p> </li> <li> <p>503 Service Unavailable: If GraphRAG service is not available.</p> </li> </ul> <p>Example Response:</p> <pre><code>{\n    \"ready\": true,\n    \"message\": \"Chat ready with 5 processed files\",\n    \"processed_files\": 5,\n    \"total_files\": 10,\n    \"processing_percentage\": 50\n}\n</code></pre> <p>Code Example:</p> <pre><code>@router.get(\"/status\")\nasync def get_chat_status(\n    db: Session = Depends(database.get_db),\n    current_user: models.User = Depends(get_current_user)\n):\n    \"\"\"Get chat readiness status for user\"\"\"\n    if not GRAPHRAG_AVAILABLE:\n        return {\n            \"ready\": False,\n            \"message\": \"GraphRAG service not available\",\n            \"processed_files\": 0,\n            \"total_files\": 0\n        }\n\n    # Count user's files and processing status\n    files = db.query(models.FileSystemItem).filter_by(\n        owner_id=current_user.id,\n        type=\"file\"\n    ).all()\n\n    total_files = len(files)\n    processed_files = sum(1 for f in files if getattr(f, 'ai_processed', False))\n\n    return {\n        \"ready\": processed_files &gt; 0,\n        \"message\": f\"Chat ready with {processed_files} processed files\" if processed_files &gt; 0 else \"No files processed yet\",\n        \"processed_files\": processed_files,\n        \"total_files\": total_files,\n        \"processing_percentage\": (processed_files / total_files * 100) if total_files &gt; 0 else 0\n    }\n</code></pre> <ol> <li>GraphRAG Integration 3.1 What is GraphRAG?</li> </ol> <p>GraphRAG is an AI-powered tool for processing and querying PDF documents and other types of data. It enables users to ask context-aware questions and receive answers by analyzing document contents, relationships, and AI-generated insights.</p> <ul> <li> <p>PDFChatter: This is a module in GraphRAG that is used to process PDF documents and generate responses based on their content.</p> </li> <li> <p>Graph-Based Queries: The system uses graph-based algorithms to understand the relationships between entities within the documents and provide relevant answers.</p> </li> </ul> <p>3.2 Chat Handling</p> <p>Each user has a GraphRAG chatter instance that interacts with documents and data specific to that user.</p> <p>The PDFChatter processes files (if available) and returns relevant answers to the queries asked by users.</p> <p>The system allows for context-specific queries, where users can ask questions related to a specific file or document.</p> <p>3.3 Example Workflow</p> <ul> <li> <p>User Query: A user asks, \"What is the total revenue from last year's financial report?\"</p> </li> <li> <p>Context Verification: If the query specifies a file context, the system ensures the user owns the file.</p> </li> <li> <p>GraphRAG Processing: The PDFChatter analyzes the document to extract relevant information.</p> </li> <li> <p>Response: The system returns the extracted answer and relevant sources from the document, if available.</p> </li> <li> <p>Error Handling and Fallbacks</p> </li> </ul> <p>GraphRAG Not Available: If the GraphRAG service is not available, the system will provide a mock response indicating that GraphRAG is unavailable.</p> <p>Context Not Found: If the specified context (e.g., a file ID) is invalid or the user does not own the file, an HTTP 404 error is returned.</p> <p>Query Processing Error: Any unexpected error during query processing (e.g., failure in AI processing) will result in a 500 Internal Server Error with a detailed error message.</p> <ol> <li>Conclusion</li> </ol> <p>The Chat API in TheDrive enables users to interact with their files through contextual AI-powered queries, leveraging GraphRAG for document-based insights. By integrating GraphRAG, TheDrive provides a unique and intelligent way for users to retrieve information from their files. This system allows for both file-specific queries and semantic search, providing users with accurate, context-aware answers while ensuring smooth integration with AI processing and graph-based models.</p> <p>The API is designed to handle errors gracefully and provide fallback responses when necessary, ensuring a seamless user experience even when the underlying AI service is unavailable.</p>"},{"location":"api/chat_endpoints/#key-sections","title":"Key Sections:","text":"<ol> <li>POST /query: Handles AI-driven chat queries, processes context, and returns answers.</li> <li>GET /status: Provides the status of the AI processing for the user\u2019s files.</li> <li>GraphRAG Integration: Explains how GraphRAG and PDFChatter are used to process user queries and provide insights.</li> <li>Error Handling: Describes how the system manages errors, such as unavailable services or invalid contexts.</li> </ol> <p>This Chat API documentation gives users and developers a clear understanding of how the AI-powered chat feature works in TheDrive, ensuring seamless interaction with files and AI models.</p>"},{"location":"api/file-operations/","title":"File Operations API for TheDrive","text":"<p>This document provides a detailed overview of the File Operations API for TheDrive. It includes endpoints for file uploads, folder creation, file management, AI processing, and other file-related operations.</p>"},{"location":"api/file-operations/#1-endpoints-overview","title":"1. Endpoints Overview","text":"<p>The following endpoints are available in the File Operations API:</p> <ol> <li>POST /upload: Uploads a file to AWS S3, stores metadata in the database, and optionally triggers AI processing.</li> <li>POST /folder: Creates a new folder within a user's file system.</li> <li>PUT /item/{item_id}: Renames an existing file or folder.</li> <li>DELETE /item/{item_id}: Deletes a file or folder, including removal from AWS S3 and related AI indexes.</li> <li>GET /items: Retrieves a list of files and folders for a specific user and parent folder.</li> <li>GET /item/{item_id}/view-link: Generates a pre-signed URL for viewing a file.</li> <li>GET /item/{item_id}/ai-status: Retrieves the AI processing status for a file.</li> </ol>"},{"location":"api/file-operations/#2-endpoint-details","title":"2. Endpoint Details","text":""},{"location":"api/file-operations/#21-post-upload","title":"2.1 POST /upload","text":"<p>This endpoint allows users to upload files to the system. The file is stored in AWS S3, and metadata (such as file name, type, and S3 key) is saved in the PostgreSQL database. If GraphRAG (AI processing) is available, the file will be processed in the background.</p>"},{"location":"api/file-operations/#request-body","title":"Request Body:","text":"<ul> <li>parentId: The ID of the parent folder (can be 'root' for the main folder).</li> <li>file: The file to be uploaded.</li> </ul>"},{"location":"api/file-operations/#response","title":"Response:","text":"<ul> <li>201 Created: Successfully uploaded the file.</li> </ul> <p>Example Response:</p> <pre><code>json\n{\n    \"id\": \"file-12345\",\n    \"name\": \"document.pdf\",\n    \"type\": \"file\",\n    \"s3_key\": \"user123/file-12345/document.pdf\",\n    \"size_bytes\": 102400,\n    \"mime_type\": \"application/pdf\",\n    \"ai_processed\": false,\n    \"ai_processing_status\": \"pending\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.post(\"/upload\", response_model=schemas.FileSystemItem, status_code=201)\ndef upload_file(\n    parentId: str,\n    background_tasks: BackgroundTasks,\n    file: UploadFile = File(...),\n    db: Session = Depends(database.get_db),\n    current_user: models.User = Depends(get_current_user)\n):\n    file_id = f\"file-{uuid.uuid4()}\"\n    s3_key = f\"{current_user.id}/{file_id}/{file.filename}\"\n\n    # Read file content into memory safely\n    contents = file.file.read()\n    file_size = len(contents)\n\n    # Upload to S3\n    s3_service.upload_file_obj(\n        io.BytesIO(contents),\n        settings.S3_BUCKET_NAME,\n        s3_key,\n        extra_args={\n            \"ContentType\": file.content_type,\n            \"ContentDisposition\": \"inline\",\n        }\n    )\n\n    # Save metadata to DB\n    new_file = models.FileSystemItem(\n        id=file_id,\n        name=file.filename,\n        type=\"file\",\n        s3_key=s3_key,\n        mime_type=file.content_type,\n        size_bytes=file_size,\n        owner_id=current_user.id,\n        parent_id=None if parentId == 'root' else parentId,\n        ai_processed=False,\n        ai_processing_status=\"pending\"\n    )\n\n    db.add(new_file)\n    db.commit()\n    db.refresh(new_file)\n\n    # Process file for GraphRAG automatically\n    if GRAPHRAG_AVAILABLE:\n        background_tasks.add_task(\n            process_file_for_graphrag,\n            file_id=new_file.id,\n            s3_key=s3_key,\n            user_id=current_user.id,\n            filename=file.filename\n        )\n\n    return new_file\n</code></pre> <p>2.2 POST /folder</p> <p>This endpoint creates a new folder within a user's file system. The folder is saved in the database, and the parent folder (if any) is specified.</p> <pre><code>Request Body:\n{\n    \"name\": \"New Folder\",\n    \"parentId\": \"root\"\n}\n</code></pre> <p>Response:</p> <pre><code>201 Created: Successfully created the folder.\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"id\": \"folder-12345\",\n    \"name\": \"New Folder\",\n    \"type\": \"folder\",\n    \"owner_id\": 1,\n    \"parent_id\": \"root\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.post(\"/folder\", response_model=schemas.FileSystemItem, status_code=201)\ndef create_folder(folder: schemas.FolderCreate, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    parent_db_id = None if folder.parentId == 'root' else folder.parentId\n\n    # Check for duplicates\n    existing = db.query(models.FileSystemItem).filter_by(\n        owner_id=current_user.id, parent_id=parent_db_id, name=folder.name\n    ).first()\n    if existing:\n        raise HTTPException(status_code=409, detail=\"An item with this name already exists\")\n\n    new_folder = models.FileSystemItem(\n        id=f\"folder-{uuid.uuid4()}\", name=folder.name, type=\"folder\",\n        owner_id=current_user.id, parent_id=parent_db_id\n    )\n    db.add(new_folder)\n    db.commit()\n    db.refresh(new_folder)\n    return new_folder\n</code></pre> <p>2.3 PUT /item/{item_id}</p> <p>This endpoint allows users to rename a file or folder. It ensures that the new name doesn't conflict with other existing items in the same directory.</p> <p>Request Body:</p> <pre><code>{\n    \"name\": \"Updated File Name\"\n}\n</code></pre> <p>Response:</p> <pre><code>200 OK: Successfully renamed the item.\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"id\": \"file-12345\",\n    \"name\": \"Updated File Name\",\n    \"type\": \"file\",\n    \"s3_key\": \"user123/file-12345/document.pdf\",\n    \"size_bytes\": 102400,\n    \"mime_type\": \"application/pdf\",\n    \"ai_processed\": false,\n    \"ai_processing_status\": \"pending\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.put(\"/item/{item_id}\", response_model=schemas.FileSystemItem)\ndef rename_item(item_id: str, item_update: schemas.ItemUpdate, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    item = db.query(models.FileSystemItem).filter_by(id=item_id, owner_id=current_user.id).first()\n    if not item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    # Check for name conflicts\n    existing = db.query(models.FileSystemItem).filter(\n        models.FileSystemItem.id != item_id,\n        models.FileSystemItem.parent_id == item.parent_id,\n        models.FileSystemItem.name == item_update.name,\n        models.FileSystemItem.owner_id == current_user.id\n    ).first()\n    if existing:\n        raise HTTPException(status_code=409, detail=\"An item with this name already exists\")\n\n    item.name = item_update.name\n    db.commit()\n    db.refresh(item)\n    return item\n</code></pre> <p>2.4 DELETE /item/{item_id}</p> <p>This endpoint deletes a file or folder. If the item is a file, it will also be removed from AWS S3.</p> <p>Response:</p> <pre><code>204 No Content: Successfully deleted the item.\n</code></pre> <p>Example Code:</p> <pre><code>@router.delete(\"/item/{item_id}\", status_code=204)\ndef delete_item(item_id: str, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    item = db.query(models.FileSystemItem).filter_by(id=item_id, owner_id=current_user.id).first()\n    if not item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    if item.type == 'file' and item.s3_key:\n        s3_service.delete_file(settings.S3_BUCKET_NAME, item.s3_key)\n        # Add background task to remove from AI indexes\n\n    db.delete(item)\n    db.commit()\n</code></pre> <p>2.5 GET /item/{item_id}/view-link</p> <p>This endpoint generates a pre-signed URL for viewing a file directly from AWS S3. This URL can be used to access the file for a limited time.</p> <p>Response:</p> <pre><code>200 OK: Returns the pre-signed URL for viewing the file.\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"url\": \"https://example.com/s3_presigned_url\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.get(\"/item/{item_id}/view-link\", response_model=schemas.ViewLinkResponse)\ndef get_view_link(item_id: str, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    item = db.query(models.FileSystemItem).filter_by(id=item_id, owner_id=current_user.id).first()\n    if not item or item.type != 'file' or not item.s3_key:\n        raise HTTPException(status_code=404, detail=\"File not found or is not a viewable file.\")\n\n    url = s3_service.generate_presigned_url(settings.S3_BUCKET_NAME, item.s3_key)\n    return {\"url\": url}\n</code></pre> <p>2.6 GET /item/{item_id}/ai-status</p> <p>This endpoint allows users to retrieve the AI processing status of a file. The AI processing status includes information such as whether the file has been processed, the number of entities and relationships detected, and any processing errors.</p> <p>Response:</p> <pre><code>200 OK: Returns the AI processing status of the file.\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"file_id\": \"file-12345\",\n    \"processed\": true,\n    \"status\": \"completed\",\n    \"entities\": 20,\n    \"relationships\": 10,\n    \"communities\": 5\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.get(\"/item/{item_id}/ai-status\")\ndef get_ai_status(item_id: str, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    \"\"\"Get AI processing status for a file\"\"\"\n    item = db.query(models.FileSystemItem).filter_by(id=item_id, owner_id=current_user.id).first()\n    if not item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    return {\n        \"file_id\": item_id,\n        \"processed\": getattr(item, 'ai_processed', False),\n        \"status\": getattr(item, 'ai_processing_status', 'pending'),\n        \"entities\": getattr(item, 'ai_entities', 0),\n        \"relationships\": getattr(item, 'ai_relationships', 0),\n        \"communities\": getattr(item, 'ai_communities', 0)\n    }\n</code></pre> <ol> <li>Conclusion</li> </ol> <p>The File Operations API for TheDrive provides robust functionality for managing files and folders, including uploading, creating, renaming, deleting, and viewing files. Additionally, it supports AI processing for file contents and generates presigned URLs for secure file access. The integration with AWS S3 and PostgreSQL ensures efficient and scalable file storage and metadata management.</p> <p>This API is designed to support users in managing their digital assets securely and intelligently, with the added benefit of AI-driven insights and graph-based search features.</p>"},{"location":"api/file-operations/#key-sections","title":"Key Sections:","text":"<ol> <li>File Upload: Handles file upload to AWS S3 and metadata saving to PostgreSQL.</li> <li>Folder Creation: Creates folders within the user\u2019s file system.</li> <li>File Management: Renaming and deletion of files, including removal from S3.</li> <li>AI Processing: Includes endpoints for checking the AI status of files and generating pre-signed URLs for file access.</li> </ol>"},{"location":"architecture/data-flow/","title":"Data Flow in TheDrive","text":"<p>This document provides a detailed explanation of the data flow in TheDrive, including how data moves through the system from the frontend to the backend, storage, AI processing, and how user queries are handled.</p>"},{"location":"architecture/data-flow/#1-high-level-data-flow","title":"1. High-Level Data Flow","text":"<p>TheDrive follows a user-centered flow where data is processed and managed through several stages:</p> <ol> <li>User Uploads File:</li> <li>The user uploads a file via the frontend interface.</li> <li> <p>The file is processed, stored, and metadata is stored in the backend.</p> </li> <li> <p>Metadata Storage:</p> </li> <li>File metadata (e.g., file name, type, size) is saved to PostgreSQL.</li> <li> <p>The file is uploaded to AWS S3 and its S3 key is saved in the database.</p> </li> <li> <p>AI Processing:</p> </li> <li>Once the file is uploaded, the system triggers AI processing to analyze the contents (e.g., text, images).</li> <li> <p>Metadata related to AI analysis (e.g., entities, relationships, communities) is stored in the database.</p> </li> <li> <p>User Queries Files:</p> </li> <li>The user interacts with the system by querying files via the frontend interface.</li> <li>The backend processes the query by referencing the file metadata and AI-driven insights (e.g., semantic search, AI-based content retrieval).</li> </ol>"},{"location":"architecture/data-flow/#2-detailed-data-flow","title":"2. Detailed Data Flow","text":""},{"location":"architecture/data-flow/#21-file-upload-process","title":"2.1 File Upload Process","text":""},{"location":"architecture/data-flow/#user-action","title":"User Action:","text":"<ul> <li>The user selects a file to upload through the Next.js frontend.</li> <li>The file is sent via an HTTP POST request to the backend API (FastAPI).</li> </ul>"},{"location":"architecture/data-flow/#backend-action","title":"Backend Action:","text":"<ul> <li>FastAPI receives the file and triggers the AWS S3 upload process.</li> <li>The file is stored in an S3 bucket. S3 generates a unique storage key for the file, which is used for future access and reference.</li> </ul>"},{"location":"architecture/data-flow/#metadata-storage","title":"Metadata Storage:","text":"<ul> <li>Once the file is uploaded to S3, metadata (such as file name, file type, file size, and S3 key) is stored in the PostgreSQL database.</li> <li>A new entry is created in the FileSystemItem table, which references the uploaded file.</li> </ul> <p>Example Data Flow: 1. User uploads file via frontend. 2. File is sent to FastAPI backend. 3. FastAPI stores the file in AWS S3. 4. File metadata is inserted into PostgreSQL.</p> <pre><code>python\n# FastAPI backend receives the file\ndef upload_file(file: UploadFile = File(...), db: Session = Depends(get_db)):\n    s3_key = upload_to_s3(file)  # Upload file to AWS S3\n    file_metadata = {\n        'name': file.filename,\n        'type': 'file',  # or 'folder'\n        's3_key': s3_key,\n        'size_bytes': len(file.file.read())\n    }\n    # Store metadata in PostgreSQL\n    new_file = models.FileSystemItem(**file_metadata)\n    db.add(new_file)\n    db.commit()\n    return {\"message\": \"File uploaded successfully!\"}\n</code></pre>"},{"location":"architecture/overview/","title":"TheDrive - System Architecture Overview","text":"<p>TheDrive is an AI-powered cloud storage system that integrates secure file storage, AI-driven content analysis, and intelligent querying for users. The system is designed with a microservices architecture that ensures scalability, security, and flexibility.</p>"},{"location":"architecture/overview/#1-high-level-architecture","title":"1. High-Level Architecture","text":"<p>TheDrive\u2019s architecture is divided into several key components:</p>"},{"location":"architecture/overview/#11-frontend-nextjs","title":"1.1 Frontend (Next.js)","text":"<ul> <li>Role: Provides a responsive user interface for file management, querying, and interacting with AI-powered features.</li> <li>Technologies: Next.js for server-side rendering and static page generation, Tailwind CSS for styling, and Axios for API requests.</li> </ul>"},{"location":"architecture/overview/#12-backend-fastapi","title":"1.2 Backend (FastAPI)","text":"<ul> <li>Role: Handles file management, user authentication, and interactions with AI models. The backend serves as the main API layer for the system.</li> <li>Technologies: FastAPI for building the RESTful API, SQLAlchemy for database interaction, and JWT for authentication.</li> </ul>"},{"location":"architecture/overview/#13-cloud-storage-aws-s3","title":"1.3 Cloud Storage (AWS S3)","text":"<ul> <li>Role: Stores files securely in the cloud with high scalability and availability.</li> <li>Technologies: AWS S3 for scalable file storage, leveraging IAM for secure access.</li> </ul>"},{"location":"architecture/overview/#14-database-postgresql-neo4j","title":"1.4 Database (PostgreSQL &amp; Neo4j)","text":"<ul> <li>PostgreSQL: Used for storing user credentials, file metadata, and other structured data.</li> <li>Neo4j: A graph database that models relationships between files and metadata, enabling AI-based queries and semantic search.</li> </ul>"},{"location":"architecture/overview/#15-ai-layer","title":"1.5 AI Layer","text":"<ul> <li>Role: Processes files for AI-driven insights, such as text extraction, entity recognition, and semantic search.</li> <li>Technologies: GraphRAG for processing documents, PyTorch/TensorFlow for machine learning models, and OCR for image processing.</li> </ul>"},{"location":"architecture/overview/#2-security-and-privacy","title":"2. Security and Privacy","text":"<ul> <li>JWT Authentication: Used for user authentication and maintaining secure sessions.</li> <li>End-to-End Encryption (E2EE): All user data is encrypted at rest and in transit (future enhancement, currently in progress).</li> <li>Data Segregation: Each user's data is isolated to ensure privacy and security.</li> </ul>"},{"location":"architecture/overview/#3-data-flow","title":"3. Data Flow","text":"<ol> <li>File Upload: Files are uploaded via the frontend, stored in AWS S3, and metadata is saved in PostgreSQL.</li> <li>AI Processing: Once a file is uploaded, it is processed by the AI layer to extract insights (e.g., entities, relationships).</li> <li>User Queries: Users can query files via the frontend, with AI-driven results fetched from PostgreSQL and Neo4j.</li> </ol>"},{"location":"architecture/overview/#conclusion","title":"Conclusion","text":"<p>TheDrive combines secure cloud storage, AI-powered processing, and advanced querying capabilities to offer users an intelligent and scalable solution for managing their digital assets. The system is built with a modular architecture that ensures flexibility, security, and high performance.</p>"},{"location":"architecture/security/","title":"Security Implementation in TheDrive","text":"<p>TheDrive focuses on ensuring user authentication and authorization within the system, protecting sensitive information through token-based authentication and secure password hashing. While end-to-end encryption (E2EE) is not implemented at this stage, user input is filtered during login to prevent unauthorized access.</p> <p>This document provides a detailed explanation of the login system, token creation, user authentication process, as well as the database schema for storing user credentials and file system metadata.</p>"},{"location":"architecture/security/#1-user-authentication","title":"1. User Authentication","text":""},{"location":"architecture/security/#11-password-hashing","title":"1.1 Password Hashing","text":"<p>To ensure that user passwords are stored securely, TheDrive uses bcrypt hashing for passwords. bcrypt is a widely adopted and secure password hashing algorithm that makes it difficult to reverse the hash into the original password.</p>"},{"location":"architecture/security/#functions","title":"Functions:","text":"<ul> <li> <p><code>verify_password(plain_password: str, hashed_password: str) -&gt; bool</code>:    This function compares the plain text password with the hashed password stored in the database.</p> </li> <li> <p><code>get_password_hash(password: str) -&gt; str</code>:    This function takes a plain password and hashes it using bcrypt before storing it in the database.</p> </li> </ul> <p>Example Code:</p> <pre><code>python\nfrom passlib.context import CryptContext\n\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\ndef verify_password(plain_password: str, hashed_password: str) -&gt; bool:\n    return pwd_context.verify(plain_password, hashed_password)\n\ndef get_password_hash(password: str) -&gt; str:\n    return pwd_context.hash(password)\n</code></pre> <p>1.2 Token Generation (JWT)</p> <p>TheDrive uses JSON Web Tokens (JWT) for session management and user authentication. Once the user logs in, they are issued a JWT token, which is used for subsequent requests to authenticate the user.</p> <p>Functions:</p> <ul> <li> <p><code>create_access_token(data: dict, expires_delta: Optional[timedelta] = None)</code> : This function generates a JWT token, encoding user information (email) along with an expiration time. If no expiration is provided, it defaults to the value defined in the settings.</p> </li> <li> <p><code>get_token_from_cookie(request: Request) -&gt; Optional[str]</code> : This function retrieves the JWT token from the cookies sent by the client. The token is stored in a cookie as part of the authentication flow.</p> </li> </ul> <p>Example Code:</p> <pre><code>from datetime import datetime, timedelta\nfrom typing import Optional\nfrom jose import JWTError, jwt\nfrom fastapi import Request\nfrom .config import settings\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=\"HS256\")\n    return encoded_jwt\n\ndef get_token_from_cookie(request: Request) -&gt; Optional[str]:\n    return request.cookies.get(\"access_token\")\n</code></pre> <p>1.3 User Authentication Flow</p> <p>Once a user logs in, a JWT token is created and returned in the response. This token is then stored in the client's cookies and used for authentication in subsequent requests.</p> <p>User Login and Token Creation:</p> <p>User enters their email and password.</p> <p>The password is verified by comparing the plain password with the hashed password stored in the database.</p> <p>If the credentials are valid, an access token (JWT) is generated and sent back to the client.</p> <p>Token Validation :</p> <p>For subsequent requests, the token is sent with the request (stored in the cookies).</p> <p>The token is decoded using the SECRET_KEY.</p> <p>If the token is valid and not expired, the user is authenticated, and their details are fetched from the database.</p> <p>Example Code for Token Validation:</p> <pre><code>from fastapi import Depends, HTTPException, status\nfrom sqlalchemy.orm import Session\nfrom .database import get_db\nfrom . import models\n\ndef get_current_user(token: str = Depends(get_token_from_cookie), db: Session = Depends(get_db)):\n    if token is None:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\")\n\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n    )\n    try:\n        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[\"HS256\"])\n        email: str = payload.get(\"sub\")\n        if email is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n    user = db.query(models.User).filter(models.User.email == email).first()\n    if user is None:\n        raise credentials_exception\n    return user\n</code></pre> <ol> <li>Database Schema</li> </ol> <p>The User and FileSystemItem models are designed to store essential information about users and their uploaded files. These models are implemented using SQLAlchemy ORM and are used to persist data in the PostgreSQL database.</p> <p>2.1 User Model</p> <p>The User model is used to store user credentials (email and hashed password) and their associated files.</p> <p>Fields:</p> <p><code>email</code> : The unique email address used by the user for login.</p> <p><code>hashed_password</code> : The hashed password of the user.</p> <p><code>items</code> : The relationship between users and their FileSystemItem (files or folders).</p> <p>User Model Example:</p> <pre><code>from sqlalchemy import Column, Integer, String\nfrom sqlalchemy.orm import relationship\nfrom .database import Base\n\nclass User(Base):\n    __tablename__ = \"users\"\n    id = Column(Integer, primary_key=True, index=True)\n    email = Column(String, unique=True, index=True, nullable=False)\n    hashed_password = Column(String, nullable=False)\n    items = relationship(\"FileSystemItem\", back_populates=\"owner\")\n</code></pre> <p>2.2 FileSystemItem Model</p> <p>The FileSystemItem model is used to store metadata about the files and folders uploaded by users. It tracks important file properties such as the file name, size, type, and S3 storage key.</p> <p>Fields:</p> <p>name: The name of the file or folder.</p> <p>type: The type of the item (either a file or a folder).</p> <p>s3_key: The key used to access the file stored in AWS S3.</p> <p>owner_id: The ID of the user who owns the file or folder.</p> <p>parent_id: The ID of the parent folder (if the item is a file within a folder).</p> <p>ai_processed: Boolean flag indicating whether the file has been processed by the AI pipeline.</p> <p>ai_processing_status: The current status of the AI processing (e.g., pending, completed).</p> <p>ai_entities: The number of entities detected by the AI in the file.</p> <p>ai_relationships: The number of relationships detected by the AI in the file.</p> <p>ai_communities: The number of communities detected by the AI in the file.</p> <p>FileSystemItem Model Example:</p> <pre><code>from sqlalchemy import Column, Integer, String, ForeignKey, Enum, Boolean\nfrom sqlalchemy.orm import relationship\nfrom .database import Base\n\nclass FileSystemItem(Base):\n    __tablename__ = \"filesystem_items\"\n    id = Column(String, primary_key=True, index=True)\n    name = Column(String, index=True)\n    type = Column(Enum(\"file\", \"folder\", name=\"item_type_enum\"), nullable=False)\n    s3_key = Column(String, unique=True, nullable=True)\n    mime_type = Column(String, nullable=True)\n    size_bytes = Column(Integer, nullable=True)\n\n    # AI processing fields\n    ai_processed = Column(Boolean, default=False)\n    ai_processing_status = Column(String, nullable=True, default=\"pending\")\n    ai_entities = Column(Integer, default=0)\n    ai_relationships = Column(Integer, default=0)\n    ai_communities = Column(Integer, default=0)\n\n    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n    parent_id = Column(String, ForeignKey(\"filesystem_items.id\"), nullable=True)\n\n    owner = relationship(\"User\", back_populates=\"items\")\n    children = relationship(\"FileSystemItem\", backref=\"parent\", remote_side=[id], cascade=\"all, delete-orphan\", single_parent=True)\n</code></pre> <ol> <li>Security Considerations 3.1 Token-Based Authentication</li> </ol> <p>JWT tokens are used to authenticate users. The token is generated when a user logs in, and it is stored in the browser cookies. The token is sent with each request to validate the user\u2019s identity.</p> <p>JWT Generation: When a user successfully logs in, the system generates a JWT token, which includes the user\u2019s email as a payload. This token expires after a certain time, and the user must log in again to get a new token.</p> <p>Token Validation: Each time the user makes a request, the backend validates the token and retrieves the user\u2019s details from the database.</p> <p>3.2 Password Security</p> <p>Passwords are hashed using bcrypt before being stored in the database, ensuring that plaintext passwords are never stored or exposed. bcrypt is a strong and widely-used hashing algorithm that incorporates salting and multiple rounds of hashing to make brute-force attacks difficult.</p> <p>Conclusion</p> <p>TheDrive implements a secure login system using JWT tokens for authentication and bcrypt for password hashing. The system stores user credentials and file metadata securely in the PostgreSQL database, with relationships between users and their files managed using SQLAlchemy ORM. The FileSystemItem model tracks file metadata and AI processing status, allowing the platform to offer intelligent file management and analysis features in the future.</p> <p>This approach ensures that user data remains secure and private, while also providing a flexible foundation for adding advanced features such as AI-driven file analysis and semantic search.</p>"},{"location":"architecture/security/#explanation","title":"Explanation:","text":"<ol> <li>Password Hashing: Uses bcrypt for secure password hashing, making it hard for attackers to retrieve user passwords even if the database is compromised.</li> <li>JWT Tokens: Tokens are used to manage user sessions. The token includes expiration and is validated for each request, ensuring that only authenticated users can access their data.</li> <li>Database Schema: The User model stores user credentials, and the FileSystemItem model tracks file metadata, including the AI processing status and relationships.</li> <li>Security: JWT-based authentication and bcrypt ensure secure password handling, while the database ensures integrity and safe storage of sensitive user data.</li> </ol> <p>This Markdown file provides a detailed overview of the security features, focusing on user authentication and data protection within TheDrive.</p>"},{"location":"architecture/system-design/","title":"TheDrive - System Design Documentation","text":"<p>This document provides a comprehensive overview of the system design for TheDrive, focusing on its architecture, key components, data flow, technologies used, and security considerations. TheDrive is an AI-powered cross-platform file storage system designed to help users securely store, manage, and interact with their digital assets, leveraging cutting-edge AI technologies and a robust architecture.</p>"},{"location":"architecture/system-design/#1-introduction","title":"1. Introduction","text":"<p>TheDrive is designed to be a next-generation cloud storage platform that integrates AI-powered features with secure file storage. The system provides intelligent ways to interact with files, analyze data, and derive insights, all while ensuring top-tier security and privacy. TheDrive combines cloud storage, AI models, and secure databases to deliver a seamless user experience.</p>"},{"location":"architecture/system-design/#key-features","title":"Key Features:","text":"<ul> <li>Multi-format file support (documents, spreadsheets, images, etc.)</li> <li>AI-powered insights for querying, understanding, and managing data.</li> <li>End-to-end encryption (E2EE) for data privacy and security.</li> <li>Cloud storage integration with AWS S3 for file storage.</li> <li>Graph-based AI pipeline for processing relationships and complex data interactions.</li> </ul>"},{"location":"architecture/system-design/#2-architecture-overview","title":"2. Architecture Overview","text":""},{"location":"architecture/system-design/#high-level-architecture","title":"High-Level Architecture","text":"<p>TheDrive follows a microservices-based architecture with clearly defined components for handling different tasks, ensuring scalability and maintainability.</p> <ul> <li>Frontend (Next.js): Provides a dynamic, responsive web interface for users to interact with their files and perform actions (uploads, queries, AI-powered features).</li> <li>Backend (FastAPI): The core API layer responsible for file management, metadata storage, AI model interactions, user authentication, and more.</li> <li>AI Layer: Handles all AI-driven tasks, such as text analysis, image recognition, and semantic search. This layer uses a graph-based AI pipeline to manage relationships between files and metadata.</li> <li>Storage Layer:</li> <li>AWS S3: Used for secure and scalable file storage.</li> <li>PostgreSQL: Stores user credentials, file metadata, and other structured data.</li> <li>Neo4j: Used for graph-based AI processing and managing complex relationships between files.</li> </ul>"},{"location":"architecture/system-design/#diagram-of-high-level-architecture","title":"Diagram of High-Level Architecture","text":"<p>image</p>"},{"location":"architecture/system-design/#3-key-components-and-technologies","title":"3. Key Components and Technologies","text":""},{"location":"architecture/system-design/#31-frontend-nextjs","title":"3.1 Frontend (Next.js)","text":"<ul> <li>Next.js powers the frontend, providing server-side rendering (SSR) and static site generation (SSG) for fast page loading and SEO benefits.</li> <li>React is used for building reusable components and managing the state.</li> <li>The frontend interacts with the FastAPI backend through RESTful APIs using Axios.</li> <li>File upload functionality is handled through HTML5 and custom components for managing file previews.</li> </ul> <p>Technologies: - Next.js: For dynamic web applications with React. - Tailwind CSS: For responsive, utility-first CSS design. - Axios: For API communication between the frontend and backend.</p>"},{"location":"architecture/system-design/#32-backend-fastapi","title":"3.2 Backend (FastAPI)","text":"<p>The backend is powered by FastAPI, providing high-performance API handling. It serves multiple roles, including user authentication, file upload handling, and interaction with the AI pipeline.</p> <ul> <li>File Management: When a file is uploaded, FastAPI handles the storage in AWS S3 and the insertion of metadata in PostgreSQL.</li> <li>AI Integration: FastAPI manages the communication between the AI layer and the frontend.</li> <li>Authentication: Users are authenticated using JWT tokens to ensure secure access.</li> </ul> <p>Technologies: - FastAPI: Fast and modern Python web framework for API handling. - Python-Jose: For JWT authentication and token management. - SQLAlchemy: ORM for interacting with the PostgreSQL database. - boto3: AWS SDK for managing file storage in S3.</p>"},{"location":"architecture/system-design/#33-file-storage-with-aws-s3","title":"3.3 File Storage with AWS S3","text":"<p>AWS S3 is used to store files securely. It offers scalable storage and integrates well with FastAPI to handle file uploads and downloads.</p> <ul> <li>File Upload: When a user uploads a file, it is stored in an S3 bucket. The file metadata (file name, size, type, and user ID) is stored in the PostgreSQL database for easy reference.</li> <li>File Retrieval: Files are retrieved from S3 whenever needed, with access control managed via AWS IAM.</li> </ul> <p>Technologies: - AWS S3: Scalable object storage. - boto3: Python SDK for AWS services.</p>"},{"location":"architecture/system-design/#34-postgresql-for-credentials-and-metadata","title":"3.4 PostgreSQL for Credentials and Metadata","text":"<p>PostgreSQL is used to store user credentials, file metadata, and other relational data. It ensures the integrity of the data and allows for fast lookups and queries.</p> <ul> <li>User Management: Stores user credentials (hashed passwords) and user metadata.</li> <li>File Metadata: Stores details about files such as file names, types, sizes, and storage paths (links to S3).</li> <li>Data Relationships: Although PostgreSQL is a relational database, it is used here to store basic metadata and user data in structured tables.</li> </ul> <p>Technologies: - PostgreSQL: Relational database management system for secure, structured data storage.</p>"},{"location":"architecture/system-design/#35-graph-based-ai-pipeline-neo4j","title":"3.5 Graph-Based AI Pipeline (Neo4j)","text":"<p>The AI layer in TheDrive uses Neo4j, a graph database, to manage and analyze the relationships between different files and metadata. This is crucial for implementing the AI-powered features such as semantic search and file cross-referencing.</p> <ul> <li>Graph Representation: The AI pipeline treats files, metadata, and other entities as nodes in a graph, with relationships (edges) between them. For example, files that share tags or concepts are connected in the graph.</li> <li>AI Features: Graph-based models allow for advanced queries such as \"find all files related to this topic\" or \"retrieve files linked by similar keywords.\" It also powers semantic search, allowing the system to understand the context of search queries.</li> <li>AI Processing: The AI pipeline uses machine learning models to analyze textual content (e.g., for summarization) and images (via OCR and computer vision models).</li> </ul> <p>Technologies: - Neo4j: Graph database used to manage file relationships and metadata. - Graph Algorithms: Implementing algorithms such as community detection and graph traversal to analyze relationships between files.</p>"},{"location":"architecture/system-design/#36-ai-layer","title":"3.6 AI Layer","text":"<p>The AI layer performs several tasks, including:</p> <ul> <li>Text Analysis: Utilizes Natural Language Processing (NLP) models to extract meaning from documents, such as summarization, keyword extraction, and contextual analysis.</li> <li>Image Processing: Uses OCR to extract text from images and applies image recognition models to understand visual data (e.g., charts, graphs, diagrams).</li> <li>Semantic Search: Enhances search by understanding the meaning behind search queries, not just matching keywords.</li> </ul> <p>Technologies: - spaCy / NLTK: For text processing and NLP tasks. - PyTorch / TensorFlow: For training and deploying machine learning models. - OpenCV: For basic image processing and recognition.</p>"},{"location":"architecture/system-design/#4-data-flow","title":"4. Data Flow","text":""},{"location":"architecture/system-design/#file-upload-and-processing","title":"File Upload and Processing","text":"<ol> <li>User Uploads File:</li> <li>The user selects a file to upload via the frontend.</li> <li> <p>The frontend sends the file to the FastAPI backend, where it is uploaded to AWS S3 for storage.</p> </li> <li> <p>Metadata Storage:</p> </li> <li> <p>Metadata (file name, size, type, etc.) is stored in PostgreSQL for easy retrieval and indexing.</p> </li> <li> <p>AI Processing:</p> </li> <li> <p>The file content is passed through the AI pipeline:</p> <ul> <li>Text files undergo NLP analysis (e.g., summarization).</li> <li>Images are processed using OCR and image recognition.</li> </ul> </li> <li> <p>Graph-Based Analysis:</p> </li> <li>Metadata and file relationships are stored in Neo4j for graph-based analysis, enabling semantic search and advanced file relationships.</li> </ol>"},{"location":"architecture/system-design/#query-handling","title":"Query Handling","text":"<ol> <li>User Queries Files:</li> <li>The user enters a query (e.g., \"Show all files related to 'financial report'\").</li> <li>The frontend sends the query to the FastAPI backend.</li> <li> <p>The backend processes the query, utilizing semantic search powered by the AI layer and Neo4j graph database to find relevant files.</p> </li> <li> <p>AI Insights:</p> </li> <li>The AI models analyze the content and relationships between files to return meaningful results, which are then sent back to the frontend.</li> </ol>"},{"location":"architecture/system-design/#5-security-considerations","title":"5. Security Considerations","text":""},{"location":"architecture/system-design/#data-encryption","title":"Data Encryption","text":"<ul> <li>End-to-End Encryption (E2EE) ensures that all data is encrypted both at rest (in storage) and in transit (during upload/download).</li> <li>Files stored in AWS S3 are encrypted using AES-256 encryption.</li> </ul>"},{"location":"architecture/system-design/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li>JWT Authentication is used to securely authenticate users.</li> <li>Role-Based Access Control (RBAC) ensures that users only have access to files and features they are authorized to interact with.</li> </ul>"},{"location":"architecture/system-design/#data-isolation","title":"Data Isolation","text":"<ul> <li>Data segregation ensures that each user's files are stored in isolated environments, preventing unauthorized access between users.</li> <li>AWS IAM roles are used to manage access to cloud services.</li> </ul>"},{"location":"architecture/system-design/#6-conclusion","title":"6. Conclusion","text":"<p>TheDrive combines secure cloud storage, AI-powered file analysis, and a graph-based AI pipeline to provide a smarter, more efficient way for users to interact with their digital assets. With the use of AWS S3 for scalable storage, PostgreSQL for metadata management, and Neo4j for complex data relationships, TheDrive is built to scale and provide advanced features for personal and enterprise-level use.</p> <p>By integrating these technologies, TheDrive offers a secure, scalable, and intelligent cloud storage solution designed for the future of data interaction.</p>"},{"location":"architecture/trade-off/","title":"Technology Choices and Trade-offs","text":"<p>This document explains the technology choices made for TheDrive, detailing why certain technologies were chosen for the backend, storage, and AI pipeline, as well as the trade-offs compared to other alternatives. TheDrive ensures that all selected technologies comply with free-tier services as required by the project guidelines.</p>"},{"location":"architecture/trade-off/#1-cloud-storage-aws-s3-free-tier","title":"1. Cloud Storage: AWS S3 (Free Tier)","text":""},{"location":"architecture/trade-off/#why-aws-s3","title":"Why AWS S3?","text":"<p>AWS S3 was chosen as the cloud storage solution for TheDrive because it meets the project\u2019s free-tier requirements while providing reliable, scalable, and secure file storage. Here's why:</p> <ul> <li>Free Tier: AWS offers a free-tier for S3, which provides up to 5GB of standard storage, 20,000 GET requests, and 2,000 PUT requests per month at no charge. This is sufficient for development, testing, and small-scale production environments.</li> <li>Scalability: AWS S3 is designed to handle large-scale storage needs, so as the project grows, you can easily transition to a paid tier if more storage or higher request limits are needed.</li> <li>Security: AWS S3 provides encryption at rest and in transit, IAM roles for access control, and data durability (99.999999999% durability).</li> <li>Integration with Other AWS Services: S3 integrates seamlessly with other AWS services (like Lambda, EC2, and Rekognition), which can help scale the project if needed.</li> </ul>"},{"location":"architecture/trade-off/#trade-offs","title":"Trade-offs:","text":"<ul> <li>Vendor Lock-in: By using AWS S3, TheDrive is tied to the AWS ecosystem, and migrating to another cloud provider would incur effort and potential costs.</li> <li>Cost Scaling: The free-tier provides limited storage and requests. As the usage grows, costs will scale. However, the transition to paid tiers is gradual and can be optimized.</li> </ul>"},{"location":"architecture/trade-off/#alternative-choices","title":"Alternative Choices:","text":"<ul> <li>Google Cloud Storage / Azure Blob Storage: These cloud storage options also offer free-tier services with similar features. However, AWS was chosen because of its wide adoption, integration with other AWS services, and familiarity with the platform for most developers.</li> <li>Self-hosted Solutions (e.g., MinIO): Although self-hosted object storage like MinIO offers control over the environment and eliminates vendor lock-in, it adds operational complexity (e.g., maintaining the infrastructure) and does not offer the same level of scalability and integration as S3.</li> </ul>"},{"location":"architecture/trade-off/#2-database-postgresql-free-tier","title":"2. Database: PostgreSQL (Free Tier)","text":""},{"location":"architecture/trade-off/#why-postgresql","title":"Why PostgreSQL?","text":"<p>PostgreSQL was chosen as the relational database for managing user credentials, file metadata, and other structured data, as it meets the project\u2019s free-tier requirements.</p> <ul> <li>Free Tier: Cloud platforms like Heroku and ElephantSQL offer free-tier PostgreSQL databases with limited storage (up to 1GB or more) suitable for development and small-scale production environments.</li> <li>ACID Compliance: PostgreSQL is a reliable, ACID-compliant relational database, ensuring data integrity and consistency. This is crucial when handling structured data like credentials and file metadata.</li> <li>Flexibility: PostgreSQL supports JSON and JSONB, allowing it to handle both structured and semi-structured data. This gives flexibility when storing file metadata or data that does not fit neatly into a table schema.</li> </ul>"},{"location":"architecture/trade-off/#trade-offs_1","title":"Trade-offs:","text":"<ul> <li>Not as Fast as NoSQL for Unstructured Data: PostgreSQL is ideal for structured data, but for large volumes of unstructured data (e.g., documents or non-relational datasets), NoSQL databases like MongoDB might perform better.</li> <li>Scaling for High Traffic: While PostgreSQL can handle large datasets, high traffic and complex queries might require tuning and optimization. The free-tier databases may also have resource limitations (e.g., slower performance with large datasets).</li> </ul>"},{"location":"architecture/trade-off/#alternative-choices_1","title":"Alternative Choices:","text":"<ul> <li>MongoDB (NoSQL): MongoDB is a good alternative if the data model was document-based and required greater flexibility in handling unstructured data. However, PostgreSQL was preferred due to its data integrity and support for both structured and semi-structured data.</li> <li>MySQL: MySQL is another relational database option but PostgreSQL was chosen due to its richer feature set and advanced querying capabilities.</li> </ul>"},{"location":"architecture/trade-off/#3-graph-database-neo4j-free-tier","title":"3. Graph Database: Neo4j (Free Tier)","text":""},{"location":"architecture/trade-off/#why-neo4j","title":"Why Neo4j?","text":"<p>Neo4j is used as the graph database to handle relationships between files, metadata, and AI-driven features. Neo4j was chosen because it is an industry-leading graph database with a free-tier offering.</p> <ul> <li>Free Tier: Neo4j offers a free-tier cloud database that allows up to 200,000 nodes and 400,000 relationships. This free-tier is sufficient for small-scale graph database applications, ideal for development and early-stage production use.</li> <li>Efficient Relationship Mapping: Neo4j is optimized for handling complex relationships between data points, which is ideal for features like semantic search, file cross-referencing, and content analysis in TheDrive.</li> <li>Graph-Based AI Pipeline: TheDrive uses Neo4j to represent AI-powered relationships between files and their content, making it easy to query and analyze data relationships.</li> </ul>"},{"location":"architecture/trade-off/#trade-offs_2","title":"Trade-offs:","text":"<ul> <li>Learning Curve: Neo4j\u2019s graph database model and its Cypher query language require a learning curve, especially for teams that are more familiar with relational databases.</li> <li>Performance: For large-scale systems with complex graph queries, Neo4j might experience performance bottlenecks. Proper indexing and optimization techniques are required to ensure scalability.</li> </ul>"},{"location":"architecture/trade-off/#alternative-choices_2","title":"Alternative Choices:","text":"<ul> <li>Amazon Neptune: AWS offers Neptune, a fully managed graph database service, which could be used instead of Neo4j. However, Neptune does not offer a free-tier service, making it less suitable for the project\u2019s budget constraints.</li> <li>ArangoDB: A multi-model database that combines document, key-value, and graph databases. Although this could be a good alternative, Neo4j was preferred for its specialized focus on graph-based data and its robust community support.</li> </ul>"},{"location":"architecture/trade-off/#conclusion","title":"Conclusion","text":"<p>TheDrive uses a combination of AWS S3, PostgreSQL, and Neo4j to provide a robust, scalable, and secure system while adhering to the free-tier service requirement of the project. These technologies were selected based on their ability to meet the project\u2019s goals of:</p> <ul> <li>Scalability: Each technology can handle the growing data and operational needs of TheDrive.</li> <li>Security: Each solution provides strong security mechanisms, including encryption and access control.</li> <li>AI Integration: Neo4j\u2019s graph-based capabilities enable complex AI features like semantic search and cross-referencing.</li> </ul> <p>While there are trade-offs, such as potential vendor lock-in with AWS S3 and performance considerations with Neo4j, these technologies offer the best combination of features and flexibility within the free-tier constraints of the project.</p>"},{"location":"setup/local_install/","title":"Local Setup Guide","text":"<p>This guide explains how to set up both the Frontend (Next.js) and Backend (FastAPI) locally for development. It also covers the environment variable configuration necessary for both parts of the application.</p>"},{"location":"setup/local_install/#prerequisites","title":"Prerequisites","text":"<p>Before running the project locally, ensure the following prerequisites are installed.</p>"},{"location":"setup/local_install/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.10+</li> <li>Node.js 18+ (for Next.js frontend)</li> <li>npm or yarn</li> <li>Git</li> <li>PostgreSQL (or any other supported database for the backend)</li> <li>Neo4j (for graph-based AI data storage, optional but recommended)</li> <li>AWS CLI (for integration with AWS services)</li> </ul>"},{"location":"setup/local_install/#backend-setup-fastapi","title":"Backend Setup (FastAPI)","text":""},{"location":"setup/local_install/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>Clone the project repository to your local machine:</p> <pre><code>git clone &lt;your-repository-url&gt;\ncd &lt;project-directory&gt;\n</code></pre>"},{"location":"setup/local_install/#step-2-installing-requirements","title":"Step 2: Installing requirements","text":"<p>Move into <code>backend</code> folder and install requirements by: </p> <pre><code>cd backend\npip install -r requirements.txt\n</code></pre>"},{"location":"setup/local_install/#step-3-configure-environment-variables-both-for-frontend-and-backend","title":"Step 3: Configure Environment variables both for frontend and backend","text":"<p>Create a <code>.env</code> file in backend like this example:</p> <pre><code>envexample\n</code></pre> <p>Then move to frontend and create a <code>.env.local</code> with contents like this : <code>code</code></p> <ul> <li>Install dependencies</li> </ul> <pre><code>cd &lt;project_name&gt;\nnpm install\n</code></pre> <ul> <li>Build and run the project</li> </ul> <pre><code>npm start\n</code></pre> <p>Navigate to <code>http://localhost:8001</code></p> <ul> <li>API Document endpoints</li> </ul> <p>swagger Spec Endpoint : http://localhost:8001/api-docs </p> <p>swagger-ui  Endpoint : http://localhost:8001/docs </p> <p>The main purpose of this repository is to show a project setup and workflow for writing microservice. The Rest APIs will be using the Swagger (OpenAPI) Specification.</p>"},{"location":"setup/local_install/#getting-typescript","title":"Getting TypeScript","text":"<p>Add Typescript to project <code>npm</code>.</p> <pre><code>npm install -D typescript\n</code></pre>"},{"location":"setup/local_install/#project-structure","title":"Project Structure","text":"<p>The folder structure of this app is explained below:</p> Name Description dist Contains the distributable (or output) from your TypeScript build. node_modules Contains all  npm dependencies src Contains  source code that will be compiled to the dist dir configuration Application configuration including environment-specific configs src/controllers Controllers define functions to serve various express routes. src/lib Common libraries to be used across your app. src/middlewares Express middlewares which process the incoming requests before handling them down to the routes src/routes Contain all express routes, separated by module/area of application src/models Models define schemas that will be used in storing and retrieving data from Application database src/monitoring Prometheus metrics src/index.ts Entry point to express app package.json Contains npm dependencies as well as build scripts tslint.json Config settings for TSLint code style checking"},{"location":"setup/local_install/#building-the-project","title":"Building the project","text":""},{"location":"setup/local_install/#configuring-typescript-compilation","title":"Configuring TypeScript compilation","text":"<pre><code>{\n    \"compilerOptions\": {\n      \"target\": \"es5\",\n      \"module\": \"commonjs\",\n      \"outDir\": \"dist\",\n      \"sourceMap\": true\n    },\n\n    \"include\": [\n      \"src/**/*.ts\"\n\n\n    ],\n    \"exclude\": [\n      \"src/**/*.spec.ts\",\n      \"test\",\n      \"node_modules\"\n\n    ]\n  }\n\n</code></pre>"},{"location":"setup/local_install/#running-the-build","title":"Running the build","text":"<p>All the different build steps are orchestrated via npm scripts. Npm scripts basically allow us to call (and chain) terminal commands via npm.</p> Npm Script Description <code>start</code> Runs full build and runs node on dist/index.js. Can be invoked with <code>npm start</code> <code>build:copy</code> copy the *.yaml file to dist/ folder <code>build:live</code> Full build. Runs ALL build tasks <code>build:dev</code> Full build. Runs ALL build tasks with all watch tasks <code>dev</code> Runs full build before starting all watch tasks. Can be invoked with <code>npm dev</code> <code>test</code> Runs build and run tests using mocha <code>lint</code> Runs TSLint on project files"}]}