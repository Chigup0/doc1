{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TheDrive - AI-Powered Cross-Platform File Storage System","text":""},{"location":"#project-overview","title":"Project Overview","text":"<p>TheDrive is an AI-powered cloud storage platform designed to transform the way users store, interact with, and manage their digital assets. Unlike traditional cloud storage systems like Google Drive or OneDrive, TheDrive integrates advanced AI technologies to provide smarter, more intuitive ways to interact with files, derive meaningful insights, and enhance overall user experience. </p> <p>With the growing concerns around data privacy and the increasing need for security, TheDrive has been built to not only offer intelligent features but also ensure that user data remains safe, secure, and private. This platform is not just about storing files\u2014it's about empowering users with the tools to analyze, manage, and gain insights from their digital assets, all while maintaining the highest standards of security.</p>"},{"location":"#problem-statement","title":"Problem Statement","text":"<p>As the amount of digital data grows, users are faced with challenges not only in managing this information but also in ensuring its security and privacy. Traditional cloud storage systems allow for basic file storage, sharing, and collaboration, but they fall short when it comes to the following:</p> <ul> <li>Intelligence: Current systems do not provide advanced tools to analyze and interact with file contents effectively.</li> <li>Security: While they offer security features, many systems are vulnerable to data breaches or unauthorized access, often exposing user data to administrators and third parties.</li> <li>User Experience: Current solutions don't integrate AI-powered features such as content-based search, query resolution, or semantic understanding of data in a seamless and intuitive way.</li> </ul> <p>TheDrive aims to solve these issues by creating an intelligent, secure cloud storage system that enables users to interact with their files in ways never before possible.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#1-multi-file-format-support","title":"1. Multi-File Format Support","text":"<p>TheDrive supports a wide range of file formats, making it a versatile platform for various use cases:</p> <ul> <li>Document Formats: <code>.pdf</code>, <code>.docx</code>, <code>.txt</code>, <code>.pptx</code> (PowerPoint presentations)</li> <li>Spreadsheet Formats: <code>.csv</code>, <code>.xlsx</code> (Excel spreadsheets)</li> <li>Image Formats: <code>.jpg</code>, <code>.png</code></li> </ul> <p>Each of these file types is processed and displayed with content integrity, preserving the structure, layout, and formatting. This ensures that no matter the file type, users can interact with their files in a seamless, efficient way.</p>"},{"location":"#2-ai-driven-smarter-storage-system","title":"2. AI-Driven Smarter Storage System","text":""},{"location":"#a-contextual-chat","title":"a. Contextual Chat","text":"<p>One of the most innovative features of TheDrive is its AI-powered contextual chat system. This allows users to interact with their storage by asking questions about their files, folders, or documents, and the system will provide relevant answers. </p> <ul> <li>Chat Over Whole Drive: </li> <li>Users can ask questions about the entire contents of their drive, such as \"What documents contain the word 'budget'?\" or \"Show me the documents that mention 'project proposal'.\" The system can also calculate aggregation metrics, such as the total number of words across all documents.</li> <li> <p>This feature goes beyond just keyword search\u2014it's about understanding the context of the files, providing more intelligent responses.</p> </li> <li> <p>Chat Over Folders:</p> </li> <li> <p>Users can query specific folders without revealing information from other folders, ensuring data privacy at the folder level. For example, a user could ask, \"How many documents are there in this folder related to 'marketing'?\" and receive a relevant answer without exposing files from other folders.</p> </li> <li> <p>Chat Over Documents:</p> </li> <li>Users can ask about specific documents, and the AI will provide context and insights based on the content of the file.</li> </ul>"},{"location":"#b-highlight-based-query-resolution-and-explanation","title":"b. Highlight-Based Query Resolution and Explanation","text":"<p>TheDrive goes beyond just storing files\u2014users can highlight specific text or portions of a document, image, or table, and the system will provide AI-powered insights. </p> <ul> <li>Text Highlighting: </li> <li> <p>When a user highlights a word or phrase, the system will provide its definition, usage, and related explanations. This makes it easy for users to quickly understand complex terms without leaving the platform.</p> </li> <li> <p>Image/Picture Interaction:</p> </li> <li> <p>For images or presentations, users can highlight portions of a visual (e.g., a chart, diagram, or handwritten note) to get an AI-generated description or analysis of that section.</p> </li> <li> <p>Table Analysis:</p> </li> <li>In spreadsheet files, users can select portions of tables to get aggregation metrics, summaries, or insights based on the data, helping users analyze their files more efficiently.</li> </ul>"},{"location":"#c-cross-referencing-files-and-sections","title":"c. Cross-Referencing Files and Sections","text":"<p>Cross-referencing is a key feature that allows users to trace the origins of the data used to answer a query. Whenever a user asks a question about a document, the system provides:</p> <ul> <li>Source Citation: Links to the specific file and section that answered the query.</li> <li>Chunk Highlighting: When opening the linked file, the exact section of text or data used for the answer will be highlighted for quick reference.</li> </ul>"},{"location":"#d-other-ai-powered-features","title":"d. Other AI-Powered Features","text":"<ul> <li>Image Understanding: </li> <li> <p>TheDrive can analyze and understand content within images, such as handwritten notes (via OCR), charts, diagrams, and more. Users can ask the system to describe, explain, or analyze parts of images, making the system more versatile for users dealing with visual data.</p> </li> <li> <p>Semantic Search: </p> </li> <li>Unlike traditional search engines that rely solely on keywords, semantic search allows users to search based on the meaning and context of the query, returning more relevant and accurate results.</li> </ul>"},{"location":"#system-architecture","title":"System Architecture","text":"<p>TheDrive is built using a microservices architecture with a serverless backend and scalable cloud infrastructure to ensure both reliability and performance.</p>"},{"location":"#key-components","title":"Key Components:","text":"<ul> <li>Frontend (Next.js):</li> <li> <p>Provides a dynamic, responsive web application that enables users to interact with their files, perform searches, and access AI-powered insights.</p> </li> <li> <p>Backend (FastAPI):</p> </li> <li> <p>Powers the core services of the application, handling user authentication, file management, and AI query processing. It communicates with databases and AI models to provide intelligent responses.</p> </li> <li> <p>AI Layer:</p> </li> <li> <p>A RAG+KAG pipeline that process text, images, and tables to provide users with insights, document summaries, and analytics.</p> </li> <li> <p>Database Layer:</p> </li> <li> <p>The system uses a combination of SQL databases (for general metadata and user data) and graph databases (Neo4j) for representing complex relationships between files and metadata.</p> </li> <li> <p>Cloud Integration:</p> </li> <li>The system is hosted on cloud platforms like AWS and Azure, ensuring scalability, high availability, and reliability.</li> </ul>"},{"location":"#security","title":"Security","text":"<ul> <li>Data Isolation: Each user\u2019s data is physically and logically isolated to prevent unauthorized access or leakage.</li> </ul>"},{"location":"ai_pipeline/ai-features/","title":"Ai features","text":""},{"location":"ai_pipeline/ai-features/#docsusageai-featuresmd","title":"docs/usage/ai-features.md","text":""},{"location":"ai_pipeline/ai-features/#ai-features-guide","title":"AI Features Guide","text":"<p>Zenith Drive incorporates advanced AI capabilities to enhance your file management experience.</p>"},{"location":"ai_pipeline/ai-features/#contextual-chat","title":"Contextual Chat","text":""},{"location":"ai_pipeline/ai-features/#drive-wide-chat","title":"Drive-Wide Chat","text":"<p>Ask questions about all files in your drive:</p> <ul> <li>\"What's the total size of all my PDF documents?\"</li> <li>\"Find all files related to project Zenith\"</li> <li>\"Show me a summary of my financial documents\"</li> </ul>"},{"location":"ai_pipeline/ai-features/#folder-level-chat","title":"Folder-Level Chat","text":"<p>Restrict queries to specific folders:</p> <ul> <li>\"What images are in the Vacation folder?\"</li> <li>\"Summarize the documents in the Work/Reports folder\"</li> </ul>"},{"location":"ai_pipeline/ai-features/#document-specific-chat","title":"Document-Specific Chat","text":"<p>Ask questions about individual files:</p> <ul> <li>\"What are the key points in this PDF?\"</li> <li>\"Extract the main conclusions from this report\"</li> </ul>"},{"location":"ai_pipeline/ai-features/#highlight-based-features","title":"Highlight-Based Features","text":""},{"location":"ai_pipeline/ai-features/#text-selection","title":"Text Selection","text":"<p>Highlight any text in documents to:</p> <ul> <li>Get definitions of unfamiliar terms</li> <li>Receive explanations of complex concepts</li> <li>Request summaries of selected content</li> </ul>"},{"location":"ai_pipeline/ai-features/#image-analysis","title":"Image Analysis","text":"<p>Select portions of images to:</p> <ul> <li>Identify objects and people</li> <li>Extract text via OCR</li> <li>Analyze charts and diagrams</li> </ul> <p>Technical Implementation - Multi-Modal Image Processing:</p> <pre><code>def process_uploaded_image(image_data: bytes, llm=None) -&gt; Document:\n    \"\"\"Process uploaded image and return a Document with comprehensive analysis.\"\"\"\n    try:\n        # Open image\n        image = Image.open(io.BytesIO(image_data))\n\n        # Perform comprehensive analysis\n        analysis_results = image_processor.comprehensive_image_analysis(image, llm)\n\n        # Create content for the document\n        content_parts = []\n\n        # Add OCR text if available\n        if analysis_results['ocr'].get('full_text'):\n            content_parts.append(f\"OCR Text: {analysis_results['ocr']['full_text']}\")\n\n        # Add structure information\n        if analysis_results['structure'].get('shapes'):\n            shapes_info = f\"Detected {analysis_results['structure']['total_shapes']} shapes and {analysis_results['structure']['total_lines']} lines\"\n            content_parts.append(shapes_info)\n\n        # Add entity information if available\n        if analysis_results.get('entities', {}).get('entities_detected'):\n            content_parts.append(f\"Entities: {analysis_results['entities']['entities_detected']}\")\n\n        # Add chart analysis if available\n        if analysis_results.get('chart', {}).get('chart_analysis'):\n            content_parts.append(f\"Chart Analysis: {analysis_results['chart']['chart_analysis']}\")\n\n        content = '\\n\\n'.join(content_parts) if content_parts else analysis_results['summary']\n\n        # Create document\n        document = Document(\n            page_content=content,\n            metadata={\n                'type': 'image_analysis',\n                'analysis_results': analysis_results,\n                'has_text': bool(analysis_results['ocr'].get('full_text')),\n                'has_shapes': bool(analysis_results['structure'].get('shapes')),\n                'has_entities': bool(analysis_results.get('entities', {}).get('entities_detected')),\n                'image_size': analysis_results['image_info']['size'],\n                'timestamp': analysis_results['timestamp']\n            }\n        )\n\n        return document\n\n    except Exception as e:\n        logger.error(f\"Image processing failed: {e}\")\n        return Document(\n            page_content=f\"Error processing image: {str(e)}\",\n            metadata={'type': 'image_error', 'error': str(e)}\n        )\n</code></pre>"},{"location":"ai_pipeline/ai-features/#table-operations","title":"Table Operations","text":"<p>Select table data to:</p> <ul> <li>Calculate aggregates (sum, average, etc.)</li> <li>Filter and sort data</li> <li>Create visualizations</li> </ul>"},{"location":"ai_pipeline/ai-features/#semantic-search","title":"Semantic Search","text":"<p>Search based on meaning rather than just keywords:</p> <ul> <li>\"Find documents about machine learning security\"</li> <li>\"Locate images from beach vacations\"</li> <li>\"Find spreadsheets with sales data\"</li> </ul> <p>Technical Implementation - Hybrid RAG + GraphRAG Search:</p> <pre><code>def rag_answer(question: str, conversation_id: str = None, confidence_threshold: float = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Advanced semantic search with conversation awareness and confidence gating\n    \"\"\"\n    if confidence_threshold is None:\n        confidence_threshold = CONFIDENCE_THRESHOLD\n\n    try:\n        # Check if this is a follow-up question\n        is_followup = False\n        if conversation_id:\n            is_followup = detect_followup_question(question, conversation_id)\n\n        # Vector similarity search\n        vector_docs = vector_db.similarity_search_with_score(question, k=10)\n        confident_docs = [(doc, score) for doc, score in vector_docs if score &gt;= confidence_threshold]\n\n        if not confident_docs:\n            return {\n                \"answer\": \"I couldn't find relevant information with sufficient confidence. Try being more specific.\",\n                \"confidence\": 0.0,\n                \"sources\": [],\n                \"method\": \"insufficient_confidence\"\n            }\n\n        # Graph-based contextual enhancement\n        graph_context = \"\"\n        if get_graph_db().is_connected():\n            entities = extract_entities_simple(question)\n            if entities:\n                graph_facts = get_subgraph_facts(entities, max_facts=5)\n                if graph_facts:\n                    graph_context = f\"\\n\\nKnowledge Graph Context:\\n{graph_facts}\"\n\n        # Prepare context for LLM\n        context_parts = []\n        source_info = []\n\n        for doc, confidence in confident_docs[:5]:  # Top 5 most confident results\n            context_parts.append(f\"Document: {doc.metadata.get('source', 'unknown')}\\n{doc.page_content}\")\n            source_info.append({\n                \"source\": doc.metadata.get('source', 'unknown'),\n                \"confidence\": float(confidence),\n                \"chunk_id\": doc.metadata.get('chunk_id', 'unknown')\n            })\n\n        full_context = \"\\n\\n---\\n\\n\".join(context_parts) + graph_context\n\n        # Generate answer with conversation awareness\n        conversation_prompt = \"\"\n        if is_followup and conversation_id:\n            conversation_prompt = f\"\\nNote: This appears to be a follow-up question in conversation {conversation_id}. Consider previous context appropriately.\"\n\n        prompt = f\"\"\"Based on the following context, answer the question accurately and concisely.\n\nContext:\n{full_context}\n\nQuestion: {question}{conversation_prompt}\n\nProvide a helpful answer based on the context. If the context doesn't contain enough information, say so clearly.\"\"\"\n\n        # Generate response\n        model = genai.GenerativeModel('gemini-1.5-flash')\n        response = model.generate_content(prompt)\n\n        # Calculate overall confidence\n        avg_confidence = sum(conf for _, conf in confident_docs[:5]) / len(confident_docs[:5])\n\n        return {\n            \"answer\": response.text,\n            \"confidence\": round(avg_confidence, 3),\n            \"sources\": source_info,\n            \"method\": \"hybrid_rag_graphrag\",\n            \"is_followup\": is_followup,\n            \"graph_enhanced\": bool(graph_context.strip())\n        }\n\n    except Exception as e:\n        return {\n            \"answer\": f\"Error processing question: {e}\",\n            \"confidence\": 0.0,\n            \"sources\": [],\n            \"method\": \"error\"\n        }\n\ndef detect_followup_question(question: str, conversation_id: str) -&gt; bool:\n    \"\"\"\n    Detect if a question is a follow-up in a conversation using AI\n    \"\"\"\n    followup_indicators = [\n        \"what about\", \"and\", \"also\", \"furthermore\", \"additionally\",\n        \"what\", \"how\", \"why\", \"when\", \"where\", \"which\", \"that\",\n        \"it\", \"this\", \"these\", \"those\", \"they\", \"them\"\n    ]\n\n    question_lower = question.lower()\n\n    # Check for pronouns and follow-up indicators\n    has_followup_words = any(indicator in question_lower for indicator in followup_indicators)\n\n    # Check for short questions (likely follow-ups)\n    is_short = len(question.split()) &lt; 8\n\n    # Check for missing context (pronouns without clear antecedents)\n    pronouns = [\"it\", \"this\", \"that\", \"they\", \"them\", \"these\", \"those\"]\n    has_pronouns = any(pronoun in question_lower.split() for pronoun in pronouns)\n\n    return (has_followup_words and is_short) or has_pronouns\n</code></pre>"},{"location":"ai_pipeline/ai-features/#cross-reference-features","title":"Cross-Reference Features","text":"<p>When the AI references information from files:</p> <ol> <li>Source citations are provided with links</li> <li>Clicking a citation opens the source file</li> <li>The relevant section is highlighted for context</li> </ol>"},{"location":"ai_pipeline/ai-features/#knowledge-graph-visualization","title":"Knowledge Graph Visualization","text":"<p>View connections between your files:</p> <ul> <li>Visual representation of related documents</li> <li>Topic-based clustering of content</li> <li>Timeline views of document creation and modification</li> </ul>"},{"location":"ai_pipeline/ai-features/#api-integration","title":"API Integration","text":"<p>Developers can access AI features through our API:</p> <pre><code>// Example: Querying drive context\nconst response = await fetch(\"/api/ai/query\", {\n  method: \"POST\",\n  headers: {\n    Authorization: `Bearer ${token}`,\n    \"Content-Type\": \"application/json\",\n  },\n  body: JSON.stringify({\n    query: \"Summarize my project documents\",\n    context: \"drive\", // or \"folder/:id\", \"file/:id\"\n    options: {\n      include_citations: true,\n      generate_visualization: false,\n    },\n  }),\n});\n</code></pre>"},{"location":"ai_pipeline/features/","title":"Key Features","text":""},{"location":"ai_pipeline/features/#multi-file-format-support","title":"Multi-File Format Support","text":"<ul> <li>Documents: Supports PDF, DOCX, TXT, PPTX. Each format is parsed using specialized loaders, with metadata extraction and layout preservation.</li> <li>Spreadsheets: Handles CSV and XLSX, with intelligent chunking and column/row summarization for semantic search.</li> <li>Images: JPG, PNG, and other formats are processed for text (OCR), diagrams, charts, and entity detection. Visual content is indexed and made searchable.</li> <li>Hybrid Ingestion: Automatically detects file type and applies the optimal processing pipeline, including multimodal analysis for files containing both text and images.</li> </ul>"},{"location":"ai_pipeline/features/#rag-retrieval-augmented-generation-functionality","title":"RAG (Retrieval-Augmented Generation) Functionality","text":"<ul> <li>Universal Ingestion: Files are detected and loaded using format-specific strategies, supporting batch and incremental uploads.</li> <li>Chunking: Documents are split into context-aware chunks, with parameters tuned for each file type to maximize retrieval quality.</li> <li>Semantic Embeddings: Chunks are embedded using Google Gemini models, enabling meaning-based search and retrieval.</li> <li>Graph-Based Knowledge: Entities and relationships are extracted from text and images using LLMs, then stored in a Neo4j graph database for advanced querying and analytics.</li> <li>Multimodal Support: Images are analyzed for printed and handwritten text (Tesseract, EasyOCR), diagrams, charts, and entities using vision models. Results are combined and deduplicated for comprehensive coverage.</li> </ul>"},{"location":"ai_pipeline/features/#intelligent-chat-search","title":"Intelligent Chat &amp; Search","text":"<ul> <li>Drive-Level Chat: Users can ask questions spanning all files; answers are grounded in indexed content and cite sources, with context-aware retrieval.</li> <li>Folder/Document-Level Chat: Restricts context to the selected folder or document, ensuring privacy and relevance for targeted queries.</li> <li>Highlight-Based AI Popups: Users can highlight text, table regions, or image areas for instant explanations, definitions, or metrics, powered by LLMs and vision models.</li> <li>Semantic Search: Supports meaning-based search across all content, including cross-file and cross-modal queries.</li> <li>Cross-Referencing: Answers include citations and direct links to file locations; opening a file highlights the relevant chunk for user convenience.</li> <li>Knowledge Visualization: Relationships and connections are visualized as interactive knowledge graphs, supporting explainability and analytics.</li> </ul> <p>code snippet: Conversation-Aware RAG</p> <pre><code>def detect_followup_question(current_query: str, conversation_history: List[Dict]) -&gt; Dict:\n    \"\"\"\n    Detect if the current query is a follow-up to previous conversation.\n    Returns analysis with follow-up detection and context preservation recommendation.\n    \"\"\"\n    if not conversation_history:\n        return {\n            \"is_followup\": False,\n            \"confidence\": 0.0,\n            \"reasoning\": \"No previous conversation\",\n            \"should_preserve_context\": False\n        }\n\n    # Get the most recent conversation turn\n    last_turn = conversation_history[-1]\n    prev_query = last_turn.get(\"query\", \"\")\n    prev_response = last_turn.get(\"response\", \"\")[:500]  # Limit response length\n\n    # Simple heuristic checks first\n    current_lower = current_query.lower()\n\n    # Check for obvious follow-up indicators\n    followup_indicators = [\n        \"what about\", \"how about\", \"tell me more\", \"can you explain\", \"expand on\",\n        \"what does that mean\", \"what is that\", \"what are they\", \"what is it\",\n        \"more details\", \"elaborate\", \"clarify\", \"continue\", \"also\",\n        \"in addition\", \"furthermore\", \"what else\", \"anything else\"\n    ]\n\n    pronoun_indicators = [\n        \" it \", \" this \", \" that \", \" they \", \" them \", \" these \", \" those \",\n        \"the above\", \"mentioned\", \"previous\", \"earlier\"\n    ]\n\n    # Quick heuristic scoring\n    heuristic_score = 0.0\n    reasons = []\n\n    for indicator in followup_indicators:\n        if indicator in current_lower:\n            heuristic_score += 0.3\n            reasons.append(f\"Contains follow-up phrase: '{indicator}'\")\n\n    for pronoun in pronoun_indicators:\n        if pronoun in current_lower:\n            heuristic_score += 0.4\n            reasons.append(f\"Contains reference pronoun: '{pronoun.strip()}'\")\n\n    # Check if query is very short (likely a follow-up)\n    if len(current_query.split()) &lt;= 5 and heuristic_score &gt; 0:\n        heuristic_score += 0.2\n        reasons.append(\"Short query with follow-up indicators\")\n\n    # Check for question words in short queries\n    question_words = [\"what\", \"how\", \"why\", \"when\", \"where\", \"which\", \"who\"]\n    if any(current_lower.startswith(qw) for qw in question_words) and len(current_query.split()) &lt;= 8:\n        heuristic_score += 0.1\n        reasons.append(\"Short question likely referencing previous context\")\n\n    # If heuristic score is high enough, consider it a follow-up\n    if heuristic_score &gt;= 0.4:\n        return {\n            \"is_followup\": True,\n            \"confidence\": min(0.9, heuristic_score),\n            \"reasoning\": \"; \".join(reasons),\n            \"should_preserve_context\": True\n        }\n\n    # For borderline cases, use LLM analysis\n    if heuristic_score &gt; 0.1 or len(current_query.split()) &lt;= 6:\n        try:\n            prompt = FOLLOW_UP_DETECTION_PROMPT.format(\n                prev_query=prev_query,\n                prev_response=prev_response,\n                current_query=current_query\n            )\n\n            llm_instance = llm()\n            response = llm_instance.invoke(prompt).content.strip()\n\n            # Parse LLM response\n            llm_analysis = _safe_extract_json(response, dict)\n            if llm_analysis:\n                return llm_analysis\n        except Exception as e:\n            pass\n\n    return {\n        \"is_followup\": False,\n        \"confidence\": 1.0 - heuristic_score,\n        \"reasoning\": \"No clear follow-up indicators found\",\n        \"should_preserve_context\": False\n    }\n</code></pre>"},{"location":"ai_pipeline/features/#data-management-cleanup","title":"Data Management &amp; Cleanup","text":"<ul> <li>Complete File Deletion: Removes files from both vector database and knowledge graph, ensuring no orphaned data.</li> <li>Bulk Operations: Supports batch deletion of multiple files with detailed success/failure tracking.</li> <li>Orphaned Data Detection: Identifies and cleans up data that may have been orphaned due to external file deletions.</li> <li>Preview Operations: Allows users to preview what will be deleted before actual deletion.</li> </ul> <p>code snippet: Complete File Deletion</p> <pre><code>def delete_file_completely(file_id: str) -&gt; Dict:\n    \"\"\"\n    Delete a file completely from both vector database and knowledge graph.\n    \"\"\"\n    result = {\n        \"file_id\": file_id,\n        \"vector_db\": {\"success\": False},\n        \"knowledge_graph\": {\"success\": False},\n        \"overall_success\": False\n    }\n\n    # Delete from vector database\n    vector_result = delete_file_from_vector_db(file_id)\n    result[\"vector_db\"] = vector_result\n\n    # Delete from knowledge graph\n    graph_result = delete_file_from_knowledge_graph(file_id)\n    result[\"knowledge_graph\"] = graph_result\n\n    # Overall success if at least one deletion succeeded\n    result[\"overall_success\"] = vector_result[\"success\"] or graph_result[\"success\"]\n\n    return result\n\ndef cleanup_orphaned_entities() -&gt; Dict:\n    \"\"\"\n    Clean up entities that have no mention relationships from any chunks.\n    This should be called after deleting files to clean up orphaned entities.\n    \"\"\"\n    result = {\n        \"success\": False,\n        \"deleted_entities\": 0,\n        \"deleted_relations\": 0,\n        \"errors\": []\n    }\n\n    graphrag_db = _get_graphrag_db()\n    if not graphrag_db or not graphrag_db.is_connected():\n        result[\"errors\"].append(\"GraphRAG not available or not connected\")\n        return result\n\n    try:\n        with graphrag_db.driver.session() as session:\n            # First, delete all RELATES relationships connected to orphaned entities\n            orphaned_relations_query = \"\"\"\n            MATCH (e:Entity)\n            WHERE NOT EXISTS((:Chunk)-[:MENTIONS]-&gt;(e))\n            MATCH (e)-[r:RELATES]-(other)\n            DELETE r\n            RETURN count(r) as deleted_relations\n            \"\"\"\n            relations_result = session.run(orphaned_relations_query).single()\n            deleted_relations = relations_result[\"deleted_relations\"] if relations_result else 0\n\n            # Then delete the orphaned entities themselves\n            cleanup_query = \"\"\"\n            MATCH (e:Entity)\n            WHERE NOT EXISTS((:Chunk)-[:MENTIONS]-&gt;(e))\n            DELETE e\n            RETURN count(e) as deleted_entities\n            \"\"\"\n            cleanup_result = session.run(cleanup_query).single()\n            result[\"deleted_entities\"] = cleanup_result[\"deleted_entities\"] if cleanup_result else 0\n            result[\"deleted_relations\"] = deleted_relations\n            result[\"success\"] = True\n\n            print(f\"[GraphRAG] Cleaned up {result['deleted_entities']} orphaned entities and {deleted_relations} orphaned relationships\")\n\n    except Exception as e:\n        result[\"errors\"].append(f\"GraphRAG error: {str(e)}\")\n        print(f\"[GraphRAG] Failed to cleanup orphaned entities: {e}\")\n\n    return result\n</code></pre>"},{"location":"ai_pipeline/features/#performance-optimization","title":"Performance Optimization","text":""},{"location":"ai_pipeline/features/#caching-strategy","title":"Caching Strategy","text":"<ul> <li>Vector embeddings: Cached to avoid recomputation</li> <li>Graph queries: Intelligent caching of subgraph results</li> <li>LLM responses: Context-aware response caching</li> </ul>"},{"location":"ai_pipeline/features/#batch-processing","title":"Batch Processing","text":"<ul> <li>Bulk ingestion: Efficient processing of multiple files</li> <li>Parallel extraction: Concurrent entity/relation extraction</li> <li>Optimized indexing: Smart chunking and vector generation</li> </ul> <p>code snippet: Performance Monitoring &amp; Health Checks</p> <pre><code>import time\nfrom functools import wraps\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\ndef performance_monitor(func):\n    \"\"\"Decorator to monitor function performance\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start_time = time.time()\n        try:\n            result = func(*args, **kwargs)\n            execution_time = time.time() - start_time\n\n            # Add performance data to result if it's a dict\n            if isinstance(result, dict):\n                result['_performance'] = {\n                    'execution_time': round(execution_time, 3),\n                    'function': func.__name__\n                }\n\n            return result\n\n        except Exception as e:\n            execution_time = time.time() - start_time\n            print(f\"Function {func.__name__} failed after {execution_time:.3f}s: {e}\")\n            raise\n\n    return wrapper\n\ndef get_system_health() -&gt; Dict[str, Any]:\n    \"\"\"Comprehensive system health check\"\"\"\n    health_status = {\n        \"timestamp\": time.time(),\n        \"overall_status\": \"healthy\",\n        \"components\": {}\n    }\n\n    # Check vector database\n    try:\n        test_query = vector_db.similarity_search(\"test\", k=1)\n        health_status[\"components\"][\"vector_db\"] = {\n            \"status\": \"healthy\",\n            \"response_time\": \"&lt; 100ms\"\n        }\n    except Exception as e:\n        health_status[\"components\"][\"vector_db\"] = {\n            \"status\": \"unhealthy\",\n            \"error\": str(e)\n        }\n        health_status[\"overall_status\"] = \"degraded\"\n\n    # Check graph database\n    if get_graph_db().is_connected():\n        try:\n            with get_graph_db().driver.session() as session:\n                result = session.run(\"RETURN 1 as test\").single()\n                health_status[\"components\"][\"graph_db\"] = {\n                    \"status\": \"healthy\",\n                    \"connection\": \"active\"\n                }\n        except Exception as e:\n            health_status[\"components\"][\"graph_db\"] = {\n                \"status\": \"unhealthy\",\n                \"error\": str(e)\n            }\n            health_status[\"overall_status\"] = \"degraded\"\n    else:\n        health_status[\"components\"][\"graph_db\"] = {\n            \"status\": \"disconnected\",\n            \"connection\": \"inactive\"\n        }\n        health_status[\"overall_status\"] = \"degraded\"\n\n    return health_status\n</code></pre>"},{"location":"ai_pipeline/features/#system-reliability","title":"System Reliability","text":""},{"location":"ai_pipeline/features/#error-handling","title":"Error Handling","text":"<ul> <li>Graceful degradation: System continues with reduced functionality</li> <li>Automatic retries: Smart retry logic for transient failures</li> <li>Fallback mechanisms: Alternative processing methods when primary fails</li> </ul>"},{"location":"ai_pipeline/features/#monitoring-and-logging","title":"Monitoring and Logging","text":"<ul> <li>Performance tracking: Response times and resource usage</li> <li>Error reporting: Detailed error logs with context</li> <li>Health checks: Continuous monitoring of system components</li> </ul>"},{"location":"ai_pipeline/functions/","title":"RAG Pipeline: Key Functions and Workflow","text":"<ol> <li>File Type Detection</li> <li><code>detect_file_type(file_path, content_type)</code>: Determines file type using extension and MIME type. Ensures robust handling of PDFs, DOCX, TXT, CSV, images, and more. This step enables the pipeline to select the correct loader and chunking strategy.</li> </ol> <p>code snippet:</p> <pre><code>def detect_file_type(file_path: str, content_type: Optional[str] = None) -&gt; str:\n    \"\"\"Detect file type from extension and MIME type.\"\"\"\n    path = Path(file_path)\n    extension = path.suffix.lower()\n\n    # Primary detection by extension\n    if extension == '.pdf':\n        return 'pdf'\n    elif extension in ['.txt', '.md', '.rst']:\n        return 'text'\n    elif extension in ['.docx', '.doc']:\n        return 'docx'\n    elif extension == '.csv':\n        return 'csv'\n    elif extension in ['.jpeg', '.jpg', '.png', '.gif', '.bmp', '.webp', '.tiff', '.tif']:\n        return 'image'\n\n    # Fallback to MIME type\n    if content_type:\n        if 'pdf' in content_type:\n            return 'pdf'\n        elif 'text' in content_type:\n            return 'text'\n        elif 'spreadsheet' in content_type or 'csv' in content_type:\n            return 'csv'\n        elif 'word' in content_type or 'document' in content_type:\n            return 'docx'\n        elif 'image' in content_type:\n            if 'jpeg' in content_type or 'jpg' in content_type:\n                return 'jpg'\n            elif 'png' in content_type:\n                return 'png'\n            elif 'gif' in content_type:\n                return 'gif'\n</code></pre> <ol> <li> <p>Specialized Loading</p> </li> <li> <p><code>load_pdf(path)</code>, <code>load_text(path)</code>, <code>load_docx(path)</code>, <code>load_csv(path)</code>: Each loader parses its format, extracts metadata, and prepares content for chunking. For images, <code>process_uploaded_image(image_data, llm)</code> applies OCR, diagram, and entity analysis.</p> </li> <li> <p>Chunking</p> </li> <li> <p><code>chunk_docs(docs, file_type, chunk_size, overlap)</code>: Splits loaded documents into context-rich chunks. Chunk size and overlap are tuned per file type (e.g., larger for CSVs, structured for DOCX/PDF) to maximize retrieval quality and context preservation.</p> </li> <li> <p>Embedding &amp; Vector Store</p> </li> <li> <p><code>embedder()</code>: Returns the embedding model (Google Gemini) for semantic search.</p> </li> <li> <p><code>get_vs()</code>, <code>get_cache_vs()</code>: Access the main and cache Chroma vector stores. Chunks are embedded and stored for fast semantic retrieval and caching.</p> </li> <li> <p>Graph Construction (Optional)</p> </li> <li> <p><code>get_graphrag_functions()</code>: Loads GraphRAG functions for entity/relation extraction and graph updates. Entities and relationships are extracted from text and images, then stored in Neo4j for advanced cross-file reasoning.</p> </li> <li> <p>Image &amp; Multimodal Analysis</p> </li> <li><code>process_uploaded_image(image_data, llm)</code>: Performs OCR (Tesseract, EasyOCR), diagram structure analysis, chart/graph analysis, and entity detection. Results are deduplicated and combined for comprehensive coverage.</li> <li><code>get_image_summary(image_bytes, llm)</code>: Uses a vision-capable LLM to generate descriptive summaries for images, including objects, text, and visual features.</li> </ol> <p>code snippet: Image Processing Pipeline</p> <pre><code>class ImageProcessor:\n    \"\"\"Advanced image processing for OCR and visual understanding.\"\"\"\n\n    def __init__(self):\n        # Initialize EasyOCR reader for handwritten text\n        self.ocr_reader = easyocr.Reader(['en'])\n\n    def preprocess_image_for_ocr(self, image: Image.Image) -&gt; Image.Image:\n        \"\"\"Preprocess image to improve OCR accuracy.\"\"\"\n        # Convert to grayscale\n        if image.mode != 'L':\n            image = image.convert('L')\n\n        # Enhance contrast\n        enhancer = ImageEnhance.Contrast(image)\n        image = enhancer.enhance(2.0)\n\n        # Apply sharpening filter\n        image = image.filter(ImageFilter.SHARPEN)\n\n        # Convert to OpenCV format for advanced processing\n        opencv_image = cv2.cvtColor(np.array(image), cv2.COLOR_GRAY2BGR)\n\n        # Apply morphological operations to clean up text\n        kernel = np.ones((1,1), np.uint8)\n        opencv_image = cv2.morphologyEx(opencv_image, cv2.MORPH_CLOSE, kernel)\n\n        # Convert back to PIL\n        processed_image = Image.fromarray(cv2.cvtColor(opencv_image, cv2.COLOR_BGR2GRAY))\n        return processed_image\n\n    def extract_text_comprehensive(self, image: Image.Image) -&gt; Dict[str, Union[str, List[Dict]]]:\n        \"\"\"Extract text using both OCR methods and combine results.\"\"\"\n        tesseract_result = self.extract_text_tesseract(image)\n        easyocr_result = self.extract_text_easyocr(image)\n\n        # Combine results - prioritize EasyOCR for handwritten text detection\n        combined_text_blocks = []\n        combined_text = \"\"\n\n        # Use EasyOCR results as primary\n        if easyocr_result['text_blocks']:\n            combined_text_blocks.extend(easyocr_result['text_blocks'])\n            combined_text += easyocr_result['full_text'] + \" \"\n\n        # Add Tesseract results if they provide additional information\n        if tesseract_result['text_blocks']:\n            # Simple deduplication - avoid adding very similar text\n            for block in tesseract_result['text_blocks']:\n                is_duplicate = any(\n                    self._text_similarity(block['text'], existing['text']) &gt; 0.8\n                    for existing in combined_text_blocks\n                )\n                if not is_duplicate:\n                    combined_text_blocks.append({**block, 'method': 'tesseract'})\n                    combined_text += block['text'] + \" \"\n\n        return {\n            'full_text': combined_text.strip(),\n            'text_blocks': combined_text_blocks,\n            'method': 'comprehensive'\n        }\n</code></pre> <ol> <li> <p>Universal Ingestion</p> </li> <li> <p><code>ingest_file(path, file_id, folder_id, file_type)</code>: Orchestrates detection, loading, chunking, embedding, and storage. Handles multimodal files and updates the knowledge graph if enabled.</p> </li> <li> <p>Retrieval &amp; Answer Generation</p> </li> <li><code>_rewrite_query(q)</code>: Refines user queries for better retrieval.</li> <li><code>_search_docs_with_scores(vs, query, where, k)</code>: Retrieves relevant chunks from the vector store, using semantic similarity and filtering by scope.</li> <li><code>rag_answer(query, scope, folder_id, file_id, k)</code>: Main function for answering queries\u2014retrieves context, generates answers with LLM, and provides citations and confidence scores.</li> </ol> <p>code snippet: Query Processing with Confidence</p> <pre><code>def rag_answer(query: str, scope: str = \"drive\", folder_id: Optional[str] = None,\n               file_id: Optional[str] = None, k: int = 10) -&gt; Dict:\n    \"\"\"\n    Non-streaming answer. (Your UI can still prefer SSE.)\n    Includes namespaced cache and confidence gating.\n    \"\"\"\n    vs = get_vs()\n    cache_vs = get_cache_vs()\n\n    # Auto-detect file scope based on query content\n    scope, file_id = _auto_detect_file_scope(query, vs, scope, file_id)\n\n    # scope filter\n    where: Dict[str, str] = {}\n    if scope == \"folder\" and folder_id:\n        where[\"folder_id\"] = folder_id\n    if scope == \"file\" and file_id:\n        where[\"file_id\"] = file_id\n\n    print(f\"[RAG] rag_answer called with scope={scope}, file_id={file_id}, folder_id={folder_id}\")\n    print(f\"[RAG] Where filter: {where}\")\n\n    # namespaced cache key/query\n    cache_query = f\"scope={scope}|folder={folder_id}|file={file_id}|q={query}\"\n    try:\n        ch = cache_vs.similarity_search_with_score(cache_query, k=1)\n        if ch:\n            doc, dist = ch[0]\n            if dist &lt;= CACHE_DISTANCE_MAX:\n                meta = doc.metadata or {}\n                if \"answer_json\" in meta:\n                    return json.loads(meta[\"answer_json\"])\n    except Exception:\n        pass\n\n    # retrieval\n    q_lower = query.lower()\n    if (\"summarize\" in q_lower or \"summary\" in q_lower) and (scope == \"file\" and file_id):\n        rewritten_query = query\n        pairs = _search_docs_with_scores(vs, \"summary\", where, k=max(k, 30))\n    else:\n        rewritten_query = _rewrite_query(query)\n        kw = \" \".join(_keywords(query))\n        boosted = f\"{rewritten_query} {kw}\".strip()\n        pairs = _search_docs_with_scores(vs, boosted, where, k=k)\n\n    docs = [d for d, _ in pairs]\n    distances = [dist for _, dist in pairs]\n    avg_distance = (sum(distances)/len(distances)) if distances else 1.0\n    confidence = round(_confidence_from_distances(distances), 3)\n\n    if not docs or avg_distance &gt; CONF_DISTANCE_MAX:\n        return {\n            \"answer\": \"I don't know based on the indexed files.\",\n            \"citations\": [],\n            \"rewritten_query\": rewritten_query,\n            \"confidence\": confidence,\n        }\n\n    # build base context from vector hits\n    ctx = _context_block(docs, settings.MAX_CONTEXT_CHARS)\n\n    # prepend Graph Facts when enabled\n    graphrag_funcs = get_graphrag_functions()\n    if _GR_ENABLED and graphrag_funcs:\n        try:\n            ents = graphrag_funcs['extract_query_entities'](query)\n            # Use confidence-based graph fact retrieval with minimum confidence filtering\n            gctx = graphrag_funcs['get_subgraph_facts'](\n                ents,\n                file_id=file_id if scope == \"file\" else None,\n                folder_id=folder_id if scope == \"folder\" else None,\n                max_facts=settings.MAX_CONTEXT_CHARS // 100,  # Adjust based on context limit\n                min_confidence=0.6  # Only include high-confidence facts\n            )\n            if gctx:\n                ctx = f\"# Graph Facts\\n{gctx}\\n---\\n\" + ctx\n        except Exception:\n            pass\n\n    prompt = f\"{SYSTEM_PROMPT}\\n\\n# Question\\n{query}\\n\\n# Context\\n{ctx}\\n# Answer\"\n    answer_text = response_llm().invoke(prompt).content.strip()  # Use separate response model\n\n    citations = []\n    for d in docs:\n        m = d.metadata or {}\n        citations.append({\n            \"file_id\": m.get(\"file_id\"),\n            \"chunk_no\": m.get(\"chunk_no\"),\n            \"page\": m.get(\"page\"),\n            \"source\": m.get(\"source\"),\n        })\n</code></pre> <ol> <li>Conversation Awareness</li> <li><code>detect_followup_question(current_query, conversation_history)</code>: Detects follow-up questions using heuristics and LLM analysis, preserving context for multi-turn conversations.</li> <li><code>conversation_aware_rag(query, conversation_history, scope, folder_id, file_id, k)</code>: Maintains context across turns, adjusts retrieval scope, and enhances query with conversation history for coherent answers.</li> </ol>"},{"location":"ai_pipeline/overview/","title":"AI-Powered Cross-Platform File Storage System","text":""},{"location":"ai_pipeline/overview/#overview","title":"Overview","text":"<p>This backend is designed to power a next-generation, secure, and intelligent file storage platform, inspired by services like Google Drive and OneDrive but enhanced with advanced AI capabilities. It supports both web and mobile clients, providing:</p> <ul> <li>Multi-modal support: Handles documents (PDF, DOCX, TXT, PPTX), spreadsheets (CSV, XLSX), and images (JPG, PNG), with robust parsing and indexing for each format.</li> <li>AI-powered search and chat: Enables users to ask natural language questions across all their files, with answers grounded in indexed content and supported by citations.</li> <li>Knowledge graph construction: Extracts entities and relationships from files using LLMs and stores them in a Neo4j graph database, enabling cross-file and cross-modal reasoning.</li> <li>Multimodal analysis: Processes images for text (OCR), diagrams, charts, and entities using Tesseract, EasyOCR, and vision models, making visual content searchable and explorable.</li> <li>Privacy and security: Implements user data isolation and access control, ensuring that AI and retrieval functions only operate on data the user is permitted to access.</li> </ul> <p>The system is modular, extensible, and designed for competition-grade performance, with a focus on explainability, analytics, and user-centric privacy.</p>"},{"location":"ai_pipeline/overview/#why-not-simple-rag","title":"Why Not Simple RAG?","text":"<p>Simple RAG systems retrieve text chunks and generate answers using LLMs, but they lack explicit modeling of entities, relationships, and provenance. This limits their ability to:</p> <ul> <li>Aggregate knowledge across files and modalities: Simple RAG cannot connect information from different sources or understand cross-file relationships.</li> <li>Provide source-linked, explainable answers: Without entity tracking, it's difficult to provide precise citations and explain reasoning paths.</li> <li>Visualize knowledge connections: Simple vector search doesn't capture the rich relationship structure that enables knowledge graph visualization.</li> <li>Filter by confidence or validate extracted information: Simple RAG lacks confidence scoring and validation mechanisms for extracted facts.</li> <li>Support multi-hop reasoning and advanced analytics: Vector similarity alone cannot support complex queries requiring multiple inference steps.</li> </ul> <p>GraphRAG overcomes these limitations by:</p> <ol> <li>Building a knowledge graph from all files: Entities and relations are extracted and linked, creating a structured knowledge representation.</li> <li>Supporting context-aware, cross-file, and cross-modal retrieval: Graph queries can traverse relationships and combine vector similarity with structured knowledge.</li> <li>Providing source-linked answers: Every fact is linked back to its source chunks, enabling precise citations and provenance tracking.</li> <li>Enabling advanced analytics: Graph statistics, confidence metrics, and relationship analysis support deeper insights.</li> <li>Supporting explainable AI: The graph structure makes reasoning paths visible and interpretable.</li> </ol> <p>This approach is essential for the intelligent, secure, and multi-modal file storage system described above, where users need to explore complex relationships across diverse content types.</p>"},{"location":"api/authentication/","title":"Authentication API for TheDrive","text":"<p>This document provides a detailed overview of the Authentication API implemented in TheDrive using FastAPI. The API includes endpoints for signup, login, logout, and retrieving user details. It uses JWT tokens for session management and bcrypt for password hashing.</p>"},{"location":"api/authentication/#1-endpoints-overview","title":"1. Endpoints Overview","text":"<p>The Authentication API consists of the following endpoints:</p> <ol> <li>POST /signup: Registers a new user.</li> <li>POST /login: Authenticates a user and issues a JWT token.</li> <li>POST /logout: Logs out the user by deleting the authentication cookie.</li> <li>GET /me: Fetches the current logged-in user's details.</li> </ol>"},{"location":"api/authentication/#2-endpoint-details","title":"2. Endpoint Details","text":""},{"location":"api/authentication/#21-post-signup","title":"2.1 POST /signup","text":"<p>This endpoint allows a new user to register by providing an email and a password. It checks if the email is already registered in the system and, if not, hashes the password and stores the new user's details in the PostgreSQL database.</p>"},{"location":"api/authentication/#request-body","title":"Request Body:","text":"<pre><code>json\n{\n    \"email\": \"user@example.com\",\n    \"password\": \"strongpassword\"\n}\n</code></pre> <p>Response:</p> <p>201 Created: User is successfully created.</p> <p>409 Conflict: Email is already registered.</p> <p>Example Response:</p> <p>{     \"email\": \"user@example.com\",     \"hashed_password\": \"$2b$12$...\",     \"id\": 1 }</p> <p>Code Example: @router.post(\"/signup\", response_model=schemas.User, status_code=status.HTTP_201_CREATED) def signup(user: schemas.UserCreate, db: Session = Depends(database.get_db)):     db_user = get_user(db, email=user.email)     if db_user:         raise HTTPException(status_code=status.HTTP_409_CONFLICT, detail=\"Email already registered\")     hashed_password = get_password_hash(user.password)     db_user = models.User(email=user.email, hashed_password=hashed_password)     db.add(db_user)     db.commit()     db.refresh(db_user)     return db_user</p> <p>2.2 POST /login</p> <p>This endpoint authenticates a user. It accepts the email and password via the OAuth2PasswordRequestForm and returns an access token (JWT) in a cookie for session management.</p> <p>Request Body: {     \"username\": \"user@example.com\",     \"password\": \"strongpassword\" }</p> <p>Response:</p> <p>200 OK: Login successful, JWT token is issued and set in the access_token cookie.</p> <p>401 Unauthorized: Incorrect email or password.</p> <p>Example Response:</p> <p>{     \"message\": \"Login successful\" }</p> <p>Code Example:</p> <pre><code>@router.post(\"/login\")\ndef login(response: Response, db: Session = Depends(database.get_db), form_data: OAuth2PasswordRequestForm = Depends()):\n    user = get_user(db, email=form_data.username)\n    if not user or not verify_password(form_data.password, user.hashed_password):\n        raise HTTPException(\n            status_code=status.HTTP_401_UNAUTHORIZED,\n            detail=\"Incorrect email or password\",\n        )\n    access_token = create_access_token(data={\"sub\": user.email})\n\n    response.set_cookie(\n        key=\"access_token\", \n        value=access_token,  # UPDATED: No \"Bearer \" prefix\n        httponly=True,\n        samesite='lax',\n    )\n    return {\"message\": \"Login successful\"}\n</code></pre> <p>2.3 POST /logout</p> <p>This endpoint logs out the current user by deleting the access_token cookie. It invalidates the user's session.</p> <p>Request Body:</p> <p>None</p> <p>Response:</p> <p>200 OK: Logout successful, access_token cookie is deleted.</p> <p>Example Response:</p> <p>{     \"message\": \"Logout successful\" }</p> <p>Code Example:</p> <pre><code>@router.post(\"/logout\")\ndef logout(response: Response):\n    response.delete_cookie(\"access_token\")\n    return {\"message\": \"Logout successful\"}\n</code></pre> <p>2.4 GET /me</p> <p>This endpoint fetches the details of the currently logged-in user. The access_token is retrieved from the cookie, and the user's identity is validated using the token. The user's details are returned if the token is valid.</p> <p>Response :</p> <p>200 OK : User details are returned.</p> <p>401 Unauthorized : If the access_token is not provided or is invalid.</p> <p>Example Response:</p> <pre><code>{\n    \"email\": \"user@example.com\",\n    \"id\": 1\n}\n</code></pre> <p>Code Example:</p> <pre><code>@router.get(\"/me\", response_model=schemas.User)\ndef read_users_me(current_user: models.User = Depends(get_current_user)):\n    \"\"\"\n    Fetch the current logged in user by verifying the cookie.\n    \"\"\"\n    return current_user\n\n\n</code></pre> <ol> <li>Security Features 3.1 Password Hashing</li> </ol> <p>bcrypt is used for password hashing. When a user registers, their password is hashed and stored in the database. During login, the entered password is compared to the hashed version in the database to verify the credentials.</p> <p>3.2 Token-Based Authentication</p> <p>The system uses JWT tokens to manage user sessions. The JWT token is generated after successful login and stored in a secure, HTTP-only cookie to prevent XSS attacks. The token includes the user's email (as a claim) and an expiration time.</p> <p>Token Expiration: The token expires after a pre-configured amount of time (e.g., 30 minutes), requiring the user to log in again.</p> <p>3.3 Cookie Security</p> <p>The JWT token is stored in an HTTP-only cookie, which makes it inaccessible to JavaScript running in the user's browser. This prevents XSS attacks by ensuring that malicious scripts cannot access the authentication token.</p> <p>SameSite Cookie Attribute: The SameSite attribute is set to lax to provide protection against cross-site request forgery (CSRF) attacks.</p> <p>3.4 Session Management</p> <p>Login: When a user logs in, the backend generates a JWT token and stores it in the user's cookies.</p> <p>Logout: The /logout endpoint removes the access_token cookie, effectively ending the user's session.</p> <p>User Data: The /me endpoint uses the token to retrieve the current user's data from the database.</p> <ol> <li>Data Flow for Authentication 4.1 Sign-up Flow</li> </ol> <p>User enters their email and password.</p> <p>The backend checks if the email already exists in the database.</p> <p>If the email is not registered, the backend hashes the password and stores the user details in the database.</p> <p>The backend returns the user data (without the password) as a response.</p> <p>4.2 Login Flow</p> <p>User enters their email and password.</p> <p>The backend checks if the provided credentials are correct by verifying the password hash.</p> <p>If valid, the backend generates a JWT token and sends it to the user in an HTTP-only cookie.</p> <p>The user can now access protected endpoints by sending the token with subsequent requests.</p> <p>4.3 Logout Flow</p> <p>User clicks on the Logout button, which triggers the /logout endpoint.</p> <p>The backend deletes the access_token cookie.</p> <p>The user is logged out and must log in again to obtain a new token.</p> <p>4.4 Fetch User Data Flow</p> <p>User makes a GET /me request.</p> <p>The backend retrieves the access_token from the user's cookies.</p> <p>The backend decodes the token to verify the user's identity.</p> <p>The user's data is returned in the response.</p> <ol> <li>Conclusion</li> </ol> <p>TheDrive\u2019s Authentication API ensures that users can securely sign up, log in, and manage their sessions with JWT tokens stored in cookies. Passwords are hashed using bcrypt, ensuring that sensitive data is not exposed. The JWT-based authentication system provides secure session management, while cookie security features like HTTP-only and SameSite protect against common web security threats such as XSS and CSRF.</p> <p>This authentication system enables secure and efficient user management for TheDrive, with easy-to-understand data flows for logging in, logging out, and fetching user details.</p>"},{"location":"api/authentication/#explanation","title":"Explanation:","text":"<ul> <li>The Authentication API consists of endpoints for signup, login, logout, and user details retrieval.</li> <li>JWT tokens are used for session management, and bcrypt ensures secure password storage.</li> <li>Cookie security with HTTP-only and SameSite attributes protects against common web vulnerabilities.</li> </ul>"},{"location":"api/chat_endpoints/","title":"RAG + Knowledge Graph (KAG) Architecture Documentation","text":""},{"location":"api/chat_endpoints/#overview","title":"Overview","text":"<p>TheDrive implements a sophisticated hybrid retrieval system that combines Retrieval-Augmented Generation (RAG) with Knowledge Graphs to provide intelligent document search and conversational AI capabilities. This system processes user documents through multiple layers of analysis to create both vector embeddings and structured knowledge representations.</p>"},{"location":"api/chat_endpoints/#system-architecture","title":"System Architecture","text":""},{"location":"api/chat_endpoints/#high-level-components","title":"High-Level Components","text":"<pre><code>graph TB\n    subgraph \"Frontend Layer\"\n        UI[React/Next.js UI]\n        Chat[Chat Interface]\n        Files[File Management]\n    end\n\n    subgraph \"API Layer\"\n        Backend[FastAPI Backend]\n        RagAPI[GraphRAG API]\n    end\n\n    subgraph \"Processing Layer\"\n        Ingestion[Document Ingestion]\n        Chunking[Text Chunking]\n        Embedding[Vector Embedding]\n        GraphExtraction[Entity/Relation Extraction]\n    end\n\n    subgraph \"Storage Layer\"\n        S3[AWS S3&lt;br/&gt;File Storage]\n        Postgres[PostgreSQL&lt;br/&gt;User/File Metadata]\n        Chroma[ChromaDB&lt;br/&gt;Vector Store]\n        Neo4j[Neo4j&lt;br/&gt;Knowledge Graph]\n    end\n\n    UI --&gt; Backend\n    Chat --&gt; RagAPI\n    Files --&gt; Backend\n    Backend --&gt; S3\n    Backend --&gt; Postgres\n    RagAPI --&gt; Ingestion\n    Ingestion --&gt; Chunking\n    Chunking --&gt; Embedding\n    Chunking --&gt; GraphExtraction\n    Embedding --&gt; Chroma\n    GraphExtraction --&gt; Neo4j\n</code></pre>"},{"location":"api/chat_endpoints/#document-ingestion-pipeline","title":"Document Ingestion Pipeline","text":""},{"location":"api/chat_endpoints/#1-file-upload-storage-flow","title":"1. File Upload &amp; Storage Flow","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant Backend\n    participant S3\n    participant Database\n    participant RAG_API\n\n    User-&gt;&gt;Frontend: Upload Document\n    Frontend-&gt;&gt;Backend: POST /upload\n    Backend-&gt;&gt;S3: Store File\n    S3--&gt;&gt;Backend: File URL\n    Backend-&gt;&gt;Database: Save File Metadata\n    Database--&gt;&gt;Backend: File ID\n    Backend-&gt;&gt;RAG_API: Trigger Ingestion\n    RAG_API--&gt;&gt;Backend: Ingestion Started\n    Backend--&gt;&gt;Frontend: Upload Success\n    Frontend--&gt;&gt;User: File Uploaded\n</code></pre>"},{"location":"api/chat_endpoints/#2-document-processing-pipeline","title":"2. Document Processing Pipeline","text":"<pre><code>graph LR\n    subgraph \"Document Ingestion\"\n        A[Raw Document] --&gt; B[File Type Detection]\n        B --&gt; C[Content Extraction]\n        C --&gt; D[Text Preprocessing]\n    end\n\n    subgraph \"Parallel Processing\"\n        D --&gt; E[Text Chunking]\n        D --&gt; F[Entity Extraction]\n\n        E --&gt; G[Vector Embedding]\n        F --&gt; H[Relation Extraction]\n        F --&gt; I[Entity Linking]\n\n        G --&gt; J[Store in ChromaDB]\n        H --&gt; K[Store in Neo4j]\n        I --&gt; K\n    end\n\n    subgraph \"Confidence Validation\"\n        K --&gt; L[Confidence Scoring]\n        L --&gt; M[Quality Filtering]\n        M --&gt; N[Graph Cleanup]\n    end\n</code></pre>"},{"location":"api/chat_endpoints/#rag-implementation","title":"RAG Implementation","text":""},{"location":"api/chat_endpoints/#traditional-rag-process","title":"Traditional RAG Process","text":"<pre><code>graph TD\n    A[User Query] --&gt; B[Query Preprocessing]\n    B --&gt; C[Vector Embedding]\n    C --&gt; D[Similarity Search in ChromaDB]\n    D --&gt; E[Retrieve Top-K Chunks]\n    E --&gt; F[Context Assembly]\n    F --&gt; G[LLM Prompt Construction]\n    G --&gt; H[Gemini API Call]\n    H --&gt; I[Generated Response]\n    I --&gt; J[Response Post-processing]\n    J --&gt; K[Return to User]\n</code></pre>"},{"location":"api/chat_endpoints/#enhanced-kag-process","title":"Enhanced KAG Process","text":"<pre><code>graph TD\n    A[User Query] --&gt; B[Query Analysis]\n    B --&gt; C[Entity Extraction from Query]\n    C --&gt; D[Parallel Retrieval]\n\n    subgraph \"Dual Retrieval System\"\n        D --&gt; E[Vector Search in ChromaDB]\n        D --&gt; F[Graph Traversal in Neo4j]\n\n        E --&gt; G[Text Chunks]\n        F --&gt; H[Graph Facts]\n    end\n\n    G --&gt; I[Context Enrichment]\n    H --&gt; I\n    I --&gt; J[Structured Context Assembly]\n    J --&gt; K[Enhanced LLM Prompt]\n    K --&gt; L[Gemini API with Rich Context]\n    L --&gt; M[Comprehensive Response]\n</code></pre>"},{"location":"api/chat_endpoints/#knowledge-graph-construction","title":"Knowledge Graph Construction","text":""},{"location":"api/chat_endpoints/#entity-and-relation-extraction","title":"Entity and Relation Extraction","text":"<pre><code>graph LR\n    subgraph \"Text Processing\"\n        A[Document Chunk] --&gt; B[Named Entity Recognition]\n        B --&gt; C[Relation Extraction]\n        C --&gt; D[Entity Linking]\n    end\n\n    subgraph \"Graph Construction\"\n        D --&gt; E[Entity Nodes Creation]\n        D --&gt; F[Relationship Edges Creation]\n        E --&gt; G[Property Assignment]\n        F --&gt; H[Confidence Scoring]\n    end\n\n    subgraph \"Quality Assurance\"\n        G --&gt; I[Validation Rules]\n        H --&gt; I\n        I --&gt; J[Confidence Thresholds]\n        J --&gt; K[Graph Storage in Neo4j]\n    end\n</code></pre>"},{"location":"api/chat_endpoints/#graph-schema","title":"Graph Schema","text":"<pre><code>// Example Neo4j Schema\n(:Document {id, name, type, owner_id})\n(:Entity {name, type, confidence, embedding_vector})\n(:Person {name, title, organization})\n(:Organization {name, industry, location})\n(:Concept {name, definition, category})\n\n// Relationships\n(:Entity)-[:MENTIONED_IN]-&gt;(:Document)\n(:Person)-[:WORKS_FOR]-&gt;(:Organization)\n(:Entity)-[:RELATED_TO {confidence, type}]-&gt;(:Entity)\n(:Document)-[:CONTAINS]-&gt;(:Entity)\n</code></pre>"},{"location":"api/chat_endpoints/#query-processing-flow","title":"Query Processing Flow","text":""},{"location":"api/chat_endpoints/#step-by-step-query-resolution","title":"Step-by-Step Query Resolution","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Frontend\n    participant RAG_API\n    participant LLM\n    participant ChromaDB\n    participant Neo4j\n\n    User-&gt;&gt;Frontend: Ask Question\n    Frontend-&gt;&gt;RAG_API: Query Request\n    RAG_API-&gt;&gt;RAG_API: Query Analysis\n    RAG_API-&gt;&gt;LLM: Extract Query Entities\n    LLM--&gt;&gt;RAG_API: Entity List\n\n    par Vector Search\n        RAG_API-&gt;&gt;ChromaDB: Similarity Search\n        ChromaDB--&gt;&gt;RAG_API: Relevant Chunks\n    and Graph Search\n        RAG_API-&gt;&gt;Neo4j: Graph Traversal\n        Neo4j--&gt;&gt;RAG_API: Connected Facts\n    end\n\n    RAG_API-&gt;&gt;RAG_API: Context Assembly\n    RAG_API-&gt;&gt;LLM: Enhanced Prompt\n    LLM--&gt;&gt;RAG_API: Generated Answer\n    RAG_API--&gt;&gt;Frontend: Response + Citations\n    Frontend--&gt;&gt;User: Final Answer\n</code></pre>"},{"location":"api/chat_endpoints/#context-enrichment-strategy","title":"Context Enrichment Strategy","text":""},{"location":"api/chat_endpoints/#information-layering","title":"Information Layering","text":"<pre><code>graph TB\n    subgraph \"Context Assembly\"\n        A[Raw Query] --&gt; B[Entity Extraction]\n        B --&gt; C[Graph Fact Retrieval]\n        B --&gt; D[Vector Similarity Search]\n\n        C --&gt; E[Structured Knowledge]\n        D --&gt; F[Unstructured Text Chunks]\n\n        E --&gt; G[Context Layering]\n        F --&gt; G\n\n        G --&gt; H[Final Context]\n    end\n\n    subgraph \"Context Structure\"\n        H --&gt; I[Graph Facts Section]\n        H --&gt; J[Raw Text Section]\n        H --&gt; K[Conversation Context]\n\n        I --&gt; L[Structured Knowledge Facts]\n        J --&gt; M[Source Document Chunks]\n        K --&gt; N[Previous Q&amp;A History]\n    end\n</code></pre>"},{"location":"api/chat_endpoints/#file-vector-db-mapping-system","title":"File-Vector DB Mapping System","text":""},{"location":"api/chat_endpoints/#re-ingestion-feature-flow","title":"Re-ingestion Feature Flow","text":"<pre><code>graph TD\n    A[User Login] --&gt; B[Check Ingestion Status API]\n    B --&gt; C[Query Database for File Status]\n    C --&gt; D{Files Need Processing?}\n\n    D --&gt;|Yes| E[Show Re-ingestion Button]\n    D --&gt;|No| F[Hide Button]\n\n    E --&gt; G[User Clicks Button]\n    G --&gt; H[Confirm Re-ingestion]\n    H --&gt; I[Start Background Processing]\n    I --&gt; J[Disable Chat Interface]\n    J --&gt; K[Process Pending Files]\n    K --&gt; L[Update File Status]\n    L --&gt; M[Poll Status Updates]\n    M --&gt; N{All Files Processed?}\n\n    N --&gt;|No| M\n    N --&gt;|Yes| O[Re-enable Chat]\n    O --&gt; P[Hide Button]\n</code></pre>"},{"location":"api/chat_endpoints/#database-schema-for-file-tracking","title":"Database Schema for File Tracking","text":"<pre><code>-- FileSystemItem table tracks ingestion status\nCREATE TABLE filesystem_items (\n    id VARCHAR PRIMARY KEY,\n    name VARCHAR NOT NULL,\n    type VARCHAR NOT NULL, -- 'file' or 'folder'\n    owner_id INTEGER REFERENCES users(id),\n    parent_id VARCHAR REFERENCES filesystem_items(id),\n    s3_key VARCHAR, -- For files only\n    mime_type VARCHAR,\n    size_bytes BIGINT,\n    ingestion_status VARCHAR, -- 'pending', 'processing', 'completed', 'failed'\n    created_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n</code></pre>"},{"location":"api/chat_endpoints/#confidence-based-quality-control","title":"Confidence-Based Quality Control","text":""},{"location":"api/chat_endpoints/#entity-confidence-scoring","title":"Entity Confidence Scoring","text":"<pre><code>graph LR\n    subgraph \"Confidence Metrics\"\n        A[Entity Extraction] --&gt; B[LLM Confidence Score]\n        A --&gt; C[Named Entity Recognition Score]\n        A --&gt; D[Context Relevance Score]\n        A --&gt; E[Cross-Reference Validation]\n    end\n\n    subgraph \"Scoring Algorithm\"\n        B --&gt; F[Weighted Average]\n        C --&gt; F\n        D --&gt; F\n        E --&gt; F\n        F --&gt; G[Final Confidence Score]\n    end\n\n    subgraph \"Quality Filtering\"\n        G --&gt; H{Score &gt;= Threshold?}\n        H --&gt;|Yes| I[Include in Graph]\n        H --&gt;|No| J[Exclude from Graph]\n    end\n</code></pre>"},{"location":"api/chat_endpoints/#graph-cleanup-process","title":"Graph Cleanup Process","text":"<pre><code>graph TD\n    A[Periodic Cleanup] --&gt; B[Identify Low-Confidence Entities]\n    B --&gt; C[Check Entity Usage]\n    C --&gt; D{Used in Queries?}\n\n    D --&gt;|Yes| E[Increase Confidence]\n    D --&gt;|No| F{Below Minimum Threshold?}\n\n    F --&gt;|Yes| G[Remove Entity]\n    F --&gt;|No| H[Keep Entity]\n\n    G --&gt; I[Update Connected Relations]\n    E --&gt; J[Maintain Entity]\n    H --&gt; J\n    I --&gt; J\n</code></pre>"},{"location":"api/chat_endpoints/#api-endpoints","title":"API Endpoints","text":""},{"location":"api/chat_endpoints/#key-rag-api-endpoints","title":"Key RAG API Endpoints","text":"<pre><code># Document Ingestion\nPOST /ingest/file\n  - Upload and process document\n  - Returns: ingestion_id, status\n\n# Query Processing\nPOST /query/stream\n  - Streaming RAG query with SSE\n  - Returns: Server-Sent Events stream\n\nGET /query\n  - Non-streaming RAG query\n  - Returns: answer, citations, confidence\n\n# Graph Management\nGET /graph/stats\n  - Graph statistics and health\n  - Returns: node_count, relationship_count, confidence_distribution\n\nPOST /graph/cleanup\n  - Clean low-confidence entities\n  - Returns: removed_count, updated_count\n\n# Status Checking\nGET /drive/check-ingestion-status\n  - Check user files needing ingestion\n  - Returns: needs_ingestion, files_count, is_active\n\nPOST /drive/reingest-files\n  - Trigger re-ingestion of pending files\n  - Returns: processed_count, failed_files\n</code></pre>"},{"location":"api/chat_endpoints/#performance-optimizations","title":"Performance Optimizations","text":""},{"location":"api/chat_endpoints/#caching-strategy","title":"Caching Strategy","text":"<pre><code>graph LR\n    subgraph \"Multi-Level Caching\"\n        A[User Query] --&gt; B[Query Cache Check]\n        B --&gt; C{Cache Hit?}\n\n        C --&gt;|Yes| D[Return Cached Result]\n        C --&gt;|No| E[Process Query]\n\n        E --&gt; F[Vector Cache Check]\n        F --&gt; G[Graph Cache Check]\n        G --&gt; H[LLM Processing]\n        H --&gt; I[Cache Result]\n        I --&gt; J[Return Response]\n    end\n</code></pre>"},{"location":"api/chat_endpoints/#parallel-processing","title":"Parallel Processing","text":"<pre><code>graph TB\n    subgraph \"Concurrent Operations\"\n        A[Query Received] --&gt; B[Split Processing]\n\n        B --&gt; C[Vector Search Thread]\n        B --&gt; D[Graph Search Thread]\n        B --&gt; E[Entity Extraction Thread]\n\n        C --&gt; F[ChromaDB Query]\n        D --&gt; G[Neo4j Query]\n        E --&gt; H[LLM Entity Analysis]\n\n        F --&gt; I[Result Aggregation]\n        G --&gt; I\n        H --&gt; I\n\n        I --&gt; J[Context Assembly]\n        J --&gt; K[Final Response]\n    end\n</code></pre>"},{"location":"api/chat_endpoints/#configuration-settings","title":"Configuration Settings","text":""},{"location":"api/chat_endpoints/#environment-variables","title":"Environment Variables","text":"<pre><code># GraphRAG Configuration\nENABLE_GRAPHRAG=1\nENTITY_CONFIDENCE_THRESHOLD=0.6\nRELATION_CONFIDENCE_THRESHOLD=0.7\nENABLE_EMBEDDING_VALIDATION=1\n\n# Vector Database\nCHROMA_URL=http://chroma:8000\nCOLLECTION=thedrive\n\n# Graph Database\nNEO4J_URL=bolt://neo4j:7687\nNEO4J_USER=neo4j\nNEO4J_PASSWORD=password123\n\n# LLM Configuration\nGEMINI_API_KEY=your_api_key\nMODEL_NAME=gemini-1.5-pro\nRESPONSE_MODEL=gemini-2.5-pro\n\n# Context Limits\nMAX_CONTEXT_CHARS=12000\n</code></pre>"},{"location":"api/chat_endpoints/#monitoring-and-analytics","title":"Monitoring and Analytics","text":""},{"location":"api/chat_endpoints/#system-health-metrics","title":"System Health Metrics","text":"<pre><code>graph LR\n    subgraph \"Performance Metrics\"\n        A[Query Response Time]\n        B[Vector Search Latency]\n        C[Graph Query Performance]\n        D[LLM API Response Time]\n    end\n\n    subgraph \"Quality Metrics\"\n        E[Answer Relevance Score]\n        F[Citation Accuracy]\n        G[Graph Fact Precision]\n        H[User Satisfaction Rating]\n    end\n\n    subgraph \"System Metrics\"\n        I[Database Connection Pool]\n        J[Memory Usage]\n        K[Storage Utilization]\n        L[API Error Rates]\n    end\n</code></pre>"},{"location":"api/chat_endpoints/#benefits-of-the-hybrid-approach","title":"Benefits of the Hybrid Approach","text":""},{"location":"api/chat_endpoints/#rag-kag-advantages","title":"RAG + KAG Advantages","text":"<ol> <li>Enhanced Context Understanding</li> <li>Vector search provides semantic similarity</li> <li>Knowledge graph provides structured relationships</li> <li> <p>Combined approach offers comprehensive context</p> </li> <li> <p>Improved Answer Quality</p> </li> <li>Factual accuracy through structured knowledge</li> <li>Contextual relevance through vector similarity</li> <li>Reduced hallucination through grounded facts</li> </ol>"},{"location":"api/file-operations/","title":"File Operations API for TheDrive","text":"<p>This document provides a detailed overview of the File Operations API for TheDrive. It includes endpoints for file uploads, folder creation, file management, AI processing, and other file-related operations.</p>"},{"location":"api/file-operations/#1-endpoints-overview","title":"1. Endpoints Overview","text":"<p>The following endpoints are available in the File Operations API:</p> <ol> <li>POST /upload: Uploads a file to AWS S3, stores metadata in the database, and optionally triggers AI processing.</li> <li>POST /folder: Creates a new folder within a user's file system.</li> <li>PUT /item/{item_id}: Renames an existing file or folder.</li> <li>DELETE /item/{item_id}: Deletes a file or folder, including removal from AWS S3 and related AI indexes.</li> <li>GET /items: Retrieves a list of files and folders for a specific user and parent folder.</li> <li>GET /item/{item_id}/view-link: Generates a pre-signed URL for viewing a file.</li> <li>GET /item/{item_id}/ai-status: Retrieves the AI processing status for a file.</li> </ol>"},{"location":"api/file-operations/#2-endpoint-details","title":"2. Endpoint Details","text":""},{"location":"api/file-operations/#21-post-upload","title":"2.1 POST /upload","text":"<p>This endpoint allows users to upload files to the system. The file is stored in AWS S3, and metadata (such as file name, type, and S3 key) is saved in the PostgreSQL database. If GraphRAG (AI processing) is available, the file will be processed in the background.</p>"},{"location":"api/file-operations/#request-body","title":"Request Body:","text":"<ul> <li>parentId: The ID of the parent folder (can be 'root' for the main folder).</li> <li>file: The file to be uploaded.</li> </ul>"},{"location":"api/file-operations/#response","title":"Response:","text":"<ul> <li>201 Created: Successfully uploaded the file.</li> </ul> <p>Example Response:</p> <pre><code>json\n{\n    \"id\": \"file-12345\",\n    \"name\": \"document.pdf\",\n    \"type\": \"file\",\n    \"s3_key\": \"user123/file-12345/document.pdf\",\n    \"size_bytes\": 102400,\n    \"mime_type\": \"application/pdf\",\n    \"ai_processed\": false,\n    \"ai_processing_status\": \"pending\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.post(\"/upload\", response_model=schemas.FileSystemItem, status_code=201)\ndef upload_file(\n    parentId: str,\n    background_tasks: BackgroundTasks,\n    file: UploadFile = File(...),\n    db: Session = Depends(database.get_db),\n    current_user: models.User = Depends(get_current_user)\n):\n    file_id = f\"file-{uuid.uuid4()}\"\n    s3_key = f\"{current_user.id}/{file_id}/{file.filename}\"\n\n    # Read file content into memory safely\n    contents = file.file.read()\n    file_size = len(contents)\n\n    # Upload to S3\n    s3_service.upload_file_obj(\n        io.BytesIO(contents),\n        settings.S3_BUCKET_NAME,\n        s3_key,\n        extra_args={\n            \"ContentType\": file.content_type,\n            \"ContentDisposition\": \"inline\",\n        }\n    )\n\n    # Save metadata to DB\n    new_file = models.FileSystemItem(\n        id=file_id,\n        name=file.filename,\n        type=\"file\",\n        s3_key=s3_key,\n        mime_type=file.content_type,\n        size_bytes=file_size,\n        owner_id=current_user.id,\n        parent_id=None if parentId == 'root' else parentId,\n        ai_processed=False,\n        ai_processing_status=\"pending\"\n    )\n\n    db.add(new_file)\n    db.commit()\n    db.refresh(new_file)\n\n    # Process file for GraphRAG automatically\n    if GRAPHRAG_AVAILABLE:\n        background_tasks.add_task(\n            process_file_for_graphrag,\n            file_id=new_file.id,\n            s3_key=s3_key,\n            user_id=current_user.id,\n            filename=file.filename\n        )\n\n    return new_file\n</code></pre> <p>2.2 POST /folder</p> <p>This endpoint creates a new folder within a user's file system. The folder is saved in the database, and the parent folder (if any) is specified.</p> <pre><code>Request Body:\n{\n    \"name\": \"New Folder\",\n    \"parentId\": \"root\"\n}\n</code></pre> <p>Response:</p> <pre><code>201 Created: Successfully created the folder.\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"id\": \"folder-12345\",\n    \"name\": \"New Folder\",\n    \"type\": \"folder\",\n    \"owner_id\": 1,\n    \"parent_id\": \"root\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.post(\"/folder\", response_model=schemas.FileSystemItem, status_code=201)\ndef create_folder(folder: schemas.FolderCreate, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    parent_db_id = None if folder.parentId == 'root' else folder.parentId\n\n    # Check for duplicates\n    existing = db.query(models.FileSystemItem).filter_by(\n        owner_id=current_user.id, parent_id=parent_db_id, name=folder.name\n    ).first()\n    if existing:\n        raise HTTPException(status_code=409, detail=\"An item with this name already exists\")\n\n    new_folder = models.FileSystemItem(\n        id=f\"folder-{uuid.uuid4()}\", name=folder.name, type=\"folder\",\n        owner_id=current_user.id, parent_id=parent_db_id\n    )\n    db.add(new_folder)\n    db.commit()\n    db.refresh(new_folder)\n    return new_folder\n</code></pre> <p>2.3 PUT /item/{item_id}</p> <p>This endpoint allows users to rename a file or folder. It ensures that the new name doesn't conflict with other existing items in the same directory.</p> <p>Request Body:</p> <pre><code>{\n    \"name\": \"Updated File Name\"\n}\n</code></pre> <p>Response:</p> <pre><code>200 OK: Successfully renamed the item.\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"id\": \"file-12345\",\n    \"name\": \"Updated File Name\",\n    \"type\": \"file\",\n    \"s3_key\": \"user123/file-12345/document.pdf\",\n    \"size_bytes\": 102400,\n    \"mime_type\": \"application/pdf\",\n    \"ai_processed\": false,\n    \"ai_processing_status\": \"pending\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.put(\"/item/{item_id}\", response_model=schemas.FileSystemItem)\ndef rename_item(item_id: str, item_update: schemas.ItemUpdate, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    item = db.query(models.FileSystemItem).filter_by(id=item_id, owner_id=current_user.id).first()\n    if not item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    # Check for name conflicts\n    existing = db.query(models.FileSystemItem).filter(\n        models.FileSystemItem.id != item_id,\n        models.FileSystemItem.parent_id == item.parent_id,\n        models.FileSystemItem.name == item_update.name,\n        models.FileSystemItem.owner_id == current_user.id\n    ).first()\n    if existing:\n        raise HTTPException(status_code=409, detail=\"An item with this name already exists\")\n\n    item.name = item_update.name\n    db.commit()\n    db.refresh(item)\n    return item\n</code></pre> <p>2.4 DELETE /item/{item_id}</p> <p>This endpoint deletes a file or folder. If the item is a file, it will also be removed from AWS S3.</p> <p>Response:</p> <pre><code>204 No Content: Successfully deleted the item.\n</code></pre> <p>Example Code:</p> <pre><code>@router.delete(\"/item/{item_id}\", status_code=204)\ndef delete_item(item_id: str, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    item = db.query(models.FileSystemItem).filter_by(id=item_id, owner_id=current_user.id).first()\n    if not item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    if item.type == 'file' and item.s3_key:\n        s3_service.delete_file(settings.S3_BUCKET_NAME, item.s3_key)\n        # Add background task to remove from AI indexes\n\n    db.delete(item)\n    db.commit()\n</code></pre> <p>2.5 GET /item/{item_id}/view-link</p> <p>This endpoint generates a pre-signed URL for viewing a file directly from AWS S3. This URL can be used to access the file for a limited time.</p> <p>Response:</p> <pre><code>200 OK: Returns the pre-signed URL for viewing the file.\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"url\": \"https://example.com/s3_presigned_url\"\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.get(\"/item/{item_id}/view-link\", response_model=schemas.ViewLinkResponse)\ndef get_view_link(item_id: str, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    item = db.query(models.FileSystemItem).filter_by(id=item_id, owner_id=current_user.id).first()\n    if not item or item.type != 'file' or not item.s3_key:\n        raise HTTPException(status_code=404, detail=\"File not found or is not a viewable file.\")\n\n    url = s3_service.generate_presigned_url(settings.S3_BUCKET_NAME, item.s3_key)\n    return {\"url\": url}\n</code></pre> <p>2.6 GET /item/{item_id}/ai-status</p> <p>This endpoint allows users to retrieve the AI processing status of a file. The AI processing status includes information such as whether the file has been processed, the number of entities and relationships detected, and any processing errors.</p> <p>Response:</p> <pre><code>200 OK: Returns the AI processing status of the file.\n</code></pre> <p>Example Response:</p> <pre><code>{\n    \"file_id\": \"file-12345\",\n    \"processed\": true,\n    \"status\": \"completed\",\n    \"entities\": 20,\n    \"relationships\": 10,\n    \"communities\": 5\n}\n</code></pre> <p>Example Code:</p> <pre><code>@router.get(\"/item/{item_id}/ai-status\")\ndef get_ai_status(item_id: str, db: Session = Depends(database.get_db), current_user: models.User = Depends(get_current_user)):\n    \"\"\"Get AI processing status for a file\"\"\"\n    item = db.query(models.FileSystemItem).filter_by(id=item_id, owner_id=current_user.id).first()\n    if not item:\n        raise HTTPException(status_code=404, detail=\"Item not found\")\n\n    return {\n        \"file_id\": item_id,\n        \"processed\": getattr(item, 'ai_processed', False),\n        \"status\": getattr(item, 'ai_processing_status', 'pending'),\n        \"entities\": getattr(item, 'ai_entities', 0),\n        \"relationships\": getattr(item, 'ai_relationships', 0),\n        \"communities\": getattr(item, 'ai_communities', 0)\n    }\n</code></pre> <ol> <li>Conclusion</li> </ol> <p>The File Operations API for TheDrive provides robust functionality for managing files and folders, including uploading, creating, renaming, deleting, and viewing files. Additionally, it supports AI processing for file contents and generates presigned URLs for secure file access. The integration with AWS S3 and PostgreSQL ensures efficient and scalable file storage and metadata management.</p> <p>This API is designed to support users in managing their digital assets securely and intelligently, with the added benefit of AI-driven insights and graph-based search features.</p>"},{"location":"api/file-operations/#key-sections","title":"Key Sections:","text":"<ol> <li>File Upload: Handles file upload to AWS S3 and metadata saving to PostgreSQL.</li> <li>Folder Creation: Creates folders within the user\u2019s file system.</li> <li>File Management: Renaming and deletion of files, including removal from S3.</li> <li>AI Processing: Includes endpoints for checking the AI status of files and generating pre-signed URLs for file access.</li> </ol>"},{"location":"architecture/data-flow/","title":"Data Flow in TheDrive","text":"<p>This document provides a detailed explanation of the data flow in TheDrive, including how data moves through the system from the frontend to the backend, storage, AI processing, and how user queries are handled.</p>"},{"location":"architecture/data-flow/#1-high-level-data-flow","title":"1. High-Level Data Flow","text":"<p>TheDrive follows a user-centered flow where data is processed and managed through several stages:</p> <ol> <li>User Uploads File:</li> <li>The user uploads a file via the frontend interface.</li> <li> <p>The file is processed, stored, and metadata is stored in the backend.</p> </li> <li> <p>Metadata Storage:</p> </li> <li>File metadata (e.g., file name, type, size) is saved to PostgreSQL.</li> <li> <p>The file is uploaded to AWS S3 and its S3 key is saved in the database.</p> </li> <li> <p>AI Processing:</p> </li> <li>Once the file is uploaded, the system triggers AI processing to analyze the contents (e.g., text, images).</li> <li> <p>Metadata related to AI analysis (e.g., entities, relationships, communities) is stored in the database.</p> </li> <li> <p>User Queries Files:</p> </li> <li>The user interacts with the system by querying files via the frontend interface.</li> <li>The backend processes the query by referencing the file metadata and AI-driven insights (e.g., semantic search, AI-based content retrieval).</li> </ol>"},{"location":"architecture/data-flow/#2-detailed-data-flow","title":"2. Detailed Data Flow","text":""},{"location":"architecture/data-flow/#21-file-upload-process","title":"2.1 File Upload Process","text":""},{"location":"architecture/data-flow/#user-action","title":"User Action:","text":"<ul> <li>The user selects a file to upload through the Next.js frontend.</li> <li>The file is sent via an HTTP POST request to the backend API (FastAPI).</li> </ul>"},{"location":"architecture/data-flow/#backend-action","title":"Backend Action:","text":"<ul> <li>FastAPI receives the file and triggers the AWS S3 upload process.</li> <li>The file is stored in an S3 bucket. S3 generates a unique storage key for the file, which is used for future access and reference.</li> </ul>"},{"location":"architecture/data-flow/#metadata-storage","title":"Metadata Storage:","text":"<ul> <li>Once the file is uploaded to S3, metadata (such as file name, file type, file size, and S3 key) is stored in the PostgreSQL database.</li> <li>A new entry is created in the FileSystemItem table, which references the uploaded file.</li> </ul> <p>Example Data Flow: 1. User uploads file via frontend. 2. File is sent to FastAPI backend. 3. FastAPI stores the file in AWS S3. 4. File metadata is inserted into PostgreSQL.</p> <pre><code>python\n# FastAPI backend receives the file\ndef upload_file(file: UploadFile = File(...), db: Session = Depends(get_db)):\n    s3_key = upload_to_s3(file)  # Upload file to AWS S3\n    file_metadata = {\n        'name': file.filename,\n        'type': 'file',  # or 'folder'\n        's3_key': s3_key,\n        'size_bytes': len(file.file.read())\n    }\n    # Store metadata in PostgreSQL\n    new_file = models.FileSystemItem(**file_metadata)\n    db.add(new_file)\n    db.commit()\n    return {\"message\": \"File uploaded successfully!\"}\n</code></pre>"},{"location":"architecture/overview/","title":"TheDrive - System Architecture Overview","text":"<p>TheDrive is an AI-powered cloud storage system that integrates secure file storage, AI-driven content analysis, and intelligent querying for users. The system is designed with a microservices architecture that ensures scalability, security, and flexibility.</p>"},{"location":"architecture/overview/#1-high-level-architecture","title":"1. High-Level Architecture","text":"<p>TheDrive\u2019s architecture is divided into several key components:</p>"},{"location":"architecture/overview/#11-frontend-nextjs","title":"1.1 Frontend (Next.js)","text":"<ul> <li>Role: Provides a responsive user interface for file management, querying, and interacting with AI-powered features.</li> <li>Technologies: Next.js for server-side rendering and static page generation, Tailwind CSS for styling, and Axios for API requests.</li> </ul>"},{"location":"architecture/overview/#12-backend-fastapi","title":"1.2 Backend (FastAPI)","text":"<ul> <li>Role: Handles file management, user authentication, and interactions with AI models. The backend serves as the main API layer for the system.</li> <li>Technologies: FastAPI for building the RESTful API, SQLAlchemy for database interaction, and JWT for authentication.</li> </ul>"},{"location":"architecture/overview/#13-cloud-storage-aws-s3","title":"1.3 Cloud Storage (AWS S3)","text":"<ul> <li>Role: Stores files securely in the cloud with high scalability and availability.</li> <li>Technologies: AWS S3 for scalable file storage, leveraging IAM for secure access.</li> </ul>"},{"location":"architecture/overview/#14-database-postgresql-neo4j","title":"1.4 Database (PostgreSQL &amp; Neo4j)","text":"<ul> <li>PostgreSQL: Used for storing user credentials, file metadata, and other structured data.</li> <li>Neo4j: A graph database that models relationships between files and metadata, enabling AI-based queries and semantic search.</li> </ul>"},{"location":"architecture/overview/#15-ai-layer","title":"1.5 AI Layer","text":"<ul> <li>Role: Processes files for AI-driven insights, such as text extraction, entity recognition, and semantic search.</li> <li>Technologies: GraphRAG for processing documents, PyTorch/TensorFlow for machine learning models, and OCR for image processing.</li> </ul>"},{"location":"architecture/overview/#2-security-and-privacy","title":"2. Security and Privacy","text":"<ul> <li>JWT Authentication: Used for user authentication and maintaining secure sessions.</li> <li>End-to-End Encryption (E2EE): All user data is encrypted at rest and in transit (future enhancement, currently in progress).</li> <li>Data Segregation: Each user's data is isolated to ensure privacy and security.</li> </ul>"},{"location":"architecture/overview/#3-data-flow","title":"3. Data Flow","text":"<ol> <li>File Upload: Files are uploaded via the frontend, stored in AWS S3, and metadata is saved in PostgreSQL.</li> <li>AI Processing: Once a file is uploaded, it is processed by the AI layer to extract insights (e.g., entities, relationships).</li> <li>User Queries: Users can query files via the frontend, with AI-driven results fetched from PostgreSQL and Neo4j.</li> </ol>"},{"location":"architecture/overview/#conclusion","title":"Conclusion","text":"<p>TheDrive combines secure cloud storage, AI-powered processing, and advanced querying capabilities to offer users an intelligent and scalable solution for managing their digital assets. The system is built with a modular architecture that ensures flexibility, security, and high performance.</p>"},{"location":"architecture/security/","title":"Security Implementation in TheDrive","text":"<p>TheDrive focuses on ensuring user authentication and authorization within the system, protecting sensitive information through token-based authentication and secure password hashing. While end-to-end encryption (E2EE) is not implemented at this stage, user input is filtered during login to prevent unauthorized access.</p> <p>This document provides a detailed explanation of the login system, token creation, user authentication process, as well as the database schema for storing user credentials and file system metadata.</p>"},{"location":"architecture/security/#1-user-authentication","title":"1. User Authentication","text":""},{"location":"architecture/security/#11-password-hashing","title":"1.1 Password Hashing","text":"<p>To ensure that user passwords are stored securely, TheDrive uses bcrypt hashing for passwords. bcrypt is a widely adopted and secure password hashing algorithm that makes it difficult to reverse the hash into the original password.</p>"},{"location":"architecture/security/#functions","title":"Functions:","text":"<ul> <li> <p><code>verify_password(plain_password: str, hashed_password: str) -&gt; bool</code>:    This function compares the plain text password with the hashed password stored in the database.</p> </li> <li> <p><code>get_password_hash(password: str) -&gt; str</code>:    This function takes a plain password and hashes it using bcrypt before storing it in the database.</p> </li> </ul> <p>Example Code:</p> <pre><code>python\nfrom passlib.context import CryptContext\n\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\ndef verify_password(plain_password: str, hashed_password: str) -&gt; bool:\n    return pwd_context.verify(plain_password, hashed_password)\n\ndef get_password_hash(password: str) -&gt; str:\n    return pwd_context.hash(password)\n</code></pre> <p>1.2 Token Generation (JWT)</p> <p>TheDrive uses JSON Web Tokens (JWT) for session management and user authentication. Once the user logs in, they are issued a JWT token, which is used for subsequent requests to authenticate the user.</p> <p>Functions:</p> <ul> <li> <p><code>create_access_token(data: dict, expires_delta: Optional[timedelta] = None)</code> : This function generates a JWT token, encoding user information (email) along with an expiration time. If no expiration is provided, it defaults to the value defined in the settings.</p> </li> <li> <p><code>get_token_from_cookie(request: Request) -&gt; Optional[str]</code> : This function retrieves the JWT token from the cookies sent by the client. The token is stored in a cookie as part of the authentication flow.</p> </li> </ul> <p>Example Code:</p> <pre><code>from datetime import datetime, timedelta\nfrom typing import Optional\nfrom jose import JWTError, jwt\nfrom fastapi import Request\nfrom .config import settings\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=\"HS256\")\n    return encoded_jwt\n\ndef get_token_from_cookie(request: Request) -&gt; Optional[str]:\n    return request.cookies.get(\"access_token\")\n</code></pre> <p>1.3 User Authentication Flow</p> <p>Once a user logs in, a JWT token is created and returned in the response. This token is then stored in the client's cookies and used for authentication in subsequent requests.</p> <p>User Login and Token Creation:</p> <p>User enters their email and password.</p> <p>The password is verified by comparing the plain password with the hashed password stored in the database.</p> <p>If the credentials are valid, an access token (JWT) is generated and sent back to the client.</p> <p>Token Validation :</p> <p>For subsequent requests, the token is sent with the request (stored in the cookies).</p> <p>The token is decoded using the SECRET_KEY.</p> <p>If the token is valid and not expired, the user is authenticated, and their details are fetched from the database.</p> <p>Example Code for Token Validation:</p> <pre><code>from fastapi import Depends, HTTPException, status\nfrom sqlalchemy.orm import Session\nfrom .database import get_db\nfrom . import models\n\ndef get_current_user(token: str = Depends(get_token_from_cookie), db: Session = Depends(get_db)):\n    if token is None:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Not authenticated\")\n\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n    )\n    try:\n        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[\"HS256\"])\n        email: str = payload.get(\"sub\")\n        if email is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n    user = db.query(models.User).filter(models.User.email == email).first()\n    if user is None:\n        raise credentials_exception\n    return user\n</code></pre> <ol> <li>Database Schema</li> </ol> <p>The User and FileSystemItem models are designed to store essential information about users and their uploaded files. These models are implemented using SQLAlchemy ORM and are used to persist data in the PostgreSQL database.</p> <p>2.1 User Model</p> <p>The User model is used to store user credentials (email and hashed password) and their associated files.</p> <p>Fields:</p> <p><code>email</code> : The unique email address used by the user for login.</p> <p><code>hashed_password</code> : The hashed password of the user.</p> <p><code>items</code> : The relationship between users and their FileSystemItem (files or folders).</p> <p>User Model Example:</p> <pre><code>from sqlalchemy import Column, Integer, String\nfrom sqlalchemy.orm import relationship\nfrom .database import Base\n\nclass User(Base):\n    __tablename__ = \"users\"\n    id = Column(Integer, primary_key=True, index=True)\n    email = Column(String, unique=True, index=True, nullable=False)\n    hashed_password = Column(String, nullable=False)\n    items = relationship(\"FileSystemItem\", back_populates=\"owner\")\n</code></pre> <p>2.2 FileSystemItem Model</p> <p>The FileSystemItem model is used to store metadata about the files and folders uploaded by users. It tracks important file properties such as the file name, size, type, and S3 storage key.</p> <p>Fields:</p> <p>name: The name of the file or folder.</p> <p>type: The type of the item (either a file or a folder).</p> <p>s3_key: The key used to access the file stored in AWS S3.</p> <p>owner_id: The ID of the user who owns the file or folder.</p> <p>parent_id: The ID of the parent folder (if the item is a file within a folder).</p> <p>ai_processed: Boolean flag indicating whether the file has been processed by the AI pipeline.</p> <p>ai_processing_status: The current status of the AI processing (e.g., pending, completed).</p> <p>ai_entities: The number of entities detected by the AI in the file.</p> <p>ai_relationships: The number of relationships detected by the AI in the file.</p> <p>ai_communities: The number of communities detected by the AI in the file.</p> <p>FileSystemItem Model Example:</p> <pre><code>from sqlalchemy import Column, Integer, String, ForeignKey, Enum, Boolean\nfrom sqlalchemy.orm import relationship\nfrom .database import Base\n\nclass FileSystemItem(Base):\n    __tablename__ = \"filesystem_items\"\n    id = Column(String, primary_key=True, index=True)\n    name = Column(String, index=True)\n    type = Column(Enum(\"file\", \"folder\", name=\"item_type_enum\"), nullable=False)\n    s3_key = Column(String, unique=True, nullable=True)\n    mime_type = Column(String, nullable=True)\n    size_bytes = Column(Integer, nullable=True)\n\n    # AI processing fields\n    ai_processed = Column(Boolean, default=False)\n    ai_processing_status = Column(String, nullable=True, default=\"pending\")\n    ai_entities = Column(Integer, default=0)\n    ai_relationships = Column(Integer, default=0)\n    ai_communities = Column(Integer, default=0)\n\n    owner_id = Column(Integer, ForeignKey(\"users.id\"), nullable=False)\n    parent_id = Column(String, ForeignKey(\"filesystem_items.id\"), nullable=True)\n\n    owner = relationship(\"User\", back_populates=\"items\")\n    children = relationship(\"FileSystemItem\", backref=\"parent\", remote_side=[id], cascade=\"all, delete-orphan\", single_parent=True)\n</code></pre> <ol> <li>Security Considerations 3.1 Token-Based Authentication</li> </ol> <p>JWT tokens are used to authenticate users. The token is generated when a user logs in, and it is stored in the browser cookies. The token is sent with each request to validate the user\u2019s identity.</p> <p>JWT Generation: When a user successfully logs in, the system generates a JWT token, which includes the user\u2019s email as a payload. This token expires after a certain time, and the user must log in again to get a new token.</p> <p>Token Validation: Each time the user makes a request, the backend validates the token and retrieves the user\u2019s details from the database.</p> <p>3.2 Password Security</p> <p>Passwords are hashed using bcrypt before being stored in the database, ensuring that plaintext passwords are never stored or exposed. bcrypt is a strong and widely-used hashing algorithm that incorporates salting and multiple rounds of hashing to make brute-force attacks difficult.</p> <p>Conclusion</p> <p>TheDrive implements a secure login system using JWT tokens for authentication and bcrypt for password hashing. The system stores user credentials and file metadata securely in the PostgreSQL database, with relationships between users and their files managed using SQLAlchemy ORM. The FileSystemItem model tracks file metadata and AI processing status, allowing the platform to offer intelligent file management and analysis features in the future.</p> <p>This approach ensures that user data remains secure and private, while also providing a flexible foundation for adding advanced features such as AI-driven file analysis and semantic search.</p>"},{"location":"architecture/security/#explanation","title":"Explanation:","text":"<ol> <li>Password Hashing: Uses bcrypt for secure password hashing, making it hard for attackers to retrieve user passwords even if the database is compromised.</li> <li>JWT Tokens: Tokens are used to manage user sessions. The token includes expiration and is validated for each request, ensuring that only authenticated users can access their data.</li> <li>Database Schema: The User model stores user credentials, and the FileSystemItem model tracks file metadata, including the AI processing status and relationships.</li> <li>Security: JWT-based authentication and bcrypt ensure secure password handling, while the database ensures integrity and safe storage of sensitive user data.</li> </ol> <p>This Markdown file provides a detailed overview of the security features, focusing on user authentication and data protection within TheDrive.</p>"},{"location":"architecture/system-design/","title":"TheDrive - System Design Documentation","text":"<p>This document provides a comprehensive overview of the system design for TheDrive, focusing on its architecture, key components, data flow, technologies used, and security considerations. TheDrive is an AI-powered cross-platform file storage system designed to help users securely store, manage, and interact with their digital assets, leveraging cutting-edge AI technologies and a robust architecture.</p>"},{"location":"architecture/system-design/#1-introduction","title":"1. Introduction","text":"<p>TheDrive is designed to be a next-generation cloud storage platform that integrates AI-powered features with secure file storage. The system provides intelligent ways to interact with files, analyze data, and derive insights, all while ensuring top-tier security and privacy. TheDrive combines cloud storage, AI models, and secure databases to deliver a seamless user experience.</p>"},{"location":"architecture/system-design/#key-features","title":"Key Features:","text":"<ul> <li>Multi-format file support (documents, spreadsheets, images, etc.)</li> <li>AI-powered insights for querying, understanding, and managing data.</li> <li>End-to-end encryption (E2EE) for data privacy and security.</li> <li>Cloud storage integration with AWS S3 for file storage.</li> <li>Graph-based AI pipeline for processing relationships and complex data interactions.</li> </ul>"},{"location":"architecture/system-design/#2-architecture-overview","title":"2. Architecture Overview","text":""},{"location":"architecture/system-design/#high-level-architecture","title":"High-Level Architecture","text":"<p>TheDrive follows a microservices-based architecture with clearly defined components for handling different tasks, ensuring scalability and maintainability.</p> <ul> <li>Frontend (Next.js): Provides a dynamic, responsive web interface for users to interact with their files and perform actions (uploads, queries, AI-powered features).</li> <li>Backend (FastAPI): The core API layer responsible for file management, metadata storage, AI model interactions, user authentication, and more.</li> <li>AI Layer: Handles all AI-driven tasks, such as text analysis, image recognition, and semantic search. This layer uses a graph-based AI pipeline to manage relationships between files and metadata.</li> <li>Storage Layer:</li> <li>AWS S3: Used for secure and scalable file storage.</li> <li>PostgreSQL: Stores user credentials, file metadata, and other structured data.</li> <li>Neo4j: Used for graph-based AI processing and managing complex relationships between files.</li> </ul>"},{"location":"architecture/system-design/#diagram-of-high-level-architecture","title":"Diagram of High-Level Architecture","text":"<p>image</p>"},{"location":"architecture/system-design/#3-key-components-and-technologies","title":"3. Key Components and Technologies","text":""},{"location":"architecture/system-design/#31-frontend-nextjs","title":"3.1 Frontend (Next.js)","text":"<ul> <li>Next.js powers the frontend, providing server-side rendering (SSR) and static site generation (SSG) for fast page loading and SEO benefits.</li> <li>React is used for building reusable components and managing the state.</li> <li>The frontend interacts with the FastAPI backend through RESTful APIs using Axios.</li> <li>File upload functionality is handled through HTML5 and custom components for managing file previews.</li> </ul> <p>Technologies: - Next.js: For dynamic web applications with React. - Tailwind CSS: For responsive, utility-first CSS design. - Axios: For API communication between the frontend and backend.</p>"},{"location":"architecture/system-design/#32-backend-fastapi","title":"3.2 Backend (FastAPI)","text":"<p>The backend is powered by FastAPI, providing high-performance API handling. It serves multiple roles, including user authentication, file upload handling, and interaction with the AI pipeline.</p> <ul> <li>File Management: When a file is uploaded, FastAPI handles the storage in AWS S3 and the insertion of metadata in PostgreSQL.</li> <li>AI Integration: FastAPI manages the communication between the AI layer and the frontend.</li> <li>Authentication: Users are authenticated using JWT tokens to ensure secure access.</li> </ul> <p>Technologies: - FastAPI: Fast and modern Python web framework for API handling. - Python-Jose: For JWT authentication and token management. - SQLAlchemy: ORM for interacting with the PostgreSQL database. - boto3: AWS SDK for managing file storage in S3.</p>"},{"location":"architecture/system-design/#33-file-storage-with-aws-s3","title":"3.3 File Storage with AWS S3","text":"<p>AWS S3 is used to store files securely. It offers scalable storage and integrates well with FastAPI to handle file uploads and downloads.</p> <ul> <li>File Upload: When a user uploads a file, it is stored in an S3 bucket. The file metadata (file name, size, type, and user ID) is stored in the PostgreSQL database for easy reference.</li> <li>File Retrieval: Files are retrieved from S3 whenever needed, with access control managed via AWS IAM.</li> </ul> <p>Technologies: - AWS S3: Scalable object storage. - boto3: Python SDK for AWS services.</p>"},{"location":"architecture/system-design/#34-postgresql-for-credentials-and-metadata","title":"3.4 PostgreSQL for Credentials and Metadata","text":"<p>PostgreSQL is used to store user credentials, file metadata, and other relational data. It ensures the integrity of the data and allows for fast lookups and queries.</p> <ul> <li>User Management: Stores user credentials (hashed passwords) and user metadata.</li> <li>File Metadata: Stores details about files such as file names, types, sizes, and storage paths (links to S3).</li> <li>Data Relationships: Although PostgreSQL is a relational database, it is used here to store basic metadata and user data in structured tables.</li> </ul> <p>Technologies: - PostgreSQL: Relational database management system for secure, structured data storage.</p>"},{"location":"architecture/system-design/#35-graph-based-ai-pipeline-neo4j","title":"3.5 Graph-Based AI Pipeline (Neo4j)","text":"<p>The AI layer in TheDrive uses Neo4j, a graph database, to manage and analyze the relationships between different files and metadata. This is crucial for implementing the AI-powered features such as semantic search and file cross-referencing.</p> <ul> <li>Graph Representation: The AI pipeline treats files, metadata, and other entities as nodes in a graph, with relationships (edges) between them. For example, files that share tags or concepts are connected in the graph.</li> <li>AI Features: Graph-based models allow for advanced queries such as \"find all files related to this topic\" or \"retrieve files linked by similar keywords.\" It also powers semantic search, allowing the system to understand the context of search queries.</li> <li>AI Processing: The AI pipeline uses machine learning models to analyze textual content (e.g., for summarization) and images (via OCR and computer vision models).</li> </ul> <p>Technologies: - Neo4j: Graph database used to manage file relationships and metadata. - Graph Algorithms: Implementing algorithms such as community detection and graph traversal to analyze relationships between files.</p>"},{"location":"architecture/system-design/#36-ai-layer","title":"3.6 AI Layer","text":"<p>The AI layer performs several tasks, including:</p> <ul> <li>Text Analysis: Utilizes Natural Language Processing (NLP) models to extract meaning from documents, such as summarization, keyword extraction, and contextual analysis.</li> <li>Image Processing: Uses OCR to extract text from images and applies image recognition models to understand visual data (e.g., charts, graphs, diagrams).</li> <li>Semantic Search: Enhances search by understanding the meaning behind search queries, not just matching keywords.</li> </ul> <p>Technologies: - spaCy / NLTK: For text processing and NLP tasks. - PyTorch / TensorFlow: For training and deploying machine learning models. - OpenCV: For basic image processing and recognition.</p>"},{"location":"architecture/system-design/#4-data-flow","title":"4. Data Flow","text":""},{"location":"architecture/system-design/#file-upload-and-processing","title":"File Upload and Processing","text":"<ol> <li>User Uploads File:</li> <li>The user selects a file to upload via the frontend.</li> <li> <p>The frontend sends the file to the FastAPI backend, where it is uploaded to AWS S3 for storage.</p> </li> <li> <p>Metadata Storage:</p> </li> <li> <p>Metadata (file name, size, type, etc.) is stored in PostgreSQL for easy retrieval and indexing.</p> </li> <li> <p>AI Processing:</p> </li> <li> <p>The file content is passed through the AI pipeline:</p> <ul> <li>Text files undergo NLP analysis (e.g., summarization).</li> <li>Images are processed using OCR and image recognition.</li> </ul> </li> <li> <p>Graph-Based Analysis:</p> </li> <li>Metadata and file relationships are stored in Neo4j for graph-based analysis, enabling semantic search and advanced file relationships.</li> </ol>"},{"location":"architecture/system-design/#query-handling","title":"Query Handling","text":"<ol> <li>User Queries Files:</li> <li>The user enters a query (e.g., \"Show all files related to 'financial report'\").</li> <li>The frontend sends the query to the FastAPI backend.</li> <li> <p>The backend processes the query, utilizing semantic search powered by the AI layer and Neo4j graph database to find relevant files.</p> </li> <li> <p>AI Insights:</p> </li> <li>The AI models analyze the content and relationships between files to return meaningful results, which are then sent back to the frontend.</li> </ol>"},{"location":"architecture/system-design/#5-security-considerations","title":"5. Security Considerations","text":""},{"location":"architecture/system-design/#data-encryption","title":"Data Encryption","text":"<ul> <li>End-to-End Encryption (E2EE) ensures that all data is encrypted both at rest (in storage) and in transit (during upload/download).</li> <li>Files stored in AWS S3 are encrypted using AES-256 encryption.</li> </ul>"},{"location":"architecture/system-design/#authentication-and-authorization","title":"Authentication and Authorization","text":"<ul> <li>JWT Authentication is used to securely authenticate users.</li> <li>Role-Based Access Control (RBAC) ensures that users only have access to files and features they are authorized to interact with.</li> </ul>"},{"location":"architecture/system-design/#data-isolation","title":"Data Isolation","text":"<ul> <li>Data segregation ensures that each user's files are stored in isolated environments, preventing unauthorized access between users.</li> <li>AWS IAM roles are used to manage access to cloud services.</li> </ul>"},{"location":"architecture/system-design/#6-conclusion","title":"6. Conclusion","text":"<p>TheDrive combines secure cloud storage, AI-powered file analysis, and a graph-based AI pipeline to provide a smarter, more efficient way for users to interact with their digital assets. With the use of AWS S3 for scalable storage, PostgreSQL for metadata management, and Neo4j for complex data relationships, TheDrive is built to scale and provide advanced features for personal and enterprise-level use.</p> <p>By integrating these technologies, TheDrive offers a secure, scalable, and intelligent cloud storage solution designed for the future of data interaction.</p>"},{"location":"architecture/trade-off/","title":"Technology Choices and Trade-offs","text":"<p>This document explains the technology choices made for TheDrive, detailing why certain technologies were chosen for the backend, storage, and AI pipeline, as well as the trade-offs compared to other alternatives. TheDrive ensures that all selected technologies comply with free-tier services as required by the project guidelines.</p>"},{"location":"architecture/trade-off/#1-cloud-storage-aws-s3-free-tier","title":"1. Cloud Storage: AWS S3 (Free Tier)","text":""},{"location":"architecture/trade-off/#why-aws-s3","title":"Why AWS S3?","text":"<p>AWS S3 was chosen as the cloud storage solution for TheDrive because it meets the project\u2019s free-tier requirements while providing reliable, scalable, and secure file storage. Here's why:</p> <ul> <li>Free Tier: AWS offers a free-tier for S3, which provides up to 5GB of standard storage, 20,000 GET requests, and 2,000 PUT requests per month at no charge. This is sufficient for development, testing, and small-scale production environments.</li> <li>Scalability: AWS S3 is designed to handle large-scale storage needs, so as the project grows, you can easily transition to a paid tier if more storage or higher request limits are needed.</li> <li>Security: AWS S3 provides encryption at rest and in transit, IAM roles for access control, and data durability (99.999999999% durability).</li> <li>Integration with Other AWS Services: S3 integrates seamlessly with other AWS services (like Lambda, EC2, and Rekognition), which can help scale the project if needed.</li> </ul>"},{"location":"architecture/trade-off/#trade-offs","title":"Trade-offs:","text":"<ul> <li>Vendor Lock-in: By using AWS S3, TheDrive is tied to the AWS ecosystem, and migrating to another cloud provider would incur effort and potential costs.</li> <li>Cost Scaling: The free-tier provides limited storage and requests. As the usage grows, costs will scale. However, the transition to paid tiers is gradual and can be optimized.</li> </ul>"},{"location":"architecture/trade-off/#alternative-choices","title":"Alternative Choices:","text":"<ul> <li>Google Cloud Storage / Azure Blob Storage: These cloud storage options also offer free-tier services with similar features. However, AWS was chosen because of its wide adoption, integration with other AWS services, and familiarity with the platform for most developers.</li> <li>Self-hosted Solutions (e.g., MinIO): Although self-hosted object storage like MinIO offers control over the environment and eliminates vendor lock-in, it adds operational complexity (e.g., maintaining the infrastructure) and does not offer the same level of scalability and integration as S3.</li> </ul>"},{"location":"architecture/trade-off/#2-database-postgresql-free-tier","title":"2. Database: PostgreSQL (Free Tier)","text":""},{"location":"architecture/trade-off/#why-postgresql","title":"Why PostgreSQL?","text":"<p>PostgreSQL was chosen as the relational database for managing user credentials, file metadata, and other structured data, as it meets the project\u2019s free-tier requirements.</p> <ul> <li>Free Tier: Cloud platforms like Heroku and ElephantSQL offer free-tier PostgreSQL databases with limited storage (up to 1GB or more) suitable for development and small-scale production environments.</li> <li>ACID Compliance: PostgreSQL is a reliable, ACID-compliant relational database, ensuring data integrity and consistency. This is crucial when handling structured data like credentials and file metadata.</li> <li>Flexibility: PostgreSQL supports JSON and JSONB, allowing it to handle both structured and semi-structured data. This gives flexibility when storing file metadata or data that does not fit neatly into a table schema.</li> </ul>"},{"location":"architecture/trade-off/#trade-offs_1","title":"Trade-offs:","text":"<ul> <li>Not as Fast as NoSQL for Unstructured Data: PostgreSQL is ideal for structured data, but for large volumes of unstructured data (e.g., documents or non-relational datasets), NoSQL databases like MongoDB might perform better.</li> <li>Scaling for High Traffic: While PostgreSQL can handle large datasets, high traffic and complex queries might require tuning and optimization. The free-tier databases may also have resource limitations (e.g., slower performance with large datasets).</li> </ul>"},{"location":"architecture/trade-off/#alternative-choices_1","title":"Alternative Choices:","text":"<ul> <li>MongoDB (NoSQL): MongoDB is a good alternative if the data model was document-based and required greater flexibility in handling unstructured data. However, PostgreSQL was preferred due to its data integrity and support for both structured and semi-structured data.</li> <li>MySQL: MySQL is another relational database option but PostgreSQL was chosen due to its richer feature set and advanced querying capabilities.</li> </ul>"},{"location":"architecture/trade-off/#3-graph-database-neo4j-free-tier","title":"3. Graph Database: Neo4j (Free Tier)","text":""},{"location":"architecture/trade-off/#why-neo4j","title":"Why Neo4j?","text":"<p>Neo4j is used as the graph database to handle relationships between files, metadata, and AI-driven features. Neo4j was chosen because it is an industry-leading graph database with a free-tier offering.</p> <ul> <li>Free Tier: Neo4j offers a free-tier cloud database that allows up to 200,000 nodes and 400,000 relationships. This free-tier is sufficient for small-scale graph database applications, ideal for development and early-stage production use.</li> <li>Efficient Relationship Mapping: Neo4j is optimized for handling complex relationships between data points, which is ideal for features like semantic search, file cross-referencing, and content analysis in TheDrive.</li> <li>Graph-Based AI Pipeline: TheDrive uses Neo4j to represent AI-powered relationships between files and their content, making it easy to query and analyze data relationships.</li> </ul>"},{"location":"architecture/trade-off/#trade-offs_2","title":"Trade-offs:","text":"<ul> <li>Learning Curve: Neo4j\u2019s graph database model and its Cypher query language require a learning curve, especially for teams that are more familiar with relational databases.</li> <li>Performance: For large-scale systems with complex graph queries, Neo4j might experience performance bottlenecks. Proper indexing and optimization techniques are required to ensure scalability.</li> </ul>"},{"location":"architecture/trade-off/#alternative-choices_2","title":"Alternative Choices:","text":"<ul> <li>Amazon Neptune: AWS offers Neptune, a fully managed graph database service, which could be used instead of Neo4j. However, Neptune does not offer a free-tier service, making it less suitable for the project\u2019s budget constraints.</li> <li>ArangoDB: A multi-model database that combines document, key-value, and graph databases. Although this could be a good alternative, Neo4j was preferred for its specialized focus on graph-based data and its robust community support.</li> </ul>"},{"location":"architecture/trade-off/#conclusion","title":"Conclusion","text":"<p>TheDrive uses a combination of AWS S3, PostgreSQL, and Neo4j to provide a robust, scalable, and secure system while adhering to the free-tier service requirement of the project. These technologies were selected based on their ability to meet the project\u2019s goals of:</p> <ul> <li>Scalability: Each technology can handle the growing data and operational needs of TheDrive.</li> <li>Security: Each solution provides strong security mechanisms, including encryption and access control.</li> <li>AI Integration: Neo4j\u2019s graph-based capabilities enable complex AI features like semantic search and cross-referencing.</li> </ul> <p>While there are trade-offs, such as potential vendor lock-in with AWS S3 and performance considerations with Neo4j, these technologies offer the best combination of features and flexibility within the free-tier constraints of the project.</p>"},{"location":"deploy/populate/","title":"Populate","text":""},{"location":"deploy/populate/#populating-the-scripts","title":"Populating the Scripts","text":"<p>The Script to populate user data in the database is given in /backend/scripts/populate_users.py to run this just run the script</p> <p>Script to populate users and their data from a folder structure.</p> <p>Expected folder structure: /users_folder/ \u251c\u2500\u2500 user1/ \u2502   \u251c\u2500\u2500 info.json \u2502   \u2514\u2500\u2500 data/ \u2502       \u251c\u2500\u2500 file1.txt \u2502       \u2514\u2500\u2500 file2.pdf \u251c\u2500\u2500 user2/ \u2502   \u251c\u2500\u2500 info.json \u2502   \u2514\u2500\u2500 data/ \u2502       \u2514\u2500\u2500 document.docx \u2514\u2500\u2500 ...</p> <p>info.json format: {   \"name\": \"John Doe\",   \"password\": \"P@ssw0rd123\",   \"email\": \"johndoe@example.com\" } \"\"\"</p>"},{"location":"pipeline/ingestion/","title":"How RAG Works for Multiple File Formats","text":""},{"location":"pipeline/ingestion/#universal-file-ingestion-pipeline","title":"Universal File Ingestion Pipeline","text":"<p>code snippet: Universal File Processing</p> <pre><code>def ingest_file(path: str, file_id: str, folder_id: Optional[str] = None, file_type: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Universal file ingestion supporting PDF, TXT, DOCX, CSV.\n    Automatically detects file type and applies appropriate processing.\n    \"\"\"\n\n    # Detect file type if not provided\n    if file_type is None:\n        file_type = detect_file_type(path)\n\n    print(f\"[Ingest] Processing {file_type.upper()} file: {path}\")\n\n    # Load documents based on file type\n    pdf_image_count = 0\n    if file_type == 'pdf':\n        # Check if PDF contains significant images\n        if has_significant_images(path):\n            print(f\"[Ingest] PDF contains images - using hybrid processing\")\n            text_docs, pdf_image_count = process_pdf_with_images(path, file_id, folder_id)\n            raw_docs = text_docs\n\n            if pdf_image_count &gt; 0:\n                print(f\"[Ingest] Processed {pdf_image_count} images from PDF\")\n        else:\n            print(f\"[Ingest] PDF has no significant images - using standard text processing\")\n            raw_docs = load_pdf(path)\n    elif file_type == 'text':\n        raw_docs = load_text(path)\n    elif file_type == 'docx':\n        raw_docs = load_docx(path)\n    elif file_type == 'csv':\n        raw_docs = load_csv(path)\n\n    elif file_type in [\"jpeg\", \"jpg\", \"png\", \"gif\"]:\n        print(f\"[RAG] Ingesting image file: {path}\")\n\n        with open(path, \"rb\") as f:\n            image_bytes = f.read()\n\n        # Use vision model for image analysis\n        llm_vision = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=settings.GEMINI_API_KEY)\n        summary = get_image_summary(image_bytes, llm_vision)\n        print(f\"[RAG] Generated image summary: {summary[:100]}...\")\n\n        # Create a Document with the summary and metadata\n        doc = Document(\n            page_content=summary,\n            metadata={\n                \"file_id\": file_id,\n                \"folder_id\": folder_id,\n                \"source\": os.path.basename(path),\n                \"type\": \"image\",\n                \"original_path\": path\n            }\n        )\n\n        # Add to the vector store\n        vs = get_vs()\n        vs.add_documents([doc])\n\n        return 1\n    else:\n        print(f\"[Ingest] Unsupported file type: {file_type}\")\n        return 0\n\n    if file_type not in [\"jpeg\", \"jpg\", \"png\", \"gif\"]:\n        if not raw_docs:\n            print(f\"[Ingest] No content extracted from {path}\")\n            return 0\n\n        print(f\"[Ingest] Extracted {len(raw_docs)} raw documents from {file_type.upper()}\")\n\n        # Chunk documents\n        chunks = chunk_docs(raw_docs, file_type)\n        print(f\"[Ingest] Created {len(chunks)} chunks\")\n</code></pre>"},{"location":"pipeline/ingestion/#file-type-specific-processing","title":"File Type Specific Processing","text":"<ol> <li>Ingestion: Files are loaded and parsed according to their type (PDF, DOCX, CSV, images, etc.).</li> <li>Chunking: Content is split into manageable, context-rich chunks.</li> <li>Embedding: Chunks are embedded using LLM-powered models for semantic search.</li> <li>Graph Construction: Entities and relationships are extracted and stored in Neo4j, enabling graph-based queries and cross-file knowledge.</li> <li>Retrieval: When a user asks a question, relevant chunks are retrieved using semantic and graph-based search, and answers are generated with citations and context.</li> <li>Multimodal Analysis:</li> <li>Image Preprocessing: Uploaded images are enhanced (grayscale, contrast, sharpening, morphological operations) to improve OCR and visual analysis accuracy.</li> <li>OCR (Text Extraction): Both Tesseract and EasyOCR are used to extract printed and handwritten text, with confidence filtering and bounding box localization. Results are deduplicated and combined for comprehensive coverage.</li> <li>Diagram Structure Analysis: Diagrams are analyzed for geometric shapes (triangles, rectangles, circles, polygons) and lines/connections using edge detection and contour analysis. This enables understanding of visual relationships and flow.</li> <li>Chart and Graph Analysis: Chart images are sent to a vision-capable LLM, which extracts chart type, axis labels, data points, trends, and key insights, making visual data explorable via chat.</li> <li>Entity Detection: Images are analyzed by a vision LLM to identify entities (people, objects, text, structures, vehicles, animals, natural elements) with descriptions, locations, and attributes.</li> <li>Comprehensive Image Analysis: All above methods are combined to produce a rich, structured summary of image content, including text, shapes, entities, and chart insights. Results are stored as searchable documents with metadata for downstream retrieval and chat-based exploration.</li> </ol> <p>code snippet: CSV Processing</p> <pre><code>def load_csv(path: str) -&gt; List[Document]:\n    \"\"\"Load CSV files with intelligent processing.\"\"\"\n    try:\n        # Try multiple encoding strategies\n        csv_params = [\n            {'encoding': 'utf-8'},\n            {'encoding': 'latin-1'},\n            {'encoding': 'cp1252'},\n            {'encoding': 'utf-8', 'sep': ';'},\n            {'encoding': 'latin-1', 'sep': ';'},\n        ]\n\n        df = None\n        for params in csv_params:\n            try:\n                df = pd.read_csv(path, **params)\n                break\n            except:\n                continue\n\n        if df is None:\n            print(f\"[Ingest] Could not read CSV file {path}\")\n            return []\n\n        docs = []\n\n        # Strategy 1: Create overview document\n        csv_text = f\"CSV File: {os.path.basename(path)}\\n\"\n        csv_text += f\"Columns: {', '.join(df.columns.tolist())}\\n\"\n        csv_text += f\"Rows: {len(df)}\\n\\n\"\n\n        # Add column descriptions\n        for col in df.columns:\n            col_info = f\"Column '{col}':\\n\"\n            if df[col].dtype == 'object':\n                unique_vals = df[col].dropna().unique()[:10]\n                col_info += f\"  Sample values: {', '.join(map(str, unique_vals))}\\n\"\n            elif df[col].dtype in ['int64', 'float64']:\n                col_info += f\"  Range: {df[col].min()} to {df[col].max()}\\n\"\n                col_info += f\"  Mean: {df[col].mean():.2f}\\n\"\n\n            csv_text += col_info + \"\\n\"\n\n        # Add sample rows\n        csv_text += \"Sample rows:\\n\"\n        csv_text += df.head(5).to_string(index=False)\n\n        docs.append(Document(\n            page_content=csv_text,\n            metadata={\n                'source': os.path.basename(path),\n                'file_type': 'csv',\n                'loader': 'PandasCSV',\n                'columns': df.columns.tolist(),\n                'rows': len(df)\n            }\n        ))\n\n        # Strategy 2: Create row-based documents for smaller CSVs\n        if len(df) &lt;= 1000:\n            for idx, row in df.iterrows():\n                row_text = f\"Row {idx + 1} from {os.path.basename(path)}:\\n\"\n                for col, val in row.items():\n                    if pd.notna(val):\n                        row_text += f\"{col}: {val}\\n\"\n\n                docs.append(Document(\n                    page_content=row_text,\n                    metadata={\n                        'source': os.path.basename(path),\n                        'file_type': 'csv',\n                        'row_number': idx + 1,\n                        'loader': 'PandasCSV'\n                    }\n                ))\n\n        return docs\n\n    except Exception as e:\n        print(f\"[Ingest] Error loading CSV {path}: {e}\")\n        return []\n</code></pre>"},{"location":"pipeline/processing/","title":"Constructing Knowledge Graphs: Major Functions","text":""},{"location":"pipeline/processing/#entity-extraction-with-llm-prompts","title":"Entity Extraction with LLM Prompts","text":"<p>code snippet: Entity Extraction Prompt</p> <pre><code>ENTITY_EXTRACTION_PROMPT = \"\"\"\nYou are a highly-specialized entity extraction model. Your task is to extract all salient entities from the provided text with confidence scores.\n\n**Instructions:**\n1. **Strict Adherence:** Analyze the entire text and extract every important entity.\n2. **Canonical Form:** Present the entity name in its most standard form (e.g., \"AI\" \u2192 \"Artificial Intelligence\").\n3. **Type Specificity:** Assign entity types: PERSON, ORGANIZATION, LOCATION, PRODUCT, TECHNOLOGY, EVENT, CONCEPT, TIME, NUMBER, MISCELLANEOUS.\n4. **Attribute Richness:** Identify all relevant attributes and properties as a structured JSON object.\n5. **Confidence Score:** Provide confidence (0.0-1.0) based on:\n   - How clearly mentioned (0.9-1.0 for explicit mentions)\n   - Relevance to main topic (0.7-0.9 for highly relevant)\n   - Supporting context (0.5-0.7 for implied/context-dependent)\n\nReturn ONLY a JSON array with 'name', 'type', 'attributes', and 'confidence' fields.\nText: {text}\n\"\"\"\n\ndef extract_entities(text: str) -&gt; List[Dict[str, str]]:\n    \"\"\"Extract entities with types, attributes, and confidence scores using LLM.\"\"\"\n    try:\n        llm_instance = get_llm()\n        prompt = ENTITY_EXTRACTION_PROMPT.format(text=text)\n        response = llm_instance.invoke(prompt).content.strip()\n\n        entities = _safe_extract_json(response, list)\n        if not entities:\n            return []\n\n        # Validate entities against text to reduce hallucinations\n        validated_entities = []\n        for entity in entities:\n            if _validate_entity_in_text(entity.get('name', ''), text):\n                validated_entities.append(entity)\n\n        # Deduplicate similar entities\n        return _deduplicate_entities(validated_entities)\n\n    except Exception as e:\n        print(f\"[GraphRAG] Entity extraction failed: {e}\")\n        return []\n</code></pre>"},{"location":"pipeline/processing/#confidence-based-graph-operations","title":"Confidence-Based Graph Operations","text":"<ol> <li>Neo4jGraphRAG Class</li> <li>Manages Neo4j connection, creates constraints and indexes, and provides graph operations.</li> <li>Ensures unique entity names and efficient graph queries via indexes.</li> </ol> <p>code snippet: Graph Database Initialization</p> <pre><code>class Neo4jGraphRAG:\n    def __init__(self):\n        self.driver = None\n        self._connect()\n\n    def _connect(self):\n        \"\"\"Initialize Neo4j connection.\"\"\"\n        try:\n            self.driver = GraphDatabase.driver(NEO4J_URL, auth=(NEO4J_USER, NEO4J_PASSWORD))\n            # Test connection\n            with self.driver.session() as session:\n                session.run(\"RETURN 1\")\n            print(f\"[GraphRAG] Connected to Neo4j at {NEO4J_URL}\")\n            self._create_constraints()\n        except Exception as e:\n            print(f\"[GraphRAG] Failed to connect to Neo4j: {e}\")\n            self.driver = None\n\n    def _create_constraints(self):\n        \"\"\"Create necessary constraints and indexes.\"\"\"\n        with self.driver.session() as session:\n            try:\n                session.run(\"CREATE CONSTRAINT entity_name IF NOT EXISTS FOR (e:Entity) REQUIRE e.name IS UNIQUE\")\n                session.run(\"CREATE INDEX entity_type_idx IF NOT EXISTS FOR (e:Entity) ON (e.type)\")\n                session.run(\"CREATE INDEX document_file_idx IF NOT EXISTS FOR (d:Document) ON (d.file_id)\")\n                session.run(\"CREATE INDEX chunk_file_idx IF NOT EXISTS FOR (c:Chunk) ON (c.file_id)\")\n                print(\"[GraphRAG] Neo4j constraints and indexes created\")\n            except Exception as e:\n                print(f\"[GraphRAG] Warning: Could not create constraints/indexes: {e}\")\n\n    def close(self):\n        \"\"\"Close Neo4j connection.\"\"\"\n        if self.driver:\n            self.driver.close()\n\n    def is_connected(self):\n        \"\"\"Check if Neo4j is connected.\"\"\"\n        return self.driver is not None\n</code></pre> <ol> <li>Entity Extraction &amp; Validation</li> <li><code>extract_entities(text)</code>: Uses LLM to extract canonical entities, types, attributes, and confidence scores from text.</li> <li><code>_validate_entity_in_text(entity_name, text)</code>: Ensures entities are present in the text to reduce hallucinations.</li> <li><code>_deduplicate_entities(entities)</code>: Merges similar entities and attributes to reduce noise.</li> <li><code>_calculate_similarity(str1, str2)</code>: Measures similarity for deduplication.</li> <li>Relation Extraction</li> <li><code>extract_relations(text, entities)</code>: Uses LLM to extract relationships (subject, predicate, object, context, confidence) between entities in text.</li> <li>Validates that both subject and object exist in the extracted entities or text.</li> <li>Graph Upsert and Linking</li> <li><code>upsert_entities_and_relations(file_id, page, chunk_no, source, text)</code>: Extracts entities/relations from a chunk and stores them in Neo4j, linking documents, chunks, entities, and relationships with confidence scores.</li> <li>Links document and chunk nodes, and connects entities and relations to their source chunks.</li> <li>Supports dynamic graph updates as new files are ingested and existing files are modified.</li> <li>Query Entity Extraction</li> <li><code>extract_query_entities(query)</code>: Uses LLM to extract key entities from a user query for targeted graph lookup.</li> <li>Subgraph Fact Retrieval</li> <li><code>get_subgraph_facts(entities, file_id, folder_id, ...)</code>: Retrieves high-confidence facts and relationships from the graph relevant to the query entities, supporting multi-hop and fuzzy matching.</li> <li>Uses exact, type-based, and partial/fuzzy matching strategies.</li> </ol> <p>code snippet: Intelligent Graph Querying</p> <pre><code>def get_subgraph_facts(\n    entities: List[str],\n    file_id: Optional[str] = None,\n    folder_id: Optional[str] = None,\n    max_facts: int = MAX_FACTS,\n    min_confidence: float = 0.6\n) -&gt; str:\n    \"\"\"\n    Build a textual block of 'facts' from the Neo4j graph, filtered by file_id and confidence.\n    Uses intelligent matching: exact names, then types, then partial matches.\n    Prioritizes high-confidence entities and relations to reduce noise.\n    \"\"\"\n    if not entities or not get_graph_db().is_connected():\n        return \"\"\n\n    try:\n        with get_graph_db().driver.session() as session:\n            facts = []\n            processed_entities = set()\n\n            for query_entity in entities:\n                query_entity_lower = query_entity.lower()\n\n                # Strategy 1: Exact name match (case-insensitive) with confidence filtering\n                entity_result = session.run(\"\"\"\n                    MATCH (e:Entity)\n                    WHERE toLower(e.name) = $entity_name\n                    AND (e.confidence IS NULL OR e.confidence &gt;= $min_confidence)\n                    OPTIONAL MATCH (c:Chunk)-[:MENTIONS]-&gt;(e)\n                    WHERE ($file_id IS NULL OR c.file_id = $file_id)\n                    RETURN e.name as name, e.type as type, e.attributes as attributes,\n                           e.confidence as confidence,\n                           collect(DISTINCT c.file_id) as mentioned_in_files\n                    ORDER BY e.confidence DESC\n                    LIMIT 5\n                \"\"\", entity_name=query_entity_lower, file_id=file_id, min_confidence=min_confidence)\n\n                found_entities = list(entity_result)\n\n                # Strategy 2: If no exact match, try type-based matching\n                if not found_entities:\n                    # Map common query terms to types\n                    type_mapping = {\n                        'sets': 'SET', 'set': 'SET',\n                        'functions': 'FUNCTION', 'function': 'FUNCTION',\n                        'concepts': 'CONCEPT', 'concept': 'CONCEPT',\n                        'locations': 'LOCATION', 'location': 'LOCATION'\n                    }\n\n                    target_type = type_mapping.get(query_entity_lower)\n                    if target_type:\n                        entity_result = session.run(\"\"\"\n                            MATCH (e:Entity)\n                            WHERE e.type = $entity_type\n                            AND (e.confidence IS NULL OR e.confidence &gt;= $min_confidence)\n                            OPTIONAL MATCH (c:Chunk)-[:MENTIONS]-&gt;(e)\n                            WHERE ($file_id IS NULL OR c.file_id = $file_id)\n                            RETURN e.name as name, e.type as type, e.attributes as attributes,\n                                   e.confidence as confidence,\n                                   collect(DISTINCT c.file_id) as mentioned_in_files\n                            ORDER BY e.confidence DESC\n                            LIMIT 5\n                        \"\"\", entity_type=target_type, file_id=file_id, min_confidence=min_confidence)\n\n                        found_entities = list(entity_result)\n</code></pre> <ol> <li>Graph Statistics and Analytics</li> <li><code>get_graph_stats()</code>: Returns counts and confidence metrics for entities, relationships, documents, and chunks in the graph.</li> <li>Tracks entity types, average confidence, and high-confidence counts.</li> <li>Supports document and chunk statistics for analytics.</li> <li>Graph Cleanup and Maintenance</li> <li><code>cleanup_low_confidence_entities(confidence_threshold)</code>: Removes entities and relations below a confidence threshold to keep the graph high-quality.</li> <li><code>update_confidence_thresholds(entity_threshold, relation_threshold)</code>: Updates global thresholds for entity/relation filtering.</li> <li><code>clear_graph()</code>: Deletes all nodes and relationships from the Neo4j graph.</li> </ol> <p>code snippet: Confidence-Based Cleanup</p> <pre><code>def cleanup_low_confidence_entities(confidence_threshold: float = None) -&gt; Dict:\n    \"\"\"\n    Remove entities and relations below the confidence threshold to clean up the graph.\n    \"\"\"\n    if not get_graph_db().is_connected():\n        return {\"error\": \"GraphRAG not connected\"}\n\n    if confidence_threshold is None:\n        confidence_threshold = ENTITY_CONFIDENCE_THRESHOLD\n\n    try:\n        with get_graph_db().driver.session() as session:\n            # Delete low-confidence entities\n            entity_result = session.run(\"\"\"\n                MATCH (e:Entity)\n                WHERE e.confidence &lt; $threshold\n                DETACH DELETE e\n                RETURN count(e) as deleted_entities\n            \"\"\", threshold=confidence_threshold)\n\n            # Delete low-confidence relations\n            relation_result = session.run(\"\"\"\n                MATCH ()-[r:RELATED_TO]-()\n                WHERE r.confidence &lt; $threshold\n                DELETE r\n                RETURN count(r) as deleted_relations\n            \"\"\", threshold=confidence_threshold)\n\n            deleted_entities = entity_result.single()[\"deleted_entities\"]\n            deleted_relations = relation_result.single()[\"deleted_relations\"]\n\n            return {\n                \"success\": True,\n                \"deleted_entities\": deleted_entities,\n                \"deleted_relations\": deleted_relations,\n                \"threshold_used\": confidence_threshold\n            }\n\n    except Exception as e:\n        return {\"error\": f\"Cleanup failed: {e}\"}\n\ndef get_graph_stats() -&gt; Dict:\n    \"\"\"Get comprehensive statistics about the knowledge graph.\"\"\"\n    if not get_graph_db().is_connected():\n        return {\"error\": \"GraphRAG not connected\"}\n\n    try:\n        with get_graph_db().driver.session() as session:\n            # Entity statistics\n            entity_stats = session.run(\"\"\"\n                MATCH (e:Entity)\n                RETURN\n                    count(e) as total_entities,\n                    avg(e.confidence) as avg_entity_confidence,\n                    count(CASE WHEN e.confidence &gt;= $high_conf THEN 1 END) as high_conf_entities,\n                    collect(DISTINCT e.type) as entity_types\n            \"\"\", high_conf=0.8).single()\n\n            # Relation statistics\n            relation_stats = session.run(\"\"\"\n                MATCH ()-[r:RELATED_TO]-()\n                RETURN\n                    count(r) as total_relations,\n                    avg(r.confidence) as avg_relation_confidence,\n                    count(CASE WHEN r.confidence &gt;= $high_conf THEN 1 END) as high_conf_relations\n            \"\"\", high_conf=0.8).single()\n\n            # Document statistics\n            doc_stats = session.run(\"\"\"\n                MATCH (d:Document)\n                OPTIONAL MATCH (d)-[:CONTAINS]-&gt;(c:Chunk)\n                RETURN\n                    count(d) as total_documents,\n                    count(c) as total_chunks\n            \"\"\").single()\n\n            return {\n                \"entities\": {\n                    \"total\": entity_stats[\"total_entities\"],\n                    \"average_confidence\": round(entity_stats[\"avg_entity_confidence\"] or 0, 3),\n                    \"high_confidence_count\": entity_stats[\"high_conf_entities\"],\n                    \"types\": entity_stats[\"entity_types\"] or []\n                },\n                \"relations\": {\n                    \"total\": relation_stats[\"total_relations\"],\n                    \"average_confidence\": round(relation_stats[\"avg_relation_confidence\"] or 0, 3),\n                    \"high_confidence_count\": relation_stats[\"high_conf_relations\"]\n                },\n                \"documents\": {\n                    \"total\": doc_stats[\"total_documents\"],\n                    \"chunks\": doc_stats[\"total_chunks\"]\n                },\n                \"thresholds\": {\n                    \"entity_confidence\": ENTITY_CONFIDENCE_THRESHOLD,\n                    \"relation_confidence\": RELATION_CONFIDENCE_THRESHOLD\n                }\n            }\n\n    except Exception as e:\n        return {\"error\": f\"Stats retrieval failed: {e}\"}\n</code></pre> <ol> <li>Constraint and Index Management</li> <li><code>_create_constraints()</code>: Ensures unique entity names and efficient graph queries via indexes.</li> <li>Lazy Initialization and Connection Checking<ul> <li><code>get_graph_db()</code>: Provides a singleton instance of the graph database connection.</li> <li><code>is_connected()</code>: Checks if the Neo4j connection is active.</li> </ul> </li> <li>Confidence-Based Filtering<ul> <li>All major extraction and retrieval functions use confidence scores to filter and rank entities and relations, improving answer quality and reducing noise.</li> </ul> </li> <li>Logging and Error Handling<ul> <li>All functions include logging and error handling to ensure robustness and traceability.</li> </ul> </li> <li>Multi-Hop and Fuzzy Matching<ul> <li>Subgraph retrieval supports multi-hop reasoning and fuzzy matching for advanced queries.</li> </ul> </li> <li>Dynamic Graph Updates<ul> <li>Graph structure is updated as files are added, modified, or deleted, keeping knowledge up-to-date.</li> </ul> </li> </ol>"},{"location":"setup/local_install/","title":"Local Setup Guide","text":"<p>This guide explains how to set up both the Frontend (Next.js) and Backend (FastAPI) locally for development. It also covers the environment variable configuration necessary for both parts of the application.</p>"},{"location":"setup/local_install/#local-run-using-docker","title":"Local run using docker","text":"<p>TO run the system using docker, just run the command</p> <p>docker-compose build docker-compose up -d</p> <p>This exposes the frontend to the 3000 port, env variables need to set by the user</p>"},{"location":"setup/local_install/#running-the-scripts","title":"Running The scripts","text":"<p><code>cd backend/scripts</code></p> <p><code>python populate_users.py &lt;path of target folder&gt;</code></p> <p>often time it was observed that tee system was unable to load .env from backend </p> <p>hence </p> <p>add the following code to top of populate_users.py</p> <p><code>load_dotenv(dotenv_path=Path(__file__).parent.parent / '.env')</code></p>"}]}